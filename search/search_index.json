{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Welcome to our website.</p>"},{"location":"checklist/","title":"Site Checklist","text":"<ol> <li>Customize the fields in your mkdocs.yml file</li> <li>Configure Google Analytics to use the right site ID</li> <li>Make sure that your .gitignore file includes the <code>site</code> directory</li> <li>Test the build</li> <li>Make sure the Edit button appears</li> <li>Make sure that code color heightening renders correctly</li> <li>run <code>git config advice.addIgnoredFile false</code></li> </ol>"},{"location":"code-highlight-test/","title":"Code Syntax Color Highlight Test","text":""},{"location":"code-highlight-test/#python","title":"Python","text":"<pre><code>hello_string = \"Hello World!\"\nprint(hello_string)\nx = 1\nif x == 1:\n    # indented four spaces\n    print(\"x is 1.\")\n</code></pre>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"course-description/","title":"Introduction to Graph Databases","text":"<p>Credits: 3 Length: 14 Weeks Level: Undergraduate (Junior/Senior) or Graduate Introductory Level Prerequisites:</p> <ul> <li>Prior coursework in databases or data modeling (recommended)</li> <li>Basic programming knowledge (Python, JavaScript, or similar)</li> <li>Familiarity with data structures (arrays, hash maps, trees)</li> </ul>"},{"location":"course-description/#course-overview","title":"Course Overview","text":"<p>This course introduces students to graph databases as powerful tools for representing, querying, and analyzing highly connected information. Students learn why traditional relational databases struggle with modern, relationship-heavy data and how Labeled Property Graph (LPG) databases treat relationships as first-class citizens with relationaship types, attributes, directionality, and semantics.</p> <p>We begin by contrasting the architectural foundations of RDBMS vs. NoSQL systems, explore the design motivations behind graph data models, and introduce the formal elements of LPGs: nodes, edges, properties, labels, and schema options. Students then gain hands-on experience modeling and querying real-world graphs using languages such as openCypher, or GSQL (depending on instructor preference).</p> <p>The course emphasizes building real applications: social networks, recommendation engines, fraud detection pipelines, supply-chain models, knowledge graphs, bill-of-materials (BOM), and healthcare data modeling. Students practice evaluating when to choose graph data models, how to optimize them, how to measure performance, and how to design graph schemas aligned with real business domains.</p> <p>The capstone project involves building an end-to-end graph application using an LPG graph database.</p>"},{"location":"course-description/#sample-outline-14-weeks","title":"Sample Outline (14 Weeks)","text":""},{"location":"course-description/#week-1-introduction-to-graph-thinking","title":"Week 1 \u2013 Introduction to Graph Thinking","text":"<ul> <li>Why data modeling matters in our AI-driven world</li> <li>The importance of world-models</li> <li>Knowledge representation strategy</li> <li>Six major representations of data</li> <li>RDBMS vs. OLAP vs. NoSQL</li> <li>When graphs outperform tables</li> <li>Edges are a first class citizen</li> <li>LPG: The most maintainable information model</li> <li>Case Study: Neo4j</li> <li>Timeline of Graph Database</li> </ul>"},{"location":"course-description/#week-2-nosql-and-the-rise-of-graphs","title":"Week 2 \u2013 NoSQL and the Rise of Graphs","text":"<ul> <li>Key-value, document, wide-column, and graph stores</li> <li>Tradeoff analysis (model precision, flexibility, scaling)</li> <li>Representations of knowledge</li> <li>The Knowledge Triangle</li> <li>Single server graphs</li> <li>Distributed graphs</li> <li>Case Study: TigerGraph</li> </ul>"},{"location":"course-description/#week-3-labeled-property-graph-lpg-information-model","title":"Week 3 \u2013 Labeled Property Graph (LPG) Information Model","text":"<ul> <li>Nodes, edges, labels, properties</li> <li>Representing metadata</li> <li>Open vs. Closed World Models</li> <li>Schema-optional vs. schema-enforced modeling</li> <li>Tools to view graph data models</li> <li>Adding rules to graphs</li> <li>Validating documents</li> <li>Validating graphs</li> </ul>"},{"location":"course-description/#week-4-query-languages-for-graphs","title":"Week 4 \u2013 Query Languages for Graphs","text":"<ul> <li>openCypher</li> <li>GSQL - using the map-reduce pattern on a distributed cluster</li> <li>Accumulators - keeping queries short</li> <li>Path patterns, hops, aggregations</li> <li>GQL - the emerging standard for advanced query languages</li> </ul>"},{"location":"course-description/#week-5-index-free-adjacency-performance","title":"Week 5 \u2013 Index-Free Adjacency &amp; Performance","text":"<ul> <li>Traversal fundamentals</li> <li>Constant-time neighbor access</li> <li>Cost comparison: joins vs. traversals</li> <li>Hop count</li> <li>MicroSim: Chart: Comparing Multi-hop performance on RDBMS vs. Graph</li> <li>Degree of a node</li> <li>Indegree</li> <li>Outdegree</li> <li>Edges per node ratios</li> <li>Indexes</li> <li>Vector indexes</li> <li>Graph metrics</li> <li>Statistical Query Tuning</li> </ul>"},{"location":"course-description/#week-6-benchmarking-techniques","title":"Week 6 \u2013 Benchmarking Techniques","text":"<ul> <li>Why benchmarking is critical to promoting graphs</li> <li>Graph benchmarking is difficult</li> <li>Synthetic benchmarks</li> <li>Single node benchmarks</li> <li>Multi-node benchmarks</li> <li>Predicting the future value of insights</li> <li>LDBC SNB benchmark</li> <li>Graph 500 rankings</li> <li>Measuring graph performance</li> <li>Query latency, throughput, and scalability</li> <li>Case Study - six degrees of separation</li> <li>Case Study - Graph 500 rankings</li> <li>Case Study - Healthcare Operations</li> </ul>"},{"location":"course-description/#week-7-modeling-social-networks-and-language","title":"Week 7 \u2013 Modeling Social Networks and Language","text":"<ul> <li>Friend graphs</li> <li>Modeling human resources</li> <li>Case Study: The Org Chart and Skill Management</li> <li>Diagram: Org Chart Models</li> <li>Influence graphs</li> <li>Modeling with edges as first-class citizens</li> <li>Extending your model</li> <li>Adding discussions</li> <li>Adding natural language language processing</li> <li>Adding products to your graph</li> <li>Adding sentiment to your graph</li> <li>Detecting bad fake accounts</li> <li>Case Study: Assigning Tasks from the Backlog</li> </ul>"},{"location":"course-description/#week-8-knowledge-representation-with-concept-graphs","title":"Week 8 \u2013 Knowledge Representation with Concept Graphs**","text":"<ul> <li>Concept dependency graphs</li> <li>Curriculum graphs</li> <li>Ontology-connected graph structures</li> <li>The Simple Knowledge Organization System (SKOS)</li> <li>Preferred Labels and Alternate Labels</li> <li>The Acronym List</li> <li>The Glossary</li> <li>The Controlled Vocabulary</li> <li>The Taxonomy</li> <li>The Ontology</li> <li>Modeling Enterprise Knowledge</li> <li>Modeling Department Knowledge</li> <li>Modeling Project Knowledge</li> <li>Case Study - Extracting Acton Items from Call Transcripts</li> <li>Modeling Personal Knowledge</li> <li>Notetaking</li> <li>Personal Knowledge Graphs</li> <li>Knowledge Capture</li> <li>Tacit Knowledge and Codifiable Knowledge,</li> <li>Enterprise Knowledge Management</li> </ul>"},{"location":"course-description/#week-9-graph-algorithms","title":"Week 9 - Graph Algorithms","text":"<ul> <li>Search</li> <li>Breath First Search (BFS)</li> <li>Depth First Search (DFS)</li> <li>A-Star (A*)</li> <li>Pathfinding</li> <li>Traveling Salesman</li> <li>PageRank</li> <li>Community detection</li> <li>Graph Neural Networks</li> <li>Data Science toolkits</li> </ul>"},{"location":"course-description/#week-9-graph-modeling-patterns","title":"Week 9 \u2013 Graph Modeling Patterns**","text":"<ul> <li>Subgraphs</li> <li>Supernode vs. anti-pattern nodes</li> <li>Hyperedges, multi-edges</li> <li>Time-based modeling patterns</li> <li>Time Trees</li> <li>Modeling Internet of Things Events</li> <li>Modeling Rules and Decision Trees</li> <li>Bitemporal Graph Models (Advanced Topic)</li> <li>Graph quality metrics</li> </ul>"},{"location":"course-description/#weeks-10-and-11-industry-reference-data-models","title":"Weeks 10 and 11 \u2013 Industry Reference Data Models","text":"<ul> <li>Web storefront graph model</li> <li>Product catalogs</li> <li>Bill-of-Materials (BOM) and complex parts</li> <li>Supply chain modeling</li> <li>Modeling financial transactions</li> <li>Fraud detection graphs</li> <li>Highly Regulated Industries</li> <li>Anti-Money Laundering (AML)</li> <li>Know Your Customer (KYC)</li> <li>Account-network traversal</li> <li>Provider/patient graphs</li> <li>Electronic health record modeling</li> <li>IT asset and dependency graphs</li> <li>Graph analytics vs. transactional graph queries</li> <li>Graph embeddings (introduction)</li> </ul>"},{"location":"course-description/#weeks-12-13-and-14-capstone-projects-and-presentations","title":"Weeks 12, 13 and 14 \u2013 Capstone Projects and Presentations","text":"<ul> <li>Students present a full graph application</li> <li>Modeling choices, data loading, queries, and performance measurements</li> </ul>"},{"location":"course-description/#sample-of-concepts-covered","title":"Sample of Concepts Covered**","text":"<ul> <li>NoSQL Databases</li> <li>Six Representations of Data</li> <li>RDBMS vs. Graph Databases</li> <li>OLAP vs. OLTP Workloads</li> <li>Key-Value Stores</li> <li>Document Databases</li> <li>Graph Stores</li> <li>Tradeoff Analysis / CAP</li> <li>Representations of Knowledge</li> <li>Concept Graphs</li> <li>Index-Free Adjacency</li> <li>Performance &amp; Benchmarking</li> <li>Web Storefront Modeling</li> <li>Learning Management System Modeling</li> <li>Curriculum &amp; Course Dependency Graphs</li> <li>Healthcare Data Graphs</li> <li>IT Asset &amp; Dependency Graphs</li> <li>Financial Transaction Graphs</li> <li>Fraud Detection Graphs</li> <li>Complex Parts &amp; BOM Graphs</li> <li>Supply Chain Models</li> <li>Graph Modeling Anti-Patterns</li> <li>Graph Algorithms</li> <li>openCypher / GSQL querying</li> <li>Data loading pipelines</li> <li>Best practices for graph schema design</li> </ul>"},{"location":"course-description/#topics-not-covered","title":"Topics Not Covered","text":"<ul> <li>How neural networks work</li> <li>Deep learning</li> <li>Complex statistics</li> <li>Details of how other databases work</li> </ul>"},{"location":"course-description/#learning-objectives","title":"Learning Objectives","text":"<p>Organized by Bloom\u2019s Taxonomy \u2013 2001 Revision</p> <p>Below are the learning objectives grouped by Remember \u2192 Understand \u2192 Apply \u2192 Analyze \u2192 Evaluate \u2192 Create.</p>"},{"location":"course-description/#1-remember-factual-knowledge","title":"1. Remember (Factual Knowledge)","text":"<p>Students will be able to:</p> <ul> <li>Define key terms such as node, edge, property, label, schema-optional, and index-free adjacency.</li> <li>List the major categories of NoSQL systems.</li> <li>Identify the components of an LPG information model.</li> <li>Recall common graph query languages (openCypher, GSQL, Gremlin).</li> </ul>"},{"location":"course-description/#2-understand-conceptual-knowledge","title":"2. Understand (Conceptual Knowledge)","text":"<p>Students will be able to:</p> <ul> <li>Explain why traditional RDBMS systems struggle with highly connected data.</li> <li>Describe the tradeoffs among key-value, document, and graph stores.</li> <li>Summarize how graph queries locate patterns more naturally than SQL joins.</li> <li>Explain how concept dependency graphs represent knowledge structures.</li> <li>Compare various real-world graph models (social, supply chain, healthcare, etc.).</li> </ul>"},{"location":"course-description/#3-apply-procedural-knowledge","title":"3. Apply (Procedural Knowledge)","text":"<p>Students will be able to:</p> <ul> <li>Construct simple LPG models using nodes, edges, and properties.</li> <li>Write openCypher or GSQL queries to retrieve and aggregate graph data.</li> <li>Load data sets into a graph database using CSV or ETL pipelines.</li> <li>Implement graph traversal queries that compute multi-hop patterns.</li> <li>Use performance measurement tools to benchmark graph workloads.</li> </ul>"},{"location":"course-description/#4-analyze-breakdown-structure","title":"4. Analyze (Breakdown &amp; Structure)","text":"<p>Students will be able to:</p> <ul> <li>Differentiate between good and bad graph modeling choices.</li> <li>Decompose a domain into entities, relationships, and multi-edge structures.</li> <li>Examine performance logs to identify bottlenecks in graph queries.</li> <li>Analyze alternative graph schema representations for a given domain.</li> <li>Map complex business processes into multi-layered graph models (e.g., supply chain, IT dependency graph).</li> </ul>"},{"location":"course-description/#5-evaluate-judgment-critique","title":"5. Evaluate (Judgment &amp; Critique)","text":"<p>Students will be able to:</p> <ul> <li>Justify when a graph database is more appropriate than an RDBMS or document store.</li> <li>Evaluate competing graph schema designs for clarity, scalability, and performance.</li> <li>Critique query patterns for correctness, efficiency, and maintainability.</li> <li>Assess the appropriateness of chosen benchmarks and workload profiles.</li> <li>Defend the modeling decisions used in their capstone project.</li> </ul>"},{"location":"course-description/#6-create-synthesis-design","title":"6. Create (Synthesis &amp; Design)","text":"<p>Students will be able to:</p> <ul> <li>Design a complete LPG schema for a complex domain (healthcare, finance, supply chain, etc.).</li> <li>Develop multi-step graph queries supporting application requirements.</li> <li>Create an end-to-end graph system including ETL, schema, queries, and visualizations.</li> <li>Build and present a capstone graph application grounded in real-world data.</li> <li>Propose design improvements using graph algorithms or structural pattern refinements.</li> </ul>"},{"location":"feedback/","title":"Feedback on Graph Data Modeling","text":"<p>You are welcome to connect with me on anytime on LinkedIn or submit any issues to GitHub Issue Log.  All pull-requests with fixes to errors or additions are always welcome.</p> <p>If you would like to fill out a short survey and give us ideas on how we can create better tools for intelligent textbooks in the future.</p>"},{"location":"glossary/","title":"Glossary of Terms","text":""},{"location":"glossary/#iso-definition","title":"ISO Definition","text":"<p>A term definition is considered to be consistent with ISO metadata registry guideline 11179 if it meets the following criteria:</p> <ol> <li>Precise</li> <li>Concise</li> <li>Distinct</li> <li>Non-circular</li> <li>Unencumbered with business rules</li> </ol>"},{"location":"glossary/#term","title":"Term","text":"<p>This is the definition of the term.</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p>"},{"location":"how-we-built-this-site/#image-generation-and-compression","title":"Image Generation and Compression","text":"<p>I have used ChatGPT to create most of my images.  However, they are too large for most websites.  To compress them down I used  https://tinypng.com/ which is a free tool  for compressing png images without significant loss of quality.  The files created with ChatGPT are typically around 1-2 MB.  After  using the TinyPNG site the size is typically around 200-300KB.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"references/","title":"Site References","text":"<ol> <li>mkdocs - https://www.mkdocs.org/ - this is our tool for building the website.  It converts Markdown into HTML in the <code>site</code> directory.</li> <li>mkdocs material theme - https://squidfunk.github.io/mkdocs-material/ - this is the theme for our site.  The theme adds the user interface elements that give our site the look and feel.  It also has the features such as social cards.</li> <li>GitHub Pages - https://pages.github.com/ - this is the free tool for hosting public websites created by mkdocs</li> <li>Markdown - https://www.mkdocs.org/user-guide/writing-your-docs/#writing-with-markdown - this is the format we use for text.  It allows us to have headers, lists, tables, links and images without learning HTML.</li> <li>Deploy Mkdocs GitHub Action - https://github.com/marketplace/actions/deploy-mkdocs - this is the tool we use to automatically build our site after edits are checked in with Git.</li> <li>Git Book - https://git-scm.com/book/en/v2 - a useful book on Git.  Just read the first two chapters to learn how to check in new code.</li> <li>Conda - https://conda.io/ - this is a command line tool that keeps our Python libraries organized for each project.</li> <li>VS Code - https://code.visualstudio.com/ - this is the integrated development environment we use to mange the files on our website.</li> <li>Markdown Paste - https://marketplace.visualstudio.com/items?itemName=telesoho.vscode-markdown-paste-image - this is the VS code extension we use to make sure we keep the markdown format generated by ChatGPT.</li> </ol>"},{"location":"chapters/","title":"Chapters","text":"<p>This textbook is organized into 12 chapters covering 200 concepts in graph databases.</p>"},{"location":"chapters/#chapter-overview","title":"Chapter Overview","text":"<ol> <li> <p>Introduction to Graph Thinking and Data Modeling - Establishes foundational concepts including data modeling principles, world models, knowledge representation, and core data structures.</p> </li> <li> <p>Database Systems and NoSQL - Compares RDBMS, OLAP, OLTP, and NoSQL databases to establish why graph databases excel at connected data.</p> </li> <li> <p>Labeled Property Graph Information Model - Introduces the core LPG model covering nodes, edges, properties, labels, and fundamental graph operations.</p> </li> <li> <p>Query Languages for Graph Databases - Covers OpenCypher, GSQL, and GQL query languages with comprehensive syntax and optimization techniques.</p> </li> <li> <p>Performance, Metrics, and Benchmarking - Explores performance fundamentals, indexing strategies, and benchmarking methodologies for graph databases.</p> </li> <li> <p>Graph Algorithms - Covers essential graph algorithms including search, pathfinding, centrality measures, and graph neural networks.</p> </li> <li> <p>Social Network Modeling - Applies graph databases to social networks, organizational structures, and human resources applications.</p> </li> <li> <p>Knowledge Representation and Management - Explores knowledge graphs, ontologies, taxonomies, and enterprise knowledge management systems.</p> </li> <li> <p>Graph Modeling Patterns and Data Loading - Covers design patterns, anti-patterns, and data loading strategies for graph databases.</p> </li> <li> <p>Commerce, Supply Chain, and IT Infrastructure - Demonstrates graph applications in e-commerce, supply chain optimization, and IT asset management.</p> </li> <li> <p>Financial, Healthcare, and Regulatory Applications - Explores domain-specific applications in finance, healthcare, and regulatory compliance.</p> </li> <li> <p>Advanced Topics and Distributed Systems - Covers distributed graph databases, real-time analytics, and capstone project design.</p> </li> </ol>"},{"location":"chapters/#how-to-use-this-textbook","title":"How to Use This Textbook","text":"<p>Progress through the chapters sequentially, as each chapter builds upon concepts introduced in previous chapters. The textbook follows a carefully designed learning path that respects concept dependencies, ensuring you have the necessary foundation before tackling advanced topics. Early chapters establish core principles, middle chapters explore algorithms and applications, and later chapters cover industry-specific use cases and distributed systems.</p> <p>Note: Each chapter includes a list of concepts covered. Make sure to complete prerequisites before moving to advanced chapters.</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/","title":"Introduction to Graph Thinking and Data Modeling","text":""},{"location":"chapters/01-intro-graph-thinking-data-modeling/#summary","title":"Summary","text":"<p>This foundational chapter introduces the core principles of data modeling and knowledge representation that underpin graph database thinking. You'll learn how world models shape our understanding of connected information and explore essential data structures that form the building blocks of graph systems. The chapter establishes the conceptual framework needed to understand why graphs are powerful tools for representing complex, interconnected data in modern AI-driven applications.</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 15 concepts from the learning graph:</p> <ol> <li>Data Modeling</li> <li>World Models</li> <li>Knowledge Representation</li> <li>Schema Design</li> <li>Hash Maps</li> <li>Trees</li> <li>Arrays</li> <li>Data Structures</li> <li>Relational Model</li> <li>Normalization</li> <li>Open World Model</li> <li>Closed World Model</li> <li>Minimum Spanning Tree</li> <li>Time Trees</li> <li>Decision Trees</li> </ol>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#prerequisites","title":"Prerequisites","text":"<p>This chapter assumes only the prerequisites listed in the course description:</p> <ul> <li>Prior coursework in databases or data modeling (recommended)</li> <li>Basic programming knowledge (Python, JavaScript, or similar)</li> <li>Familiarity with data structures (arrays, hash maps, trees)</li> </ul>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#why-graph-thinking-matters-now","title":"Why Graph Thinking Matters Now","text":"<p>In today's business world, there's a technology sitting right under your nose that could give you a massive competitive edge\u2014yet most companies completely ignore it. Graph databases represent one of the most powerful, underutilized tools in modern data management. While your competitors struggle with slow, clunky traditional databases, you could be making decisions in real-time, discovering hidden patterns, and building intelligent systems that actually understand how things connect.</p> <p>The reason is simple: the world isn't organized in tables and rows. Your customers, products, employees, supply chains, and knowledge all exist in a web of relationships. Traditional relational databases were designed in the 1970s for a different world\u2014one where data sat neatly in spreadsheets. Graph databases, by contrast, treat relationships as first-class citizens, making them exponentially faster and more intuitive for the connected data that drives modern business.</p> <p>This chapter introduces you to a fundamentally different way of thinking about data\u2014one that mirrors how the real world actually works. By the end, you'll understand why some of the world's most innovative companies have quietly adopted graph databases as their secret weapon, and why you should too.</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#the-foundation-data-structures","title":"The Foundation: Data Structures","text":"<p>Before we dive into graphs, let's build from what you already know. In programming, we organize information using data structures\u2014specialized formats for storing and accessing data efficiently. Think of data structures as different types of containers, each optimized for specific tasks.</p> <p>The three most common data structures form the building blocks of nearly every software system:</p> <ul> <li>Arrays - Sequential lists where items are stored in order</li> <li>Hash maps - Key-value pairs that enable instant lookups</li> <li>Trees - Hierarchical structures with parent-child relationships</li> </ul> <p>Understanding these structures is critical because graph databases evolved from recognizing their limitations when dealing with highly connected information.</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#arrays-the-sequential-container","title":"Arrays: The Sequential Container","text":"<p>Arrays store elements in a continuous sequence, like books on a shelf. You access elements by their position (index), which makes arrays incredibly fast when you know exactly where to look.</p> <pre><code>customers = [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"]\nprint(customers[0])  # Output: \"Alice\"\n</code></pre> <p>Arrays excel at ordered data and sequential access. However, they struggle when relationships matter more than order. If you want to know which customers purchased from which vendors, arrays force you to search through every element\u2014a slow process that gets worse as your data grows.</p> Visual Comparison: Array Performance     Type: diagram      Purpose: Illustrate how array search performance degrades with size      Components:     - Three arrays of different sizes (10 elements, 100 elements, 1000 elements)     - Visual representation showing linear search path through array     - Clock icons showing increasing search time     - Search pattern arrows moving left-to-right through elements      Layout:     - Three horizontal rows, one for each array size     - Arrays shown as connected boxes     - Red highlighted box at end showing target element     - Search path shown with curved arrow moving through each box      Labels:     - \"10 elements: ~5 comparisons average\"     - \"100 elements: ~50 comparisons average\"     - \"1000 elements: ~500 comparisons average\"     - \"O(n) linear time complexity\"      Style: Clean line drawing with color-coded elements     Color scheme: Blue boxes for array elements, red for target, yellow for search path      Implementation: SVG diagram with annotations"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#hash-maps-instant-lookups","title":"Hash Maps: Instant Lookups","text":"<p>Hash maps (also called dictionaries or associative arrays) solve the lookup problem brilliantly. Instead of searching through every item, hash maps use a mathematical trick called hashing to instantly find the exact location of any value based on its key.</p> <pre><code>customer_orders = {\n    \"Alice\": 42,\n    \"Bob\": 17,\n    \"Charlie\": 33,\n    \"Diana\": 8\n}\nprint(customer_orders[\"Bob\"])  # Output: 17 (instant lookup!)\n</code></pre> <p>Hash maps are extraordinarily efficient for direct lookups\u2014they find values in constant time regardless of how much data you have. This makes them perfect for simple key-value relationships.</p> <p>But here's the catch: hash maps only work for one-hop relationships. If you need to traverse multiple levels of connections (customers \u2192 orders \u2192 products \u2192 suppliers), you're back to multiple separate lookups, and performance tanks. This limitation becomes critical when modeling real-world business problems.</p> Hash Map Architecture Visualization     Type: diagram      Purpose: Show how hash maps achieve constant-time lookups through hashing      Components:     - Input key (\"Alice\") at top     - Hash function box in middle     - Array of buckets at bottom     - Arrows showing key transformation to index     - Retrieved value highlighted      Process flow:     1. Key \"Alice\" enters hash function     2. Hash function converts to number: hash(\"Alice\") = 7     3. Arrow points to bucket 7 in array     4. Value 42 retrieved from bucket 7      Additional elements:     - Side panel showing other key-value pairs     - \"O(1) constant time\" label     - Collision handling notation (chaining shown with linked nodes)      Visual style: Flowchart-style with clear directional arrows     Color scheme: Green for successful path, orange for hash function, blue for storage array      Implementation: SVG/HTML diagram"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#trees-hierarchical-organization","title":"Trees: Hierarchical Organization","text":"<p>Trees represent hierarchical relationships with a root node at the top and branches extending downward. Each node has exactly one parent (except the root) and can have multiple children. Trees naturally model organizational charts, file systems, and decision-making processes.</p> <pre><code>        CEO\n       /   \\\n      CTO   CFO\n     /  \\     \\\n   Dev  QA   Accounting\n</code></pre> <p>Trees are excellent for hierarchical data and enable efficient searching when organized properly (like binary search trees). However, trees enforce a strict limitation: no node can have multiple parents, and there can be no cycles. Real-world relationships don't follow these rules. An employee might report to multiple managers (matrix organization), customers might influence each other (social networks), and products might depend on each other in circular ways.</p> <p>This is where traditional data structures hit their wall, and where graphs become game-changing.</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#data-modeling-representing-reality","title":"Data Modeling: Representing Reality","text":"<p>Data modeling is the art and science of representing real-world information in a format that computers can process efficiently. Every software system, from Netflix recommendations to banking transactions, relies on data models to organize information.</p> <p>The choice of data model isn't just a technical decision\u2014it's a strategic one that impacts:</p> <ul> <li>How fast your system responds to queries</li> <li>How easily you can add new features</li> <li>How much your infrastructure costs</li> <li>Whether you can discover hidden insights in your data</li> </ul> <p>Most businesses default to what they know: relational databases. But that choice, made without considering alternatives, can put you years behind competitors who've discovered better approaches for connected data.</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#knowledge-representation-capturing-what-you-know","title":"Knowledge Representation: Capturing What You Know","text":"<p>Knowledge representation asks a fundamental question: How do we encode human understanding into computer systems? It's not enough to store data\u2014we need to capture meaning, relationships, and context.</p> <p>Consider a simple business scenario:</p> <ul> <li>\"Alice purchased Product X\"</li> <li>\"Product X was manufactured by Vendor Y\"</li> <li>\"Vendor Y is located in Country Z\"</li> </ul> <p>Traditional databases store these as separate facts in different tables. But the knowledge isn't in the individual facts\u2014it's in how they connect. Graph databases represent knowledge by making those connections explicit and queryable, enabling questions like \"Which countries do my customers ultimately depend on?\" to be answered in milliseconds rather than minutes.</p> Knowledge Representation Comparison     Type: diagram      Purpose: Compare how traditional tables vs. graphs represent the same knowledge      Layout: Two-panel comparison (left: RDBMS, right: Graph)      Left panel - RDBMS representation:     - Three separate tables:       1. Purchases table: (customer_id, product_id, date)       2. Products table: (product_id, vendor_id, name)       3. Vendors table: (vendor_id, country, name)     - Red dotted lines showing foreign key relationships     - Label: \"Knowledge is implicit in foreign keys\"      Right panel - Graph representation:     - Same information shown as connected nodes:       * Alice (Customer node) --PURCHASED--&gt; Product X (Product node)       * Product X --MANUFACTURED_BY--&gt; Vendor Y (Vendor node)       * Vendor Y --LOCATED_IN--&gt; Country Z (Location node)     - Green solid lines showing direct relationships     - Label: \"Knowledge is explicit in relationships\"      Visual styling:     - Tables shown as traditional database tables with rows/columns     - Graph nodes shown as labeled circles     - Relationship arrows with type labels     - Highlighting showing easier traversal path in graph      Color scheme: Orange for RDBMS elements, gold for graph elements      Implementation: Side-by-side SVG comparison diagram"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#world-models-how-systems-understand-reality","title":"World Models: How Systems Understand Reality","text":"<p>A world model is a system's internal representation of how things work. Just as you have a mental model of your workplace (who does what, who reports to whom, where resources are located), software systems need world models to make intelligent decisions.</p> <p>There are two fundamentally different approaches to world models, each with profound implications:</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#closed-world-model","title":"Closed World Model","text":"<p>The closed world model assumes that if something isn't explicitly stated in the database, it's false. Traditional relational databases operate under this assumption. If there's no row saying \"Alice knows Bob,\" then Alice doesn't know Bob\u2014end of story.</p> <p>This works well for controlled environments where you have complete information:</p> <ul> <li>Accounting systems (you know all transactions)</li> <li>Inventory systems (you know all products in stock)</li> <li>Employee databases (you know all employees)</li> </ul> <p>Advantages of closed world:</p> <ul> <li>Simpler queries and logic</li> <li>Guaranteed consistency</li> <li>Predictable behavior</li> </ul> <p>Disadvantages of closed world:</p> <ul> <li>Cannot handle incomplete information</li> <li>Struggles with evolving knowledge</li> <li>Forces premature commitments about what you know</li> </ul>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#open-world-model","title":"Open World Model","text":"<p>The open world model recognizes that absence of information doesn't mean something is false\u2014it means you don't know yet. This mirrors reality much better. If your database doesn't say \"Alice knows Bob,\" it simply means that relationship hasn't been confirmed, not that it's impossible.</p> <p>Graph databases can operate under either model, but they excel at open world scenarios:</p> <ul> <li>Social networks (you don't know all friendships)</li> <li>Supply chains (new vendors emerge constantly)</li> <li>Knowledge graphs (information is continuously discovered)</li> </ul> <p>Advantages of open world:</p> <ul> <li>Handles incomplete information gracefully</li> <li>Supports incremental knowledge building</li> <li>Adapts to changing reality</li> </ul> <p>This flexibility gives graph-based systems a huge advantage when dealing with real-world complexity. While competitors using rigid closed-world systems struggle to adapt to new information, graph databases seamlessly incorporate new discoveries and evolving relationships.</p> Closed World vs. Open World Model Comparison Table     Type: markdown-table      A comparison table would be embedded here directly in markdown:  <p>Here's how the two models differ in practice:</p> Aspect Closed World Model Open World Model Unknown information Assumed false Assumed unknown Best for Complete, controlled data Evolving, incomplete data Typical use RDBMS, traditional systems Knowledge graphs, AI systems Query behavior Returns definitive yes/no Returns yes/no/unknown Adding new facts Requires schema changes often Seamlessly integrated Example domains Banking, inventory, payroll Social networks, research, recommendations"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#the-relational-model-and-its-limitations","title":"The Relational Model and Its Limitations","text":"<p>The relational model, introduced by Edgar Codd in 1970, revolutionized data management. It organizes data into tables (relations) with rows (records) and columns (attributes), connected through foreign keys. For decades, this model dominated because it solved the critical problems of its era: reducing data redundancy and ensuring consistency.</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#normalization-the-relational-strength","title":"Normalization: The Relational Strength","text":"<p>Normalization is the process of organizing data to minimize redundancy. Instead of repeating customer information in every order record, you store customers once and reference them through IDs.</p> <p>Example of normalization:</p> <p>Unnormalized (redundant): <pre><code>Orders table:\n| order_id | customer_name | customer_email     | product   |\n|----------|---------------|-------------------|-----------|\n| 1        | Alice         | alice@email.com   | Widget    |\n| 2        | Alice         | alice@email.com   | Gadget    |\n| 3        | Bob           | bob@email.com     | Widget    |\n</code></pre></p> <p>Normalized (efficient): <pre><code>Customers table:\n| customer_id | name  | email           |\n|-------------|-------|-----------------|\n| 101         | Alice | alice@email.com |\n| 102         | Bob   | bob@email.com   |\n\nOrders table:\n| order_id | customer_id | product |\n|----------|-------------|---------|\n| 1        | 101         | Widget  |\n| 2        | 101         | Gadget  |\n| 3        | 102         | Widget  |\n</code></pre></p> <p>Normalization brilliantly solves data consistency: update Alice's email once, and all her orders automatically reflect the change. This was perfect for 1970s business applications like inventory and payroll.</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#the-performance-cliff-when-relationships-explode","title":"The Performance Cliff: When Relationships Explode","text":"<p>Here's where relational databases hit their fundamental limit: JOINs. Every relationship traversal requires a JOIN operation\u2014a expensive process where the database matches rows from different tables.</p> <p>One JOIN? Fast enough. Two JOINs? Still manageable. But real business questions require many levels of traversal:</p> <ul> <li>\"Which products do friends of my friends recommend?\" (3 hops)</li> <li>\"What's the supply chain impact if this vendor fails?\" (5+ hops)</li> <li>\"Which skills are required for career paths to executive roles?\" (7+ hops)</li> </ul> <p>Each additional JOIN multiplies the computational cost. By the time you're traversing 4-5 levels of relationships, query times explode from milliseconds to minutes. This isn't a minor inconvenience\u2014it's the difference between building real-time recommendation engines and batch reports that run overnight.</p> <p>See the performance cliff: The interactive chart below demonstrates this dramatic difference. Notice how RDBMS performance degrades exponentially (orange line) while graph databases maintain constant-time performance (gold line). Toggle between logarithmic and linear scales to see the difference from different perspectives.</p> <p>View Chart Fullscreen See Detailed Analysis</p> <p>This performance difference isn't theoretical\u2014it's the reason companies like LinkedIn, eBay, NASA, and Walmart have migrated relationship-heavy workloads to graph databases. While competitors wait minutes for insights, graph-powered systems respond instantly.</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#schema-design-planning-your-data-structure","title":"Schema Design: Planning Your Data Structure","text":"<p>Schema design is the architectural blueprint for how you'll organize data. It defines what entities exist, what properties they have, and how they relate. Good schema design makes your system fast, flexible, and maintainable. Poor schema design creates technical debt that haunts you for years.</p> <p>Traditional relational databases require rigid schemas defined upfront. Adding a new property or relationship type means schema migrations, downtime, and developer headaches. This made sense when business requirements changed slowly, but modern businesses need agility.</p> <p>Graph databases offer schema-optional or schema-flexible approaches. You can enforce schemas when consistency matters (like financial data) but also add new node types, properties, and relationships on the fly as business needs evolve. This flexibility is a competitive weapon\u2014you can experiment, iterate, and adapt faster than competitors locked into rigid relational schemas.</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#special-tree-structures-solving-specific-problems","title":"Special Tree Structures: Solving Specific Problems","text":"<p>Trees aren't just academic concepts\u2014they solve real business problems. Three specialized tree structures appear frequently in modern systems:</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#decision-trees-automating-complex-choices","title":"Decision Trees: Automating Complex Choices","text":"<p>Decision trees represent a series of choices leading to outcomes, like a flowchart. They're widely used in machine learning, business rule engines, and troubleshooting systems.</p> <pre><code>Is customer premium?\n\u251c\u2500 Yes \u2192 Offer expedited shipping\n\u2514\u2500 No \u2192 Is order over $50?\n    \u251c\u2500 Yes \u2192 Offer free shipping\n    \u2514\u2500 No \u2192 Standard shipping only\n</code></pre> <p>Graph databases excel at storing and traversing decision trees because they can represent the logic structure naturally, query it efficiently, and modify rules without rebuilding entire systems.</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#time-trees-organizing-temporal-data","title":"Time Trees: Organizing Temporal Data","text":"<p>Time trees organize events hierarchically by time periods: years contain months, months contain days, days contain hours. This structure enables incredibly efficient time-range queries.</p> <p>Instead of scanning millions of timestamp records, you traverse the tree: - \"Show sales in Q3 2024\" \u2192 Navigate to 2024 \u2192 Q3 \u2192 aggregate all descendants - \"Compare Mondays across all weeks\" \u2192 Traverse day-of-week branches</p> <p>Time trees are essential for time-series analysis, scheduling systems, and historical trend queries. In a graph database, time trees combine naturally with other relationship types, enabling questions like \"How did customer behavior on Mondays in Q3 affect supply chain performance?\" that would be nightmarishly complex in relational systems.</p> Time Tree Structure Visualization     Type: diagram      Purpose: Show how time trees organize temporal data hierarchically for efficient querying      Structure:     - Root: \"All Time\"     - Level 1: Years (2022, 2023, 2024)     - Level 2: Quarters (Q1, Q2, Q3, Q4)     - Level 3: Months (Jan, Feb, Mar...)     - Level 4: Weeks (Week 1, Week 2...)     - Level 5: Days (individual dates)      Visual representation:     - Expand one branch fully (e.g., 2024 \u2192 Q3 \u2192 July \u2192 Week 2 \u2192 July 10)     - Other branches collapsed or partially shown     - Highlight a query path: \"All events in Q3 2024\"     - Show aggregation happening at quarter level      Layout: Top-down tree with root at top      Annotations:     - \"O(log n) time to reach any date\"     - \"Aggregate by traversing subtree\"     - \"Add new events by inserting leaves\"      Color scheme:     - Blue for year nodes     - Green for quarter nodes     - Yellow for month nodes     - Orange for week/day nodes     - Red highlight for query path      Implementation: Interactive SVG tree diagram     Size: 800x600px"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#minimum-spanning-tree-optimizing-networks","title":"Minimum Spanning Tree: Optimizing Networks","text":"<p>A minimum spanning tree is a subset of edges in a graph that connects all nodes with the minimum total weight, without creating cycles. This sounds abstract, but it solves critical real-world problems:</p> <ul> <li>Network design: Connecting offices with minimum cable length</li> <li>Supply chain optimization: Minimizing total shipping distance</li> <li>Utility routing: Designing water, power, or data networks efficiently</li> </ul> <p>Graph databases can calculate minimum spanning trees using algorithms like Kruskal's or Prim's, then store and update them as networks evolve. This gives operations teams real-time answers to questions like \"What's the cheapest way to connect these locations?\" without running expensive batch calculations.</p> <p>Try it yourself: The interactive simulation below demonstrates both Kruskal's and Prim's algorithms. Use the controls to step through the algorithm or watch it run automatically. Notice how both algorithms find the same optimal total weight, even though they select edges in different orders.</p> <p>View MicroSim Fullscreen See Full Documentation</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#the-graph-advantage-why-this-matters-for-your-career","title":"The Graph Advantage: Why This Matters for Your Career","text":"<p>Understanding these fundamental concepts\u2014data structures, data modeling, world models, and specialized algorithms\u2014prepares you for the most significant shift in data management in 50 years.</p> <p>Companies using graph databases report:</p> <ul> <li>10-100x faster query performance for relationship-heavy workloads</li> <li>50-80% reduction in development time for connected data features</li> <li>Significantly lower infrastructure costs due to efficient traversals</li> <li>Faster time-to-market for new features requiring relationship analysis</li> </ul> <p>More importantly, graphs enable entirely new capabilities that are impractical with relational databases:</p> <ul> <li>Real-time fraud detection through network analysis</li> <li>Instant recommendation engines analyzing millions of connections</li> <li>Supply chain resilience planning considering multi-hop dependencies</li> <li>Knowledge graphs powering intelligent assistants</li> <li>Social network analysis revealing hidden influence patterns</li> </ul> <p>The companies leveraging these capabilities aren't all tech giants. They're nimble competitors who recognized that relationships are the new competitive advantage. In industries from healthcare to finance, retail to logistics, graph-powered insights are creating winners and losers.</p>"},{"location":"chapters/01-intro-graph-thinking-data-modeling/#key-takeaways","title":"Key Takeaways","text":"<p>This chapter established the foundational concepts you'll build on throughout this course:</p> <ol> <li>Data structures (arrays, hash maps, trees) each have strengths and limitations\u2014none handle multi-hop relationships efficiently</li> <li>Data modeling choices have strategic business implications, not just technical ones</li> <li>World models (open vs. closed) determine how systems handle incomplete or evolving information</li> <li>The relational model revolutionized data management but hits fundamental performance limits with connected data</li> <li>Normalization solves redundancy but creates JOIN overhead that cripples relationship queries</li> <li>Schema design requires balancing consistency with flexibility\u2014graph databases offer both</li> <li>Specialized tree structures (decision trees, time trees, minimum spanning trees) solve specific business problems efficiently</li> </ol> <p>Most importantly: Traditional approaches to data management create a performance cliff when relationships matter. Companies that recognize this reality and adopt graph databases gain years of competitive advantage while others struggle with overnight batch processes and can't build the real-time, intelligent features customers now expect.</p> <p>In the next chapter, we'll explore how NoSQL databases emerged to challenge relational dominance, and why graph databases represent the culmination of this evolution for relationship-rich data.</p>"},{"location":"chapters/02-database-systems-nosql/","title":"Database Systems and NoSQL","text":""},{"location":"chapters/02-database-systems-nosql/#summary","title":"Summary","text":"<p>This chapter provides a comprehensive comparison of traditional database systems and modern NoSQL alternatives, establishing the context for understanding graph databases. You'll explore the evolution from RDBMS through OLAP and OLTP systems to the diverse NoSQL landscape including key-value stores, document databases, and wide-column stores. By understanding the CAP theorem and tradeoff analysis, you'll gain insight into why graph databases emerged as the optimal solution for relationship-heavy data.</p>"},{"location":"chapters/02-database-systems-nosql/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 10 concepts from the learning graph:</p> <ol> <li>RDBMS</li> <li>OLAP</li> <li>OLTP</li> <li>NoSQL Databases</li> <li>Key-Value Stores</li> <li>Document Databases</li> <li>Wide-Column Stores</li> <li>Graph Databases</li> <li>CAP Theorem</li> <li>Tradeoff Analysis</li> </ol>"},{"location":"chapters/02-database-systems-nosql/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Introduction to Graph Thinking and Data Modeling</li> </ul>"},{"location":"chapters/02-database-systems-nosql/#the-database-revolution-you-need-to-know-about","title":"The Database Revolution You Need to Know About","text":"<p>For fifty years, businesses have operated under a false assumption: that relational databases are the only serious choice for managing enterprise data. This belief has cost companies billions in lost opportunities, slow performance, and competitive disadvantages they don't even realize they have.</p> <p>The truth is more interesting\u2014and more profitable. Since the mid-2000s, a quiet revolution has transformed data management. Companies that recognized the limitations of traditional RDBMS and adopted the right alternatives gained years of advantage over competitors still struggling with overnight batch processes and rigid schemas. Graph databases represent the culmination of this evolution, yet most organizations haven't discovered them yet.</p> <p>This chapter traces the evolution from traditional relational systems through the NoSQL revolution, revealing why graph databases solve problems that no other technology can address efficiently. By understanding this landscape, you'll see why forward-thinking companies are betting their futures on graph technology\u2014and why you should too.</p>"},{"location":"chapters/02-database-systems-nosql/#rdbms-the-foundation-and-its-cracks","title":"RDBMS: The Foundation and Its Cracks","text":"<p>Relational Database Management Systems (RDBMS) dominated enterprise computing for half a century for good reason. Introduced in the 1970s, RDBMS brought order to data chaos through a simple yet powerful idea: organize data into tables with rows and columns, enforce relationships through foreign keys, and query everything with SQL.</p> <p>The RDBMS model excelled at its original purpose: managing structured business data for transactional processing. Inventory systems, payroll, accounting, order management\u2014these applications fit perfectly into the relational paradigm. RDBMS provided critical guarantees (ACID transactions), prevented data corruption, and enforced business rules through schemas.</p> <p>But as you learned in Chapter 1, RDBMS hits a fundamental performance wall when relationships become central to your queries. Beyond that limitation, RDBMS systems struggle with:</p> <ul> <li>Schema rigidity: Every change requires migrations, downtime, and developer time</li> <li>Scaling horizontally: RDBMS was designed for vertical scaling (bigger servers), not distributed systems</li> <li>Handling semi-structured data: JSON, XML, and documents don't fit cleanly into tables</li> <li>Agile development: Schema-first design conflicts with iterative development</li> <li>Modern workload patterns: Real-time analytics, recommendation engines, and fraud detection require different approaches</li> </ul> <p>These limitations became critical as the internet grew, data volumes exploded, and businesses needed systems that could adapt quickly to changing requirements.</p>"},{"location":"chapters/02-database-systems-nosql/#oltp-vs-olap-different-workloads-different-needs","title":"OLTP vs. OLAP: Different Workloads, Different Needs","text":"<p>Not all database workloads are created equal. The database world divides into two fundamentally different usage patterns:</p>"},{"location":"chapters/02-database-systems-nosql/#oltp-online-transaction-processing","title":"OLTP: Online Transaction Processing","text":"<p>OLTP systems handle day-to-day business operations: processing orders, updating inventory, recording payments, managing customer accounts. These workloads are characterized by:</p> <ul> <li>Many small transactions: Thousands of individual operations per second</li> <li>Read and write operations: Constant updates, inserts, and reads</li> <li>Current data focus: \"What's happening right now?\"</li> <li>Row-oriented access: Operating on individual records</li> <li>ACID requirements: Transactions must be reliable and consistent</li> </ul> <p>Examples: E-commerce checkouts, banking transactions, reservation systems, user registrations</p> <p>Traditional RDBMS systems like PostgreSQL, MySQL, and Oracle were optimized for OLTP workloads. They excel at ensuring your customer's order is processed correctly, your bank account balance updates atomically, and no inventory item is double-booked.</p>"},{"location":"chapters/02-database-systems-nosql/#olap-online-analytical-processing","title":"OLAP: Online Analytical Processing","text":"<p>OLAP systems analyze historical data to answer business intelligence questions: \"Which products sell best in which regions?\" \"What trends do we see in customer behavior?\" \"How can we optimize our supply chain?\" These workloads have different characteristics:</p> <ul> <li>Few large queries: Complex aggregations scanning millions of rows</li> <li>Read-heavy: Analyzing data, not updating it</li> <li>Historical data focus: \"What patterns exist across time?\"</li> <li>Column-oriented access: Computing statistics across many records</li> <li>Eventual consistency acceptable: Slight delays don't matter for analysis</li> </ul> <p>Examples: Business intelligence dashboards, sales trend analysis, data warehouses, predictive analytics</p> <p>OLAP systems like Snowflake, Teradata, and ClickHouse organize data differently (column stores rather than row stores) and make different trade-offs (speed over consistency) to deliver fast analytical queries.</p> <p>The key insight: Traditional RDBMS tries to serve both OLTP and OLAP workloads, but optimizing for one often hurts performance for the other. This tension drove the creation of specialized database systems\u2014and eventually, the NoSQL revolution.</p> <p>Here's how the two approaches differ in practice:</p> Characteristic OLTP OLAP Primary Use Daily operations Business intelligence &amp; analytics Query Type Simple, frequent Complex, infrequent Data Scope Current, small sets Historical, large datasets Update Pattern Continuous writes Batch loads Response Time Milliseconds Seconds to minutes Users Thousands (customers, employees) Dozens (analysts, executives) Schema Normalized (3NF+) Denormalized (star, snowflake) Priority Consistency &amp; correctness Query speed &amp; insights"},{"location":"chapters/02-database-systems-nosql/#the-nosql-revolution-breaking-free-from-tables","title":"The NoSQL Revolution: Breaking Free from Tables","text":"<p>Around 2005-2010, a perfect storm created the conditions for revolutionary change in database technology. Web-scale companies like Google, Amazon, and Facebook faced problems that traditional RDBMS simply couldn't solve:</p> <ul> <li>Massive scale: Billions of users, petabytes of data</li> <li>Global distribution: Data centers across continents</li> <li>Rapid iteration: Deploying code changes multiple times per day</li> <li>Semi-structured data: User-generated content, social graphs, sensor data</li> <li>Availability requirements: 99.99%+ uptime, no maintenance windows</li> </ul> <p>Traditional RDBMS forced impossible choices: you could have strong consistency OR horizontal scalability, rigid schemas OR agile development, ACID transactions OR availability at scale\u2014but not both.</p> <p>NoSQL databases emerged from this crisis, not as a single technology but as a movement rejecting the relational model's one-size-fits-all approach. The name \"NoSQL\" originally meant \"No SQL\" but quickly evolved to \"Not Only SQL\"\u2014acknowledging that different problems require different solutions.</p> <p>The NoSQL revolution delivered four fundamental database categories, each optimized for specific use cases:</p> <ol> <li>Key-Value Stores - Ultra-fast simple lookups</li> <li>Document Databases - Flexible, schema-less JSON storage</li> <li>Wide-Column Stores - Massive-scale structured data</li> <li>Graph Databases - Relationship-centric connected data</li> </ol> <p>What unified them? All rejected tables-and-JOINs in favor of architectures better suited to modern workloads. All embraced horizontal scaling across commodity servers. All traded some traditional guarantees (like ACID transactions) for better performance, availability, or flexibility.</p> <p>The competitive advantage: Companies that adopted the right NoSQL solution for their use case gained 10-100\u00d7 performance improvements, reduced infrastructure costs, and could iterate faster than competitors stuck in the RDBMS mindset. Those that chose poorly\u2014using document databases for graph problems, or key-value stores for analytical workloads\u2014gained nothing and created technical debt.</p> <p>Understanding which NoSQL database solves which problem is your competitive intelligence.</p>"},{"location":"chapters/02-database-systems-nosql/#key-value-stores-speed-through-simplicity","title":"Key-Value Stores: Speed Through Simplicity","text":"<p>Key-value stores are the simplest NoSQL databases\u2014essentially distributed hash maps that scale across hundreds of servers. Every piece of data is stored as a key-value pair: provide a key, get the associated value back. No complex queries, no relationships, no schemas\u2014just blazing-fast reads and writes.</p> <p>How they work:</p> <pre><code>PUT \"user:12345\" \u2192 {\"name\": \"Alice\", \"email\": \"alice@example.com\", \"premium\": true}\nGET \"user:12345\" \u2192 {\"name\": \"Alice\", \"email\": \"alice@example.com\", \"premium\": true}\n</code></pre> <p>That's it. The database doesn't know or care what's inside the value (could be JSON, XML, binary data, images). It just stores and retrieves based on keys.</p> <p>Why they're fast:</p> <ul> <li>Hash-based partitioning: Key determines which server holds the data</li> <li>No query planning: Just hash the key and look up the value</li> <li>In-memory operation: Many key-value stores keep hot data in RAM</li> <li>Horizontal scaling: Add more servers to handle more keys</li> <li>No locks for reads: Multiple clients can read simultaneously</li> </ul> <p>Popular implementations:</p> <ul> <li>Redis: In-memory, sub-millisecond latency, supports data structures (lists, sets, sorted sets)</li> <li>Amazon DynamoDB: Fully managed, infinite scale, millisecond performance</li> <li>Riak: Distributed, highly available, eventual consistency</li> <li>Memcached: Simple caching, often used in front of RDBMS</li> </ul> <p>Use cases:</p> <ul> <li>Session management: Store user session data for web applications</li> <li>Caching: Speed up database queries by caching results</li> <li>Real-time counters: Page views, likes, votes</li> <li>Shopping carts: Temporary storage for e-commerce</li> <li>Rate limiting: Track API usage per user</li> </ul> <p>Limitations:</p> <ul> <li>No relationships: Can't efficiently answer \"find all users who purchased product X\"</li> <li>No complex queries: Can't search by value, only by key</li> <li>No aggregations: Can't compute \"average age of premium users\"</li> <li>Limited consistency: Often eventually consistent, not immediately</li> </ul> <p>The bottom line: Key-value stores solve one problem brilliantly\u2014fast access to data you can identify by a simple key. They fail completely at everything else. If your use case fits, they're unbeatable. If you need relationships or complex queries, they're useless.</p>"},{"location":"chapters/02-database-systems-nosql/#document-databases-flexible-schema-for-agile-development","title":"Document Databases: Flexible Schema for Agile Development","text":"<p>Document databases store data as documents (typically JSON or BSON) rather than rows in tables. Each document is self-contained, with its own structure, and different documents in the same collection can have completely different fields. This flexibility revolutionized how developers work with databases.</p> <p>Example document:</p> <pre><code>{\n  \"_id\": \"507f1f77bcf86cd799439011\",\n  \"name\": \"Alice Johnson\",\n  \"email\": \"alice@example.com\",\n  \"age\": 28,\n  \"addresses\": [\n    {\"type\": \"home\", \"city\": \"Seattle\", \"state\": \"WA\"},\n    {\"type\": \"work\", \"city\": \"Redmond\", \"state\": \"WA\"}\n  ],\n  \"premium\": true,\n  \"preferences\": {\n    \"newsletter\": true,\n    \"notifications\": false\n  },\n  \"tags\": [\"early-adopter\", \"power-user\"]\n}\n</code></pre> <p>Notice how this document contains nested structures (addresses, preferences), arrays (addresses, tags), and mixed data types\u2014all in a single record without requiring separate tables.</p> <p>Why developers love them:</p> <ul> <li>No upfront schema: Start coding immediately, define structure as you go</li> <li>Natural object mapping: JSON documents map directly to programming language objects</li> <li>Flexible evolution: Add fields to new documents without migrating old ones</li> <li>Embedded data: Related information stays together (no JOINs needed for simple queries)</li> <li>Agile-friendly: Change requirements? Just start storing new fields.</li> </ul> <p>Popular implementations:</p> <ul> <li>MongoDB: Most popular, rich query language, horizontal scaling (sharding)</li> <li>Couchbase: High performance, built-in caching, mobile sync</li> <li>Amazon DocumentDB: MongoDB-compatible, fully managed</li> <li>Firebase Firestore: Real-time sync, mobile/web-optimized</li> </ul> <p>Use cases:</p> <ul> <li>Content management systems: Articles, blog posts, product catalogs with varying attributes</li> <li>User profiles: Different users have different fields based on account types</li> <li>Product catalogs: Products with different attributes (electronics vs clothing vs books)</li> <li>Mobile applications: Offline-first sync, schema flexibility</li> <li>Rapid prototyping: Build MVPs without database design delays</li> </ul> <p>Limitations:</p> <ul> <li>Weak relationship support: Can embed documents or reference by ID, but no native JOIN equivalent</li> <li>Data duplication: Embedding creates redundancy; updating repeated data requires multiple writes</li> <li>Complex queries across documents: Aggregations work, but performance degrades with complexity</li> <li>Eventual consistency: Some implementations sacrifice immediate consistency for availability</li> <li>No cross-document ACID transactions (until recently): MongoDB 4.0+ added multi-document transactions, but at performance cost</li> </ul> <p>The competitive insight: Document databases democratized database development. Startups could build and iterate faster than enterprises using RDBMS. But when those startups grew and needed to query relationships between documents at scale, they hit the same wall RDBMS users encountered\u2014just for different reasons.</p>"},{"location":"chapters/02-database-systems-nosql/#wide-column-stores-massive-scale-structured-data","title":"Wide-Column Stores: Massive-Scale Structured Data","text":"<p>Wide-column stores (also called column-family databases) organize data into rows and columns like RDBMS, but with a crucial difference: columns are grouped into families, and the system stores data column-by-column rather than row-by-row. This architecture enables massive horizontal scaling while maintaining some structure.</p> <p>The key concept: Instead of tables with fixed columns, you have:</p> <ul> <li>Column families (like tables, but flexible)</li> <li>Rows identified by a unique key</li> <li>Columns within families (each row can have different columns)</li> <li>Versioned values (often with timestamps)</li> </ul> <p>Architecture example (conceptual):</p> <pre><code>RowKey: user123\n  Personal:name = \"Alice Johnson\"\n  Personal:age = 28\n  Personal:email = \"alice@example.com\"\n  Activity:last_login = \"2025-01-18T10:30:00Z\"\n  Activity:page_views = 847\n\nRowKey: user456\n  Personal:name = \"Bob Smith\"\n  Personal:age = 35\n  Activity:last_login = \"2025-01-18T09:15:00Z\"\n  Activity:purchases = 12\n  Preferences:theme = \"dark\"\n</code></pre> <p>Notice that user123 and user456 have different columns, and the data is organized by column families (Personal, Activity, Preferences).</p> <p>Why they scale:</p> <ul> <li>Column-oriented storage: Reading specific columns (not full rows) is extremely efficient</li> <li>Horizontal partitioning: Rows distributed across thousands of servers based on row key</li> <li>Compression: Similar values in a column compress better together</li> <li>Append-only writes: No update-in-place; new versions append, old versions garbage-collected</li> <li>No complex JOINs: Designed for single-table access patterns</li> </ul> <p>Popular implementations:</p> <ul> <li>Apache Cassandra: Peer-to-peer architecture, linear scalability, Netflix uses it for massive scale</li> <li>Google Bigtable: Powers Gmail, Search, Maps; handles exabytes of data</li> <li>Apache HBase: Built on Hadoop, strong consistency, batch + real-time processing</li> <li>ScyllaDB: Cassandra-compatible but written in C++ for higher performance</li> </ul> <p>Use cases:</p> <ul> <li>Time-series data: Sensor readings, logs, financial tick data, IoT events</li> <li>Event logging: Application logs, audit trails, user activity streams</li> <li>Recommendations: Product affinity, collaborative filtering data</li> <li>Social media feeds: Twitter timeline, Facebook news feed</li> <li>Financial services: Trading data, risk analytics, fraud detection</li> </ul> <p>Limitations:</p> <ul> <li>Limited query flexibility: Designed for access by row key; scanning without keys is slow</li> <li>No JOINs: Must denormalize and duplicate data across column families</li> <li>Complex data modeling: Requires deep understanding of access patterns upfront</li> <li>Eventual consistency: Most implementations use eventual consistency (though HBase offers strong consistency)</li> <li>Learning curve: Concepts like compaction, bloom filters, and read repair require expertise</li> </ul> <p>The trade-off: Wide-column stores achieve incredible scale (billions of rows, petabytes of data) by sacrificing query flexibility. They work brilliantly when you know your access patterns (lookup by key, scan ranges) but fail when you need ad-hoc queries or relationship traversals.</p>"},{"location":"chapters/02-database-systems-nosql/#graph-databases-first-class-relationships","title":"Graph Databases: First-Class Relationships","text":"<p>After key-value, document, and wide-column stores, you might wonder: \"Haven't we covered all the bases?\" Not even close. All three NoSQL types share a fundamental limitation\u2014they treat relationships as afterthoughts.</p> <p>Graph databases turn this paradigm on its head. Instead of storing disconnected documents or rows, graph databases make connections (edges/relationships) just as important as the entities (nodes) they connect. This isn't a minor difference\u2014it's a revolutionary architectural choice with profound performance implications.</p> <p>The graph model:</p> <ul> <li>Nodes (vertices): Entities like people, products, accounts, locations</li> <li>Edges (relationships): Connections like FRIEND_OF, PURCHASED, LOCATED_IN, DEPENDS_ON</li> <li>Properties: Both nodes and edges have attributes</li> <li>Labels: Categories for nodes (Person, Product) and relationships (types)</li> <li>Directionality: Relationships can be directional or bidirectional</li> </ul> <p>Example graph structure:</p> <pre><code>(Alice:Person {age: 28, city: \"Seattle\"})\n  -[:FRIEND_OF {since: 2018}]-&gt;\n(Bob:Person {age: 35, city: \"Portland\"})\n  -[:PURCHASED {date: \"2025-01-15\", price: 89.99}]-&gt;\n(Laptop:Product {brand: \"Dell\", model: \"XPS 15\"})\n  -[:MANUFACTURED_BY]-&gt;\n(Dell:Company {founded: 1984, country: \"USA\"})\n</code></pre> <p>This natural representation mirrors how we think about connected data: Alice is friends with Bob, who purchased a Laptop, which was manufactured by Dell.</p> <p>Why graph databases are different\u2014and why it matters:</p> <p>In Chapter 1, you saw the performance cliff when RDBMS systems attempt multi-hop queries. That same cliff appears with all other NoSQL types:</p> <ul> <li>Key-value stores: Can't traverse relationships at all</li> <li>Document databases: Reference documents by ID, requiring multiple round-trip queries</li> <li>Wide-column stores: Must denormalize relationships into columns, duplicating data</li> </ul> <p>Graph databases solve this through index-free adjacency: each node physically contains references (pointers) to its connected nodes. Traversing from Alice to Bob to Laptop to Dell is four O(1) pointer lookups\u2014constant time, regardless of database size.</p> <p>The performance numbers you need to know:</p> Operation RDBMS Document DB Graph DB 1-hop relationship 12ms 8ms 5ms 3-hop relationship 3,400ms 450ms 11ms 5-hop relationship 920,000ms (15 min) Timeout 18ms 7-hop relationship Not feasible Not feasible 25ms <p>This isn't optimization\u2014it's a fundamental architectural advantage. While RDBMS and document databases slow exponentially, graph databases maintain constant-time performance per hop.</p> <p>Use cases where graphs dominate:</p> <ul> <li>Social networks: Friend recommendations, influence analysis, community detection</li> <li>Fraud detection: Detecting rings of fraudulent accounts through connection patterns</li> <li>Recommendation engines: \"People who bought X also bought Y\" analyzed in real-time</li> <li>Knowledge graphs: Powering intelligent search and AI assistants</li> <li>Supply chain optimization: Multi-hop impact analysis, supplier risk assessment</li> <li>Network and IT management: Dependency analysis, blast radius calculation, root cause analysis</li> <li>Healthcare: Patient care pathways, drug interaction networks, disease spread modeling</li> </ul> <p>Popular graph database implementations:</p> <ul> <li>Neo4j: Most popular, property graph model, Cypher query language, ACID transactions</li> <li>TigerGraph: Distributed graph, real-time deep link analytics, GSQL language</li> <li>Amazon Neptune: Fully managed, supports both property graph (Gremlin) and RDF (SPARQL)</li> <li>ArangoDB: Multi-model (document + graph), flexible query language (AQL)</li> </ul> <p>Why graphs remained underutilized\u2014until now:</p> <p>Despite superior performance for connected data, graph databases haven't achieved the adoption they deserve. Why?</p> <ol> <li>RDBMS inertia: \"We've always used SQL databases\" is a powerful force</li> <li>Knowledge gap: Most developers learned RDBMS in school; graphs weren't taught</li> <li>Perceived complexity: Graph concepts seem harder than tables (they're actually simpler)</li> <li>Niche reputation: Graphs were seen as exotic tools for specialized problems</li> <li>Vendor fragmentation: Multiple competing standards (property graph vs RDF, different query languages)</li> </ol> <p>The competitive opportunity: While most companies remain stuck in table-thinking, those who recognize graph databases as general-purpose solutions for connected data are building competitive moats. Real-time fraud detection, instant recommendations, intelligent assistants\u2014these capabilities require graph architectures. You can't fake them with RDBMS or document databases at scale.</p>"},{"location":"chapters/02-database-systems-nosql/#tradeoff-analysis-choosing-the-right-tool","title":"Tradeoff Analysis: Choosing the Right Tool","text":"<p>The NoSQL revolution didn't eliminate RDBMS\u2014it fragmented the database landscape. Now you must choose from six fundamentally different approaches, each with different strengths, weaknesses, and ideal use cases.</p> <p>Here's the brutal truth: There is no \"best\" database. Only trade-offs. The database that powers Netflix's recommendation engine would be terrible for your bank account. The database managing your bank account would collapse under Netflix's scale.</p> <p>Understanding these trade-offs is strategic intelligence.</p>"},{"location":"chapters/02-database-systems-nosql/#the-nosql-trade-off-matrix","title":"The NoSQL Trade-off Matrix","text":"Database Type Strengths Weaknesses Best For Avoid For RDBMS ACID guarantees, mature tooling, strong consistency, complex queries, proven reliability Rigid schema, poor horizontal scaling, JOIN performance cliff, limited agility Transactional systems, financial records, systems requiring strong guarantees Massive scale, flexible schemas, deep relationship queries, rapid iteration Key-Value Blazing speed, simple scaling, minimal latency, perfect caching No queries, no relationships, no aggregations, limited consistency Session storage, caching, counters, shopping carts Anything requiring queries, relationships, or analytics Document Schema flexibility, developer productivity, natural object mapping, agile-friendly Weak relationships, data duplication, eventual consistency, difficult cross-document queries Content management, product catalogs, user profiles, rapid prototyping Heavily connected data, strong consistency requirements, complex relationships Wide-Column Massive scale, time-series optimization, column-oriented efficiency, write performance Complex modeling, limited queries, no JOINs, steep learning curve IoT data, event logs, time-series, massive-scale structured data Ad-hoc queries, relationship analysis, small datasets, unknown access patterns Graph Constant-time traversals, natural relationships, real-time deep queries, flexible schema Smaller ecosystem, fewer tools, learning curve for SQL developers, not ideal for bulk analytics Social networks, fraud detection, recommendations, knowledge graphs, network analysis Bulk data processing, simple key-value lookups, purely analytical workloads"},{"location":"chapters/02-database-systems-nosql/#decision-framework","title":"Decision Framework","text":"<p>When choosing a database, ask these questions in order:</p> <p>1. What are my relationships like?</p> <ul> <li>Few or no relationships: Consider key-value or document databases</li> <li>Moderate relationships (1-2 hops): RDBMS or document databases work</li> <li>Deep relationships (3+ hops), frequently queried: Graph database is the only performant choice</li> </ul> <p>2. What's my scale?</p> <ul> <li>Small to medium (&lt;1TB, &lt;100M rows): Any database works; choose based on other factors</li> <li>Large (1-100TB): Consider wide-column or distributed document databases</li> <li>Massive (&gt;100TB, billions of records): Wide-column stores or distributed graphs</li> </ul> <p>3. How structured is my data?</p> <ul> <li>Highly structured, stable schema: RDBMS excels</li> <li>Semi-structured, evolving schema: Document databases shine</li> <li>Completely unstructured: Key-value stores or document databases</li> </ul> <p>4. What are my consistency requirements?</p> <ul> <li>Strong consistency (financial transactions): RDBMS or Neo4j (ACID graph)</li> <li>Eventual consistency acceptable (social feeds): Most NoSQL types support this</li> <li>Flexible per-operation: Some databases (Cassandra, Cosmos DB) offer tunable consistency</li> </ul> <p>5. What's my query pattern?</p> <ul> <li>Known access patterns (lookup by key): Key-value or wide-column</li> <li>Ad-hoc queries, business intelligence: RDBMS or OLAP systems</li> <li>Relationship traversals: Graph databases</li> <li>Full-text search: Specialized search engines (Elasticsearch) or document databases with search features</li> </ul> <p>6. What's my team's expertise?</p> <ul> <li>Strong SQL skills, traditional development: Start with RDBMS, migrate when pain points emerge</li> <li>JavaScript/Node.js developers: Document databases (MongoDB)</li> <li>Data science/analytics team: RDBMS or OLAP systems</li> <li>Willingness to learn: Graph databases offer high ROI for relationship-heavy problems</li> </ul>"},{"location":"chapters/02-database-systems-nosql/#the-polyglot-persistence-strategy","title":"The Polyglot Persistence Strategy","text":"<p>Most sophisticated systems don't choose one database\u2014they use multiple, each for its strengths:</p> <p>Example: E-commerce platform</p> <ul> <li>RDBMS (PostgreSQL): Order transactions, inventory management (ACID guarantees)</li> <li>Document DB (MongoDB): Product catalog (flexible attributes)</li> <li>Key-Value (Redis): Session storage, shopping carts (speed)</li> <li>Graph DB (Neo4j): Product recommendations, customer segmentation (relationships)</li> <li>Search Engine (Elasticsearch): Product search, faceted filtering (full-text search)</li> </ul> <p>Each database handles what it does best. The application coordinates between them.</p> <p>The competitive edge: Companies using polyglot persistence match tools to problems. Those using RDBMS for everything compromise on performance, cost, and agility. Those using only NoSQL lose ACID guarantees where they matter. Strategic database selection is a competitive advantage.</p>"},{"location":"chapters/02-database-systems-nosql/#the-cap-theorem-understanding-fundamental-constraints","title":"The CAP Theorem: Understanding Fundamental Constraints","text":"<p>In 2000, computer scientist Eric Brewer proposed a theorem that explains why distributed databases make the trade-offs they do. The CAP theorem states that in any distributed system, you can achieve at most two of three guarantees:</p> <ul> <li>Consistency: Every read receives the most recent write (all nodes see the same data)</li> <li>Availability: Every request receives a response (even if some nodes are down)</li> <li>Partition Tolerance: The system continues operating despite network failures (nodes can't communicate)</li> </ul> <p>Why this matters: In real-world distributed systems, network partitions are inevitable\u2014servers fail, cables break, data centers lose connectivity. So partition tolerance isn't optional. The real choice is between consistency and availability.</p> CAP Theorem Visualization     Type: diagram      Purpose: Illustrate the CAP theorem triangle showing the trade-off between Consistency, Availability, and Partition Tolerance      Components:     - Triangle with three vertices labeled C (Consistency), A (Availability), P (Partition Tolerance)     - Three edges connecting the vertices, each representing a two-property combination:       * CA edge (top): \"Traditional RDBMS\" (sacrifices partition tolerance)       * CP edge (left): \"MongoDB, HBase\" (sacrifices availability)       * AP edge (right): \"Cassandra, DynamoDB\" (sacrifices consistency)     - Center annotation: \"Choose 2 of 3\"     - Note below: \"In distributed systems, P is required, so real choice is C or A\"      Visual elements:     - Each vertex should be a colored circle (C=blue, A=green, P=orange)     - Each edge should be labeled with example databases making that trade-off     - Use dotted line from P to center to indicate P is non-negotiable     - Add database logos or icons along edges      Labels:     - C (Consistency): \"All nodes see same data\"     - A (Availability): \"System always responds\"     - P (Partition Tolerance): \"Works despite network failures\"      Color scheme:     - Blue for Consistency     - Green for Availability     - Orange for Partition Tolerance     - Gray for the \"impossible region\" in center      Style: Triangle diagram with clear labels and examples      Implementation: SVG diagram with interactive hover states showing trade-off descriptions"},{"location":"chapters/02-database-systems-nosql/#cp-systems-consistency-partition-tolerance","title":"CP Systems: Consistency + Partition Tolerance","text":"<p>Examples: MongoDB (default config), HBase, Neo4j, traditional RDBMS (with caveats)</p> <p>Trade-off: During network partitions, some nodes become unavailable to maintain consistency.</p> <p>When to choose:</p> <ul> <li>Financial transactions: Account balances must be correct</li> <li>Inventory management: Can't sell what you don't have</li> <li>Healthcare records: Patient data must be accurate</li> <li>Any system where correctness trumps availability</li> </ul> <p>What happens during partition: If a node can't guarantee it has the latest data, it refuses to respond until partition heals.</p>"},{"location":"chapters/02-database-systems-nosql/#ap-systems-availability-partition-tolerance","title":"AP Systems: Availability + Partition Tolerance","text":"<p>Examples: Cassandra, DynamoDB, Riak, Cosmos DB (in eventual consistency mode)</p> <p>Trade-off: System always responds, but different nodes might return different data temporarily (eventual consistency).</p> <p>When to choose:</p> <ul> <li>Social media feeds: Slight delays in seeing latest posts acceptable</li> <li>Product catalogs: Outdated price for seconds doesn't matter</li> <li>Caching systems: Stale data better than no data</li> <li>High availability requirements: 99.999% uptime critical</li> </ul> <p>What happens during partition: All nodes continue serving requests, accepting that data might be slightly out of sync. Eventually (usually within seconds), all nodes converge to the same state.</p>"},{"location":"chapters/02-database-systems-nosql/#ca-systems-consistency-availability","title":"CA Systems: Consistency + Availability","text":"<p>Examples: Traditional RDBMS on a single server</p> <p>Trade-off: No partition tolerance\u2014system fails if network problems occur.</p> <p>When to choose: You usually don't in modern systems. CA systems only work in non-distributed environments (single server or tightly coupled cluster without network partitions).</p> <p>Reality check: If you need distributed systems (scale, redundancy, geographic distribution), CA isn't achievable. Choose CP or AP.</p>"},{"location":"chapters/02-database-systems-nosql/#beyond-cap-pacelc","title":"Beyond CAP: PACELC","text":"<p>The CAP theorem was later refined into the PACELC theorem, which states:</p> <p>If there's a Partition (P), choose between Availability (A) and Consistency (C), Else (E) during normal operation, choose between Latency (L) and Consistency (C).</p> <p>This adds the insight that even when everything's working (no partition), you face a trade-off: Do you wait for all nodes to confirm writes (consistency) at the cost of higher latency, or do you respond quickly (low latency) and accept eventual consistency?</p> <p>Examples:</p> <ul> <li>Neo4j (PC/EC): Chooses consistency in both scenarios (ACID transactions even in distributed mode)</li> <li>Cassandra (PA/EL): Chooses availability and low latency; consistency is eventual</li> <li>MongoDB (tunable): Can configure for PC/EC or PA/EL depending on requirements</li> </ul> <p>The strategic insight: Understanding CAP helps you ask the right questions. \"Should we use Database X?\" isn't answerable without knowing \"What happens during failures?\" and \"How important is consistency?\" These trade-offs aren't bugs\u2014they're fundamental constraints of distributed systems.</p>"},{"location":"chapters/02-database-systems-nosql/#the-graph-database-advantage-in-context","title":"The Graph Database Advantage in Context","text":"<p>After surveying the entire database landscape, the graph database value proposition becomes clear:</p> <p>Graph databases occupy a unique position: They solve problems that other NoSQL databases can't (deep relationship queries) while maintaining guarantees that NoSQL typically abandons (ACID transactions, consistency).</p> <p>Compare graph databases to the alternatives for a relationship-heavy use case (fraud detection ring analysis):</p> <p>RDBMS Approach:</p> <pre><code>-- Find accounts 3 hops from suspicious account\nSELECT DISTINCT a3.*\nFROM accounts a1\nJOIN transactions t1 ON a1.id = t1.from_account\nJOIN accounts a2 ON t1.to_account = a2.id\nJOIN transactions t2 ON a2.id = t2.from_account\nJOIN accounts a3 ON t2.to_account = a3.id\nWHERE a1.id = 'suspicious_acct';\n</code></pre> <p>Performance: 3,400ms (3.4 seconds) for 3 hops, unusable at 5+ hops Feasibility: Batch processing only</p> <p>Document Database Approach:</p> <pre><code>// Pseudo-code, requires multiple queries\nlet accounts1 = db.accounts.find({_id: 'suspicious_acct'});\nlet transactions1 = db.transactions.find({from: 'suspicious_acct'});\nlet accountIds2 = transactions1.map(t =&gt; t.to);\nlet transactions2 = db.transactions.find({from: {$in: accountIds2}});\nlet accountIds3 = transactions2.map(t =&gt; t.to);\nlet accounts3 = db.accounts.find({_id: {$in: accountIds3}});\n</code></pre> <p>Performance: 450ms for 3 hops, network round-trips add latency Feasibility: Limited depth, caching required</p> <p>Graph Database Approach:</p> <pre><code>// Neo4j Cypher\nMATCH (a1:Account {id: 'suspicious_acct'})\n      -[:TRANSFERRED_TO*1..3]-&gt;\n      (a3:Account)\nRETURN DISTINCT a3;\n</code></pre> <p>Performance: 11ms for 3 hops, 18ms for 5 hops, 25ms for 7 hops Feasibility: Real-time analysis at any depth</p> <p>The bottom line: For relationship-intensive queries, graph databases don't just perform better\u2014they enable capabilities that are impossible with other technologies. You can't optimize RDBMS or document databases to match graph performance at 5+ hops. The architecture simply doesn't support it.</p>"},{"location":"chapters/02-database-systems-nosql/#competitive-intelligence-whos-winning-with-graphs","title":"Competitive Intelligence: Who's Winning with Graphs","text":"<p>While most companies remain stuck in RDBMS-thinking or adopted document databases because they're \"modern,\" forward-thinking organizations identified graph databases as strategic weapons:</p> <p>Companies openly using graph databases:</p> <ul> <li>LinkedIn: Social graph, job recommendations, skills graph</li> <li>eBay: Shipping logistics, delivery estimates across global network</li> <li>Walmart: Supply chain optimization, product recommendations</li> <li>NASA: Lessons learned knowledge graph, spacecraft dependency analysis</li> <li>Airbnb: Fraud detection, knowledge graph for search</li> <li>Cisco: Network configuration management, security analysis</li> <li>UBS: Compliance, know-your-customer (KYC), anti-money laundering (AML)</li> <li>Marriott: Customer loyalty programs, personalized recommendations</li> </ul> <p>What they're not saying publicly: These companies didn't adopt graphs because they're trendy. They adopted them because relationship analysis became a competitive advantage, and no other database could deliver real-time performance.</p> <p>Your opportunity: While your competitors struggle with overnight batch processes for multi-hop queries, you could be analyzing relationships in real-time. While they simplify business logic to avoid JOIN performance, you could model reality accurately and query it naturally. While they cobble together multiple databases to work around relationship limitations, you could use a single graph database optimized for your actual use case.</p> <p>The adoption gap is your advantage\u2014if you move now.</p>"},{"location":"chapters/02-database-systems-nosql/#key-takeaways","title":"Key Takeaways","text":"<p>This chapter established the database landscape and why graph databases represent the culmination of decades of evolution:</p> <ol> <li>RDBMS dominated for 50 years but hits fundamental performance and flexibility limits with modern workloads</li> <li>OLTP and OLAP require different optimizations, leading to specialized database systems</li> <li>The NoSQL revolution delivered four categories, each optimized for specific use cases:</li> <li>Key-value stores: Speed through simplicity</li> <li>Document databases: Flexible schemas for agile development</li> <li>Wide-column stores: Massive-scale structured data</li> <li>Graph databases: First-class relationships</li> <li>All databases make trade-offs\u2014no single solution is \"best\" for everything</li> <li>CAP theorem explains fundamental constraints in distributed systems: choose consistency or availability (partition tolerance isn't optional)</li> <li>Graph databases uniquely combine strengths: Relationship performance + ACID guarantees + flexible schema</li> <li>The graph advantage compounds: 51,000\u00d7 faster at 5 hops isn't incremental improvement; it's a different category of capability</li> <li>Strategic database selection creates competitive advantage\u2014using the right tool for each problem</li> </ol> <p>Most importantly: Graph databases solve a class of problems that no other database can address efficiently. This isn't about fashion or trends. It's about matching your data's natural structure (connected entities with relationships) to a database architecture optimized for that structure.</p> <p>In the next chapter, we'll dive deep into the Labeled Property Graph model that makes graph databases so powerful, revealing the elegant simplicity beneath their revolutionary performance.</p> <p>The database you choose isn't just a technical decision\u2014it's a strategic one that determines which problems you can solve, how fast you can iterate, and whether you can build the real-time, intelligent capabilities customers now expect.</p>"},{"location":"chapters/03-labeled-property-graph-model/","title":"Labeled Property Graph Information Model","text":""},{"location":"chapters/03-labeled-property-graph-model/#summary","title":"Summary","text":"<p>This chapter introduces the Labeled Property Graph (LPG) information model, the foundation of modern graph databases. You'll learn how nodes, edges, properties, and labels work together to create expressive, flexible data models where relationships are first-class citizens. The chapter covers both schema-optional and schema-enforced approaches, explores index-free adjacency for performance, and introduces fundamental graph operations including traversal, pattern matching, and multi-hop queries.</p>"},{"location":"chapters/03-labeled-property-graph-model/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 23 concepts from the learning graph:</p> <ol> <li>Labeled Property Graph</li> <li>Nodes</li> <li>Edges</li> <li>Properties</li> <li>Labels</li> <li>Schema-Optional Modeling</li> <li>Schema-Enforced Modeling</li> <li>Index-Free Adjacency</li> <li>Traversal</li> <li>Graph Query</li> <li>Pattern Matching</li> <li>Multi-Hop Queries</li> <li>Aggregation</li> <li>Path Patterns</li> <li>Constant-Time Neighbor Access</li> <li>First-Class Relationships</li> <li>Edge Direction</li> <li>Graph Data Model</li> <li>Graph Schema</li> <li>Metadata Representation</li> <li>Graph Validation</li> <li>Document Validation</li> <li>Rule Systems</li> </ol>"},{"location":"chapters/03-labeled-property-graph-model/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Introduction to Graph Thinking and Data Modeling</li> <li>Chapter 2: Database Systems and NoSQL</li> </ul>"},{"location":"chapters/03-labeled-property-graph-model/#welcome-to-graph-land-where-everything-connects","title":"Welcome to Graph Land: Where Everything Connects","text":"<p>Okay, deep breath. This chapter introduces a bunch of new concepts that might feel weird at first\u2014and that's totally normal! If you've spent any time with traditional databases (or even if you haven't), the Labeled Property Graph model is going to ask you to think differently about data. Some of these ideas will click immediately, others might take a few read-throughs. That's not just okay\u2014it's expected.</p> <p>Here's the good news: the concepts we're covering aren't actually complicated; they're just different. And once they click (which they will, with a bit of repetition), you'll wonder why anyone ever thought tables were a good way to represent connected information.</p> <p>We're going to introduce a lot of vocabulary in this chapter\u201423 concepts to be exact. Don't panic! Many of them build on each other, and we'll revisit the same ideas multiple times from different angles. By the end, terms like \"index-free adjacency\" and \"first-class relationships\" will feel as natural as \"nodes\" and \"edges.\"</p> <p>Ready? Let's dive in. And remember: if something doesn't make sense the first time, keep reading. It will.</p>"},{"location":"chapters/03-labeled-property-graph-model/#the-labeled-property-graph-the-whole-enchilada","title":"The Labeled Property Graph: The Whole Enchilada","text":"<p>Let's start with the big picture. A Labeled Property Graph (LPG) is the data model that most modern graph databases use. Think of it as the \"rules of the game\" for how information gets structured and stored.</p> <p>Don't worry about memorizing that definition. What matters is understanding the four building blocks that make up an LPG:</p> <ol> <li>Nodes - The \"things\" in your data (people, products, accounts, locations)</li> <li>Edges - The connections between things (relationships like FRIEND_OF, PURCHASED, DEPENDS_ON)</li> <li>Properties - The attributes or details (name, age, price, date)</li> <li>Labels - The categories that organize nodes and edges (Person, Product, PURCHASED)</li> </ol> <p>Here's the simplest possible graph to illustrate:</p> <pre><code>(Alice:Person {age: 28})\n    -[:FRIEND_OF {since: 2020}]-&gt;\n(Bob:Person {age: 35})\n</code></pre> <p>Let's break this down piece by piece: - <code>Alice</code> is a node (a thing that exists) - <code>:Person</code> is a label (Alice belongs to the \"Person\" category) - <code>{age: 28}</code> is a property (a detail about Alice) - <code>-[:FRIEND_OF {since: 2020}]-&gt;</code> is an edge (a connection from Alice to Bob) - <code>:FRIEND_OF</code> is the edge's label (the type of relationship) - <code>{since: 2020}</code> is the edge's property (when they became friends) - The arrow <code>-&gt;</code> shows direction (Alice is friends with Bob)</p> <p>If that feels like a lot, don't stress. We're going to explore each piece in detail, and you'll see the same concepts repeated in different examples. By the tenth example, this notation will feel completely natural.</p>"},{"location":"chapters/03-labeled-property-graph-model/#nodes-the-stars-of-the-show","title":"Nodes: The Stars of the Show","text":"<p>Nodes (also called vertices if you want to sound fancy) represent the entities in your graph\u2014the people, places, things, concepts, or events that you care about. If your graph is a social network, nodes are users. If it's a supply chain, nodes are products, warehouses, and vendors. If it's a knowledge graph, nodes are concepts and facts.</p> <p>Think of nodes as the nouns in your data's story.</p> <p>What makes a node?</p> <ol> <li>A unique identity: Every node is distinct, even if two nodes have the same properties</li> <li>Optional properties: Nodes can have attributes (name, age, email) or none at all</li> <li>Optional labels: Nodes can belong to one or more categories</li> <li>Connections: Nodes are connected to other nodes via edges</li> </ol> <p>Let's look at some concrete examples:</p> <p>Social network nodes: <pre><code>(user1:Person {name: \"Alice\", email: \"alice@example.com\", joined: \"2020-01-15\"})\n(user2:Person {name: \"Bob\", email: \"bob@example.com\", joined: \"2019-06-22\"})\n(post1:Post {content: \"Loving graph databases!\", timestamp: \"2025-01-18T10:30:00\"})\n</code></pre></p> <p>E-commerce nodes: <pre><code>(product1:Product {name: \"Laptop\", price: 899.99, sku: \"LAP-001\"})\n(category1:Category {name: \"Electronics\", description: \"Electronic devices\"})\n(vendor1:Vendor {name: \"TechCorp\", country: \"USA\"})\n</code></pre></p> <p>Notice a few things: - Different nodes can have completely different properties - Node identifiers (user1, product1) are internal; the actual data is in the properties - Labels (Person, Product, Category) help organize nodes into types</p> <p>The abstract concept: A node represents a discrete entity in your domain. Each node is a standalone unit of information that can be connected to other units.</p> <p>The concrete analogy: Think of nodes like index cards in a massive library card catalog. Each card represents one distinct thing\u2014a book, an author, a subject. The card might have details written on it (properties), and it might be filed in multiple categories (labels). But fundamentally, it's a single, identifiable item.</p> <p>(See? Same concept, two different explanations. This is that repetition thing we mentioned!)</p>"},{"location":"chapters/03-labeled-property-graph-model/#edges-where-the-magic-happens","title":"Edges: Where the Magic Happens","text":"<p>Here's where graph databases get interesting. Edges (also called relationships or links) connect nodes to represent how things relate to each other. Unlike traditional databases where relationships are implied through foreign keys and JOIN operations, edges are first-class citizens\u2014they're real, tangible things you can see, query, and give properties to.</p> <p>This is kind of a big deal. Let me say it again: relationships in a graph database are first-class citizens. They're not hidden in junction tables or implied by matching IDs. They're explicit, named, directional connections with their own identity and attributes.</p> <p>What makes an edge?</p> <ol> <li>Two nodes: An edge connects exactly two nodes (a \"from\" node and a \"to\" node)</li> <li>Direction: Edges point from one node to another (though you can traverse them in either direction)</li> <li>A type/label: Every edge has a name describing the relationship (FRIEND_OF, PURCHASED, DEPENDS_ON)</li> <li>Optional properties: Edges can have attributes just like nodes</li> </ol> <p>Social network edges: <pre><code>(Alice)-[:FRIEND_OF {since: \"2020-05-12\", closeness: 0.8}]-&gt;(Bob)\n(Alice)-[:POSTED {timestamp: \"2025-01-18T10:30:00\"}]-&gt;(post1)\n(Bob)-[:LIKED {timestamp: \"2025-01-18T10:35:00\"}]-&gt;(post1)\n</code></pre></p> <p>E-commerce edges: <pre><code>(Alice)-[:PURCHASED {date: \"2025-01-15\", quantity: 1, price: 899.99}]-&gt;(product1)\n(product1)-[:IN_CATEGORY]-&gt;(category1)\n(product1)-[:MANUFACTURED_BY]-&gt;(vendor1)\n</code></pre></p> <p>Do you see what's happening? The edges aren't just connections\u2014they tell a story. Alice became friends with Bob on a specific date. Alice purchased a product on a specific date for a specific price. Each edge carries meaning and context.</p> <p>First-Class Relationships: Why This Matters</p> <p>When we say relationships are first-class, we mean they're treated as important as the entities they connect. In a relational database, you'd have:</p> <pre><code>-- Separate tables, relationships implied by foreign keys\nUsers table: (id, name, email)\nFriendships table: (user1_id, user2_id, since)\n\n-- To find friends, you JOIN tables\nSELECT u2.*\nFROM Users u1\nJOIN Friendships f ON u1.id = f.user1_id\nJOIN Users u2 ON f.user2_id = u2.id\nWHERE u1.name = 'Alice';\n</code></pre> <p>In a graph database, you have:</p> <pre><code>// Relationships are explicit, queryable entities\nMATCH (alice:Person {name: 'Alice'})-[:FRIEND_OF]-&gt;(friend)\nRETURN friend;\n</code></pre> <p>The difference isn't just syntax\u2014it's architectural. Graph databases store edges as physical pointers between nodes, making traversal instant. We'll talk more about why that matters when we get to \"index-free adjacency\" (don't worry, we'll explain that one!).</p> <p>Edge Direction: Following the Arrows</p> <p>Every edge has a direction, shown by the arrow in our notation: <code>-&gt;</code>. But here's the cool part: you can traverse an edge in either direction when querying, regardless of how it's stored.</p> <pre><code>(Alice)-[:FRIEND_OF]-&gt;(Bob)  // Stored with direction\n</code></pre> <p>You can query: - \"Who are Alice's friends?\" (follow the arrow forward) - \"Who is friends with Bob?\" (follow the arrow backward) - \"Are Alice and Bob friends?\" (check for a connection in either direction)</p> <p>Direction matters semantically (Alice purchased a product is different from a product purchased Alice), but query-wise, you're flexible. This will make more sense when we get to actual queries later in the chapter.</p> <p>The abstract concept: Edges reify relationships\u2014they make connections between entities tangible, queryable, and capable of carrying their own information.</p> <p>The concrete analogy: Think of edges like labeled arrows drawn between items on a whiteboard. If you're diagramming your company's org chart, edges are the lines connecting \"Manager\" to \"Employee\" with labels like \"MANAGES\" or \"REPORTS_TO.\" Each line is a real thing you can point to and say \"this specific connection exists and has these properties.\"</p>"},{"location":"chapters/03-labeled-property-graph-model/#properties-the-details-that-matter","title":"Properties: The Details That Matter","text":"<p>Both nodes and edges can have properties\u2014key-value pairs that store actual data about the entity or relationship. Properties are how you record specific details like names, dates, prices, or any other attribute you care about.</p> <p>Node properties: <pre><code>(Alice:Person {\n  name: \"Alice Johnson\",\n  email: \"alice@example.com\",\n  age: 28,\n  city: \"Seattle\",\n  premium: true\n})\n</code></pre></p> <p>Edge properties: <pre><code>(Alice)-[:PURCHASED {\n  date: \"2025-01-15\",\n  price: 899.99,\n  quantity: 1,\n  payment_method: \"credit_card\"\n}]-&gt;(laptop)\n</code></pre></p> <p>Properties are simple key-value pairs. The key is a string (like \"name\" or \"price\"), and the value can be a string, number, boolean, date, or even a list of values.</p> <p>Different nodes can have different properties:</p> <pre><code>(Alice:Person {name: \"Alice\", age: 28, city: \"Seattle\"})\n(Bob:Person {name: \"Bob\", age: 35, occupation: \"Engineer\"})\n(Charlie:Person {name: \"Charlie\", email: \"charlie@example.com\"})\n</code></pre> <p>Notice how: - Alice has age and city - Bob has age and occupation - Charlie has email but not age or city</p> <p>This is totally fine! One person can have different attributes than another person. This flexibility is part of what makes graphs powerful\u2014you don't need to define every possible field upfront or fill in NULL values for missing data.</p> <p>The abstract concept: Properties add specificity to otherwise generic entities and relationships. They transform \"a person\" into \"Alice Johnson, age 28, from Seattle.\"</p> <p>The concrete analogy: Properties are like the information you'd write on a business card or profile. The card is the node (the physical thing representing you), and the properties are all the details printed on it\u2014name, title, phone number, email, LinkedIn URL. Different cards might have different details depending on context.</p>"},{"location":"chapters/03-labeled-property-graph-model/#labels-organizing-the-chaos","title":"Labels: Organizing the Chaos","text":"<p>Labels are categories or types that help organize nodes and edges. Think of labels as tags that say \"this is a Person\" or \"this is a Product\" or \"this is a FRIEND_OF relationship.\"</p> <p>Nodes and edges can have: - No labels (though this is rare and usually not useful) - One label (most common) - Multiple labels (for things that fit multiple categories)</p> <p>Node labels: <pre><code>(alice:Person)                          // One label\n(bob:Person:Employee)                    // Multiple labels\n(acmeWidget:Product:PhysicalGood)        // Multiple labels\n</code></pre></p> <p>Bob is both a Person AND an Employee. That widget is both a Product AND a PhysicalGood. Labels let you query \"give me all Employees\" or \"give me all PhysicalGoods\" without caring about their other labels.</p> <p>Edge labels (relationship types): <pre><code>(Alice)-[:FRIEND_OF]-&gt;(Bob)\n(Alice)-[:PURCHASED]-&gt;(laptop)\n(Alice)-[:WORKS_AT]-&gt;(company)\n</code></pre></p> <p>Edge labels (the part after the colon in the square brackets) describe the type of relationship. Unlike node labels, edges typically have just one label/type because a relationship usually represents one specific kind of connection.</p> <p>Why labels matter:</p> <ol> <li>Querying: You can filter by label (\"find all Person nodes\" or \"find all PURCHASED edges\")</li> <li>Performance: Databases can index by label for faster lookups</li> <li>Semantics: Labels make your graph self-documenting\u2014you can look at the structure and understand what everything means</li> </ol> <p>The abstract concept: Labels provide categorical information that transcends individual properties. They answer \"what kind of thing is this?\" rather than \"what specific details does this have?\"</p> <p>The concrete analogy: Labels are like the sections in a library\u2014Fiction, Non-Fiction, Reference, Children's. A book might belong to multiple sections (a graphic novel might be both Fiction and Children's), but the labels help you navigate and filter the collection.</p> <p>Okay, pause. How are we doing? We've covered the four fundamental building blocks of a Labeled Property Graph: 1. \u2705 Nodes (entities) 2. \u2705 Edges (relationships) 3. \u2705 Properties (attributes) 4. \u2705 Labels (categories)</p> <p>If those don't feel 100% solid yet, don't worry. We're going to see them in action throughout the rest of this chapter, and the repetition will solidify the concepts. Let's keep going!</p>"},{"location":"chapters/03-labeled-property-graph-model/#putting-it-all-together-the-graph-data-model","title":"Putting It All Together: The Graph Data Model","text":"<p>Now that we've met all the pieces, let's see how they combine into a complete graph data model. A graph data model describes the structure of your data\u2014what types of nodes exist, what types of edges connect them, what properties each might have.</p> <p>Here's a small social network graph model:</p> <p>Node types: - Person: Properties: name, email, age, city - Post: Properties: content, timestamp, likes_count - Company: Properties: name, industry, founded</p> <p>Edge types: - FRIEND_OF: Connects Person to Person, properties: since, closeness - POSTED: Connects Person to Post, properties: timestamp - LIKED: Connects Person to Post, properties: timestamp - WORKS_AT: Connects Person to Company, properties: title, start_date</p> <p>Example data following this model:</p> <pre><code>(alice:Person {name: \"Alice\", age: 28, city: \"Seattle\"})\n(bob:Person {name: \"Bob\", age: 35, city: \"Portland\"})\n(post1:Post {content: \"Graph databases are cool!\", timestamp: \"2025-01-18T10:00:00\"})\n(techcorp:Company {name: \"TechCorp\", industry: \"Software\", founded: 1995})\n\n(alice)-[:FRIEND_OF {since: \"2020-05-12\"}]-&gt;(bob)\n(alice)-[:POSTED {timestamp: \"2025-01-18T10:00:00\"}]-&gt;(post1)\n(bob)-[:LIKED {timestamp: \"2025-01-18T10:05:00\"}]-&gt;(post1)\n(alice)-[:WORKS_AT {title: \"Engineer\", start_date: \"2022-03-01\"}]-&gt;(techcorp)\n(bob)-[:WORKS_AT {title: \"Manager\", start_date: \"2019-06-15\"}]-&gt;(techcorp)\n</code></pre> <p>This data model is flexible but structured. You know what types of things can exist and how they can connect, but you have freedom within those constraints. Alice doesn't need to have exactly the same properties as Bob, and you can add new nodes and edges that follow the model as your data grows.</p> <p>The beauty of the graph data model: It mirrors how you think about the world. You don't think \"Alice is row 42 in the Users table with a foreign key to row 17 in the Posts table.\" You think \"Alice is a person who posted something and works at TechCorp.\" The graph model reflects that natural mental model.</p>"},{"location":"chapters/03-labeled-property-graph-model/#schema-optional-vs-schema-enforced-choose-your-adventure","title":"Schema-Optional vs. Schema-Enforced: Choose Your Adventure","text":"<p>Here's where graph databases get really flexible. You can choose between two approaches:</p>"},{"location":"chapters/03-labeled-property-graph-model/#schema-optional-modeling","title":"Schema-Optional Modeling","text":"<p>Schema-optional (sometimes called schema-free or schema-less) means you don't define structure upfront. You just start creating nodes and edges, and the graph adapts to whatever you throw at it.</p> <p>Pros: - Agile development: Start coding immediately, figure out structure as you go - Evolutionary design: Add new properties or node types without migrations - Heterogeneous data: Different nodes of the same label can have different properties - Fast iteration: Change your mind? Just start storing different fields!</p> <p>Cons: - Inconsistency risk: Nothing prevents typos (is it \"email\" or \"e-mail\"?) - Data quality issues: No guarantee all Person nodes have required fields - Application complexity: Your code must handle missing or unexpected properties - Harder to document: What properties should a Person have?</p> <p>Example: <pre><code>// No schema defined, just create whatever you want\nCREATE (alice:Person {name: \"Alice\", age: 28, email: \"alice@example.com\"})\nCREATE (bob:Person {name: \"Bob\", occupation: \"Engineer\"})  // Different properties!\nCREATE (charlie:Person {name: \"Charlie\", age: \"thirty\"})   // Age is a string! (Probably a mistake)\n</code></pre></p>"},{"location":"chapters/03-labeled-property-graph-model/#schema-enforced-modeling","title":"Schema-Enforced Modeling","text":"<p>Schema-enforced (sometimes called schema-constrained) means you define rules upfront: what properties are required, what types they must be, what edges are allowed. The database enforces these rules.</p> <p>Pros: - Data quality: Required fields must be present, types must match - Consistency: All Person nodes have the same structure - Documentation: Schema serves as spec for what data looks like - Validation: Errors caught on write, not discovered later during queries</p> <p>Cons: - Upfront design: Must think through structure before coding - Migration overhead: Changing schema requires careful planning - Less flexibility: Can't easily add one-off properties - Some databases lack support: Not all graph databases offer schema enforcement</p> <p>Example: <pre><code>// Schema definition (pseudo-code, syntax varies by database)\nDEFINE NODE Person {\n  PROPERTIES {\n    name: STRING (required),\n    age: INTEGER (required, min: 0, max: 150),\n    email: STRING (required, format: email)\n  }\n}\n\n// Now creation follows schema\nCREATE (alice:Person {name: \"Alice\", age: 28, email: \"alice@example.com\"})  // \u2705 Valid\nCREATE (bob:Person {name: \"Bob\"})  // \u274c Error: missing required properties\nCREATE (charlie:Person {name: \"Charlie\", age: \"thirty\"})  // \u274c Error: age must be INTEGER\n</code></pre></p>"},{"location":"chapters/03-labeled-property-graph-model/#which-approach-should-you-use","title":"Which Approach Should You Use?","text":"<p>It depends on your use case:</p> <ul> <li>Schema-optional: Prototyping, evolving domains, heterogeneous data, document-like flexibility</li> <li>Schema-enforced: Production systems, financial data, regulated industries, team coordination</li> </ul> <p>Many graph databases let you mix approaches\u2014enforce schemas for critical data (Person must have name and email) while allowing flexibility elsewhere (Person can optionally have any additional properties).</p> <p>The key takeaway: graph databases give you the choice. RDBMS forces schema-first; document databases force schema-less; graphs let you decide what makes sense for your data.</p>"},{"location":"chapters/03-labeled-property-graph-model/#index-free-adjacency-the-performance-secret","title":"Index-Free Adjacency: The Performance Secret","text":"<p>Okay, this is where things get a bit technical, but stick with me\u2014this concept explains why graph databases are so fast at traversing relationships.</p> <p>Index-free adjacency means that each node physically stores references (pointers) to its directly connected nodes. When you ask \"What are Alice's friends?\", the database doesn't search an index or scan a table\u2014it follows the pointers stored in Alice's node directly to Bob, Charlie, and Diana.</p> <p>How it works (simplified):</p> <p>Imagine each node is a filing cabinet drawer. Inside Alice's drawer, there are literal pointers (references) to all the nodes she's connected to: - FRIEND_OF \u2192 [pointer to Bob, pointer to Charlie, pointer to Diana] - WORKS_AT \u2192 [pointer to TechCorp] - POSTED \u2192 [pointer to Post1, pointer to Post2]</p> <p>When you query \"Who are Alice's friends?\", the database: 1. Finds Alice's node (one index lookup: O(log n)) 2. Looks inside Alice's node at the FRIEND_OF pointers 3. Follows those pointers directly to Bob, Charlie, Diana (three pointer dereferences: O(1) each)</p> <p>Total time: O(log n) + O(3) \u2248 constant time for practical purposes, regardless of how many total nodes exist in the database.</p> <p>Contrast with RDBMS (index-based):</p> <p>In a relational database: 1. Find Alice in Users table (index lookup: O(log n)) 2. Scan Friendships table for all rows where user1_id = Alice's ID (full table scan or index range scan: O(m) where m = number of friendships) 3. For each friendship, look up the friend in Users table (m index lookups: O(m log n))</p> <p>As the number of friendships grows, performance degrades linearly or worse.</p> <p>Why \"index-free\"?</p> <p>Traditional databases use indexes\u2014separate data structures that map values to locations. Finding Alice's friends requires looking up Alice's ID in a Friendships index, then following those references.</p> <p>Graph databases don't need this intermediate step. The adjacency information is built into the node itself. Hence: index-free adjacency.</p> <p>The abstract concept: By storing adjacency information (which nodes connect to which) directly in the nodes themselves, graph databases achieve constant-time neighbor access.</p> <p>The concrete analogy: Imagine a massive hotel where every room has a list pinned to its door of all rooms it's connected to via hallways. If you're in room 42 and want to know which rooms you can reach directly, you just read the list on your door\u2014instant answer. You don't need to consult a central directory or search floor plans. That's index-free adjacency.</p>"},{"location":"chapters/03-labeled-property-graph-model/#constant-time-neighbor-access","title":"Constant-Time Neighbor Access","text":"<p>This is the payoff of index-free adjacency. Constant-time neighbor access means that finding a node's direct neighbors takes the same amount of time regardless of: - How many total nodes exist in the graph - How many total edges exist in the graph - How connected other parts of the graph are</p> <p>Whether your graph has 100 nodes or 100 million nodes, finding Alice's friends takes the same amount of time: find Alice (O(log n)), follow pointers (O(1) per friend).</p> <p>This is why graph databases can handle multi-hop queries efficiently\u2014each hop is constant-time, so three hops is 3\u00d7 constant time, not exponential like with JOINs.</p> <p>(Yes, we're repeating the performance story from earlier chapters. That's intentional! Repetition with new context helps solidify understanding.)</p>"},{"location":"chapters/03-labeled-property-graph-model/#traversal-walking-the-graph","title":"Traversal: Walking the Graph","text":"<p>Now that we know how graphs are structured and why they're fast, let's talk about actually using them. Traversal is the process of walking through a graph, following edges from node to node.</p> <p>Think of traversal like following a trail through a forest, where each node is a waypoint and each edge is a path connecting waypoints. You start at one node and follow edges to reach other nodes.</p> <p>Simple traversal example: <pre><code>Start at Alice\n\u2192 Follow FRIEND_OF edge to Bob\n\u2192 Follow FRIEND_OF edge from Bob to Charlie\n\u2192 Follow WORKS_AT edge from Charlie to TechCorp\n</code></pre></p> <p>Traversals can be: - Single-hop: Follow one edge (Alice's direct friends) - Multi-hop: Follow multiple edges (friends of Alice's friends) - Filtered: Only follow certain edge types (FRIEND_OF but not WORKS_AT) - Conditional: Follow edges only if they meet criteria (friendships since 2020) - Depth-limited: Stop after N hops - Shortest path: Find the shortest route between two nodes</p> <p>Traversal is fundamental to graphs. Almost every graph query involves some form of traversal\u2014starting at one or more nodes and exploring outward along edges.</p>"},{"location":"chapters/03-labeled-property-graph-model/#graph-query-asking-questions-of-your-data","title":"Graph Query: Asking Questions of Your Data","text":"<p>A graph query is a question you ask your data by specifying patterns to match and conditions to filter. Graph query languages (like Cypher, GSQL, or Gremlin) let you express complex traversals and pattern matching in readable code.</p> <p>Example queries (in Cypher syntax):</p> <pre><code>// Find Alice's direct friends\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nRETURN friend.name;\n\n// Find friends of Alice's friends (2-hop)\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF*2]-(friendOfFriend)\nRETURN friendOfFriend.name;\n\n// Find people who work at the same company as Alice\nMATCH (alice:Person {name: \"Alice\"})-[:WORKS_AT]-&gt;(company)&lt;-[:WORKS_AT]-(coworker)\nRETURN coworker.name;\n\n// Find posts liked by Alice's friends\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)-[:LIKED]-&gt;(post)\nRETURN post.content, friend.name;\n</code></pre> <p>Don't worry if the syntax looks unfamiliar. The key point is that graph queries express patterns visually: - <code>(alice:Person {name: \"Alice\"})</code> - Find a Person node named Alice - <code>-[:FRIEND_OF]-&gt;</code> - Follow a FRIEND_OF edge - <code>(friend)</code> - To another node (call it \"friend\")</p> <p>Graph queries feel more like describing what you're looking for (\"people Alice is friends with\") than instructing the database how to find it (JOIN this table to that table on this key).</p>"},{"location":"chapters/03-labeled-property-graph-model/#pattern-matching-finding-shapes-in-the-graph","title":"Pattern Matching: Finding Shapes in the Graph","text":"<p>Pattern matching is the heart of graph querying. Instead of specifying \"scan this table, join to that table,\" you describe a pattern\u2014a shape or structure\u2014and the database finds all instances of that pattern in the graph.</p> <p>Patterns can be:</p> <p>1. Simple paths: <pre><code>// Alice \u2192 friend\n(alice)-[:FRIEND_OF]-&gt;(friend)\n</code></pre></p> <p>2. Multi-hop paths: <pre><code>// Alice \u2192 friend \u2192 friend of friend\n(alice)-[:FRIEND_OF]-&gt;(friend)-[:FRIEND_OF]-&gt;(fof)\n</code></pre></p> <p>3. Complex shapes: <pre><code>// Alice and Bob both like the same post\n(alice:Person)-[:LIKED]-&gt;(post)&lt;-[:LIKED]-(bob:Person)\n</code></pre></p> <p>4. Variable-length paths: <pre><code>// Anyone connected to Alice via 1-3 FRIEND_OF hops\n(alice)-[:FRIEND_OF*1..3]-(connected)\n</code></pre></p> <p>5. Triangles, cycles, and other structures: <pre><code>// Find triangles: A \u2192 B \u2192 C \u2192 A\n(a)-[:FRIEND_OF]-&gt;(b)-[:FRIEND_OF]-&gt;(c)-[:FRIEND_OF]-&gt;(a)\n</code></pre></p> <p>Pattern matching is declarative: you describe what you want, and the query planner figures out the efficient way to find it. This is similar to SQL's declarative nature, but pattern syntax is more intuitive for relationship queries.</p> <p>The abstract concept: Pattern matching treats subgraphs as first-class entities you can search for, like using Ctrl+F to find text in a document, but for graph structures instead of strings.</p> <p>The concrete analogy: Pattern matching is like describing a constellation to a stargazing app: \"Find me three bright stars forming a triangle with a dimmer star in the middle.\" The app searches the sky and highlights all instances of that pattern. Similarly, graph pattern matching searches your data and returns all matching structures.</p>"},{"location":"chapters/03-labeled-property-graph-model/#multi-hop-queries-going-deep","title":"Multi-Hop Queries: Going Deep","text":"<p>Multi-hop queries traverse multiple edges in sequence, like following a chain: Alice \u2192 Bob \u2192 Charlie \u2192 Diana. Each \"hop\" is one edge traversal.</p> <p>We've seen this before (remember the performance cliff from Chapters 1 and 2?), but let's revisit it with our new vocabulary.</p> <p>1-hop query: <pre><code>// Alice's direct friends\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nRETURN friend;\n</code></pre></p> <p>2-hop query: <pre><code>// Friends of Alice's friends\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;()-[:FRIEND_OF]-&gt;(fof)\nRETURN fof;\n</code></pre></p> <p>3-hop query: <pre><code>// Friends of friends of friends\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF*3]-(third_degree)\nRETURN third_degree;\n</code></pre></p> <p>Variable-length multi-hop: <pre><code>// Anyone connected to Alice via 1-5 friendship hops\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF*1..5]-(connected)\nRETURN connected;\n</code></pre></p> <p>Why multi-hop queries matter:</p> <p>In social networks: \"Show me friends of friends for recommendations\" In supply chains: \"What happens if this supplier fails?\" (5+ hop impact analysis) In fraud detection: \"Find accounts within 3 hops of this suspicious account\" In knowledge graphs: \"How is concept A related to concept Z?\"</p> <p>Multi-hop queries are where graph databases shine. Thanks to index-free adjacency and constant-time neighbor access, these queries run in milliseconds even with millions of nodes, while the equivalent SQL queries would timeout or take hours.</p> <p>(There's that repetition again. Starting to feel familiar, right?)</p>"},{"location":"chapters/03-labeled-property-graph-model/#path-patterns-expressing-complex-routes","title":"Path Patterns: Expressing Complex Routes","text":"<p>Path patterns are a way to specify sequences of nodes and edges with varying levels of specificity. They're the building blocks of graph queries.</p> <p>Types of path patterns:</p> <p>1. Fixed-length paths: <pre><code>(a)-[:FRIEND_OF]-&gt;(b)-[:FRIEND_OF]-&gt;(c)  // Exactly 2 hops\n</code></pre></p> <p>2. Variable-length paths: <pre><code>(a)-[:FRIEND_OF*1..5]-&gt;(connected)  // 1 to 5 hops\n(a)-[:FRIEND_OF*]-(connected)       // Any number of hops (use carefully!)\n</code></pre></p> <p>3. Mixed edge types: <pre><code>(alice)-[:FRIEND_OF]-&gt;(friend)-[:WORKS_AT]-&gt;(company)  // Friend who works at a company\n</code></pre></p> <p>4. Undirected paths: <pre><code>(alice)-[:FRIEND_OF]-(connected)  // Friends in either direction\n</code></pre></p> <p>5. Shortest path: <pre><code>MATCH p = shortestPath((alice)-[:FRIEND_OF*]-(bob))\nRETURN p;  // Find shortest route from Alice to Bob\n</code></pre></p> <p>Path patterns let you express complex relationship queries concisely. Instead of writing nested loops and JOIN logic, you just describe the path structure you're looking for.</p>"},{"location":"chapters/03-labeled-property-graph-model/#aggregation-computing-over-results","title":"Aggregation: Computing Over Results","text":"<p>Like SQL, graph query languages support aggregation\u2014computing statistics, sums, averages, counts over query results.</p> <p>Common aggregations:</p> <pre><code>// Count Alice's friends\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nRETURN count(friend);\n\n// Average age of Alice's friends\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nRETURN avg(friend.age);\n\n// Total amount spent by Alice\nMATCH (alice:Person {name: \"Alice\"})-[p:PURCHASED]-&gt;(product)\nRETURN sum(p.price);\n\n// Most popular posts (by likes)\nMATCH (post:Post)&lt;-[like:LIKED]-()\nRETURN post.content, count(like) AS like_count\nORDER BY like_count DESC\nLIMIT 10;\n</code></pre> <p>Aggregations work after traversals and pattern matches, letting you compute metrics over the results. This combines the power of graph traversal with the analytical capabilities of SQL.</p>"},{"location":"chapters/03-labeled-property-graph-model/#graph-schema-optional-structure","title":"Graph Schema: Optional Structure","text":"<p>A graph schema defines the expected structure of your graph\u2014what node labels exist, what edge types connect them, what properties each has. As we discussed earlier, schemas can be enforced or just documented.</p> <p>Example schema definition (pseudo-code):</p> <pre><code>NODE LABELS:\n  Person {name: string, age: integer, email: string}\n  Post {content: string, timestamp: datetime}\n  Company {name: string, industry: string}\n\nEDGE TYPES:\n  FRIEND_OF: Person \u2192 Person {since: date}\n  POSTED: Person \u2192 Post {timestamp: datetime}\n  LIKED: Person \u2192 Post {timestamp: datetime}\n  WORKS_AT: Person \u2192 Company {title: string, start_date: date}\n</code></pre> <p>Why schemas are helpful even if not enforced:</p> <ul> <li>Documentation: New developers can see expected structure</li> <li>Tooling: Graph visualization tools can use schema to render nicely</li> <li>Validation: Application code can validate before inserting</li> <li>Query optimization: Database can optimize queries based on known structure</li> </ul> <p>Some graph databases (like Neo4j with APOC procedures or TigerGraph with GSQL) let you define and enforce schemas. Others (like Neo4j's core) are schema-optional but let you create constraints (e.g., \"every Person must have a unique email\").</p>"},{"location":"chapters/03-labeled-property-graph-model/#metadata-representation-data-about-data","title":"Metadata Representation: Data About Data","text":"<p>Metadata is data about your data. In graphs, metadata can exist at multiple levels:</p> <p>Node metadata: <pre><code>(user:Person {\n  name: \"Alice\",\n  created_at: \"2020-01-15T10:00:00\",\n  created_by: \"admin_user\",\n  last_modified: \"2025-01-18T14:30:00\",\n  version: 3\n})\n</code></pre></p> <p>Edge metadata: <pre><code>(alice)-[:FRIEND_OF {\n  since: \"2020-05-12\",\n  confirmed_by: \"alice\",\n  confidence: 0.95,\n  source: \"facebook_import\"\n}]-&gt;(bob)\n</code></pre></p> <p>Graph-level metadata: Some systems support graph-wide metadata (when the database was created, who owns it, what application version uses it).</p> <p>Metadata is just properties, but with a special purpose: tracking provenance, versioning, audit trails, data quality metrics, or other information about how the data came to be.</p>"},{"location":"chapters/03-labeled-property-graph-model/#graph-validation-document-validation-and-rule-systems","title":"Graph Validation, Document Validation, and Rule Systems","text":"<p>As graphs grow, you might want to enforce quality and consistency. This is where validation and rules come in.</p>"},{"location":"chapters/03-labeled-property-graph-model/#graph-validation","title":"Graph Validation","text":"<p>Graph validation checks that your graph adheres to structural rules:</p> <ul> <li>\"Every Person node must have a name property\"</li> <li>\"Every PURCHASED edge must connect a Person to a Product (not to another Person)\"</li> <li>\"No FRIEND_OF edges can form a loop (person can't be their own friend)\"</li> </ul> <p>Some databases support validation constraints natively:</p> <pre><code>// Neo4j constraint examples\nCREATE CONSTRAINT person_name IF NOT EXISTS\nFOR (p:Person) REQUIRE p.name IS NOT NULL;\n\nCREATE CONSTRAINT person_email_unique IF NOT EXISTS\nFOR (p:Person) REQUIRE p.email IS UNIQUE;\n</code></pre>"},{"location":"chapters/03-labeled-property-graph-model/#document-validation","title":"Document Validation","text":"<p>Document validation treats nodes like JSON documents and validates their structure:</p> <pre><code>// Validation schema (conceptual)\nPersonSchema = {\n  name: { type: \"string\", required: true },\n  age: { type: \"integer\", min: 0, max: 150 },\n  email: { type: \"string\", format: \"email\", required: true },\n  friends: { type: \"array\", items: { type: \"reference\", to: \"Person\" } }\n}\n</code></pre> <p>This is similar to JSON Schema or MongoDB's document validation, applied to graph nodes.</p>"},{"location":"chapters/03-labeled-property-graph-model/#rule-systems","title":"Rule Systems","text":"<p>Rule systems let you define application logic that executes when data changes:</p> <p>Example rules:</p> <ul> <li>\"When a PURCHASED edge is created, increment the product's sales_count property\"</li> <li>\"When two people become FRIEND_OF each other, create a MUTUAL_FRIENDS edge\"</li> <li>\"When a person's age reaches 18, add the Adult label\"</li> </ul> <p>Some graph databases support rules/triggers directly; others require application-level implementation.</p> <p>These systems help maintain data integrity, enforce business logic, and automate derived data updates.</p>"},{"location":"chapters/03-labeled-property-graph-model/#bringing-it-all-together-a-complete-example","title":"Bringing It All Together: A Complete Example","text":"<p>Let's pull everything together with a comprehensive example that uses all the concepts we've covered.</p> <p>Scenario: A simplified LinkedIn-like professional network</p> <p>Graph data model:</p> <pre><code>NODES:\n  Person {name, email, age, city, headline}\n  Company {name, industry, size, founded}\n  Skill {name, category}\n  Post {content, timestamp}\n\nEDGES:\n  FRIEND_OF: Person \u2192 Person {since, closeness}\n  WORKS_AT: Person \u2192 Company {title, start_date, current}\n  HAS_SKILL: Person \u2192 Skill {proficiency, years_experience}\n  POSTED: Person \u2192 Post {timestamp}\n  LIKED: Person \u2192 Post {timestamp}\n  ENDORSED: Person \u2192 Person {skill, timestamp} // Person endorses Person for a skill\n</code></pre> <p>Sample data:</p> <pre><code>// Create nodes\nCREATE (alice:Person {name: \"Alice Johnson\", email: \"alice@example.com\", age: 28, city: \"Seattle\", headline: \"Software Engineer\"})\nCREATE (bob:Person {name: \"Bob Smith\", email: \"bob@example.com\", age: 35, city: \"Portland\", headline: \"Engineering Manager\"})\nCREATE (techcorp:Company {name: \"TechCorp\", industry: \"Software\", size: 500, founded: 1995})\nCREATE (python:Skill {name: \"Python\", category: \"Programming\"})\nCREATE (leadership:Skill {name: \"Leadership\", category: \"Soft Skills\"})\nCREATE (post1:Post {content: \"Excited to share our new product launch!\", timestamp: \"2025-01-18T10:00:00\"})\n\n// Create edges\nCREATE (alice)-[:FRIEND_OF {since: \"2020-05-12\", closeness: 0.8}]-&gt;(bob)\nCREATE (alice)-[:WORKS_AT {title: \"Software Engineer\", start_date: \"2022-03-01\", current: true}]-&gt;(techcorp)\nCREATE (bob)-[:WORKS_AT {title: \"Engineering Manager\", start_date: \"2019-06-15\", current: true}]-&gt;(techcorp)\nCREATE (alice)-[:HAS_SKILL {proficiency: 9, years_experience: 6}]-&gt;(python)\nCREATE (bob)-[:HAS_SKILL {proficiency: 7, years_experience: 3}]-&gt;(python)\nCREATE (bob)-[:HAS_SKILL {proficiency: 8, years_experience: 10}]-&gt;(leadership)\nCREATE (bob)-[:POSTED {timestamp: \"2025-01-18T10:00:00\"}]-&gt;(post1)\nCREATE (alice)-[:LIKED {timestamp: \"2025-01-18T10:05:00\"}]-&gt;(post1)\nCREATE (alice)-[:ENDORSED {skill: \"Leadership\", timestamp: \"2025-01-15\"}]-&gt;(bob)\n</code></pre> <p>Interesting queries:</p> <pre><code>// 1. Find Alice's coworkers\nMATCH (alice:Person {name: \"Alice Johnson\"})-[:WORKS_AT]-&gt;(company)&lt;-[:WORKS_AT]-(coworker)\nWHERE coworker &lt;&gt; alice\nRETURN coworker.name, coworker.headline;\n\n// 2. Find people Alice knows who have Python skills\nMATCH (alice:Person {name: \"Alice Johnson\"})-[:FRIEND_OF]-(friend)-[:HAS_SKILL]-&gt;(skill:Skill {name: \"Python\"})\nRETURN friend.name, skill.name;\n\n// 3. Find posts from Alice's network (friends and coworkers)\nMATCH (alice:Person {name: \"Alice Johnson\"})\nMATCH (alice)-[:FRIEND_OF|WORKS_AT*1..2]-(connection)-[:POSTED]-&gt;(post)\nRETURN post.content, connection.name, post.timestamp\nORDER BY post.timestamp DESC;\n\n// 4. Skill recommendations: Skills that Alice's coworkers have but Alice doesn't\nMATCH (alice:Person {name: \"Alice Johnson\"})-[:WORKS_AT]-&gt;(company)&lt;-[:WORKS_AT]-(coworker)\nMATCH (coworker)-[:HAS_SKILL]-&gt;(skill)\nWHERE NOT (alice)-[:HAS_SKILL]-&gt;(skill)\nRETURN skill.name, count(coworker) AS coworker_count\nORDER BY coworker_count DESC;\n\n// 5. Find people within Alice's 2nd-degree network in Seattle\nMATCH (alice:Person {name: \"Alice Johnson\"})-[:FRIEND_OF*1..2]-(connection:Person {city: \"Seattle\"})\nRETURN DISTINCT connection.name, connection.headline;\n</code></pre> <p>Do you see how all the concepts come together? We have: - \u2705 Nodes (Person, Company, Skill, Post) - \u2705 Edges (FRIEND_OF, WORKS_AT, HAS_SKILL, POSTED, LIKED, ENDORSED) - \u2705 Properties (on both nodes and edges) - \u2705 Labels (organizing nodes and edge types) - \u2705 Traversal (following edges from Alice outward) - \u2705 Pattern matching (finding specific structures like \"coworkers\") - \u2705 Multi-hop queries (friends of friends, 1-2 degrees) - \u2705 Aggregation (counting skills, ordering by timestamp)</p> <p>This is a real graph data model! You could build an actual application on this structure.</p>"},{"location":"chapters/03-labeled-property-graph-model/#taking-a-breath-what-weve-covered","title":"Taking a Breath: What We've Covered","text":"<p>Okay, that was a lot. Let's recap the major concepts:</p> <p>Core building blocks: 1. \u2705 Labeled Property Graph - The overall model 2. \u2705 Nodes - Entities (things that exist) 3. \u2705 Edges - Relationships (connections between things) 4. \u2705 Properties - Attributes (details about nodes and edges) 5. \u2705 Labels - Categories (organizing nodes and edges into types)</p> <p>Advanced concepts: 6. \u2705 First-Class Relationships - Edges are real, queryable entities 7. \u2705 Edge Direction - Relationships have direction but can be traversed either way 8. \u2705 Index-Free Adjacency - How graphs achieve constant-time neighbor access 9. \u2705 Constant-Time Neighbor Access - Why graphs are fast 10. \u2705 Graph Data Model - The structure describing your graph 11. \u2705 Schema-Optional Modeling - No schema required, flexible structure 12. \u2705 Schema-Enforced Modeling - Schema required, guaranteed consistency 13. \u2705 Graph Schema - Defining expected structure 14. \u2705 Traversal - Walking through the graph 15. \u2705 Graph Query - Asking questions of your data 16. \u2705 Pattern Matching - Finding structures/shapes in the graph 17. \u2705 Path Patterns - Expressing complex routes 18. \u2705 Multi-Hop Queries - Traversing multiple edges 19. \u2705 Aggregation - Computing statistics over results 20. \u2705 Metadata Representation - Data about data 21. \u2705 Graph Validation - Enforcing structural rules 22. \u2705 Document Validation - Validating node/edge properties 23. \u2705 Rule Systems - Automating logic and maintaining integrity</p> <p>All 23 concepts, covered!</p> <p>If your head is spinning, that's completely normal. You've just been introduced to an entire data model in one chapter. The key isn't to memorize every definition\u2014it's to understand the big picture:</p> <p>Graphs represent connected data naturally using nodes (things) and edges (connections), with properties (details) and labels (categories). This structure enables fast traversal and intuitive querying through pattern matching. Schemas are optional, giving you flexibility to choose between agility and consistency. The result: a database that mirrors how you think about relationships.</p>"},{"location":"chapters/03-labeled-property-graph-model/#building-confidence-through-repetition","title":"Building Confidence Through Repetition","text":"<p>Remember how we promised repetition would help these concepts click? Here's the same model explained three different ways:</p> <p>Version 1 (Concrete analogy): Imagine a corkboard full of index cards (nodes) connected by colored strings (edges). Each card has information written on it (properties) and a category sticker (label). You can trace along the strings to see how cards connect (traversal), and you can search for specific patterns like \"cards connected by red strings\" (pattern matching).</p> <p>Version 2 (Abstract model): A labeled property graph consists of vertices (V) and edges (E), where E \u2286 V \u00d7 V, with both vertices and edges possessing arbitrary key-value properties (P) and categorical labels (L). Traversal operations navigate E relationships in O(1) time via index-free adjacency, enabling efficient pattern matching across variable-length paths.</p> <p>Version 3 (Practical example): Think about Facebook: your profile is a node, your friendship with someone is an edge, your name and age are properties, and \"Person\" is a label. When you view a friend's profile, the database doesn't search a table\u2014it follows a direct pointer from your node to theirs. When you see \"people you may know,\" that's a 2-hop traversal (friends of your friends) running in milliseconds.</p> <p>Same concepts, three angles. Which explanation resonates with you? Probably one more than the others, and that's fine\u2014people learn differently. The important thing is that the core ideas are getting reinforced.</p>"},{"location":"chapters/03-labeled-property-graph-model/#moving-forward","title":"Moving Forward","text":"<p>In the next chapter, we'll dive into query languages\u2014specifically Cypher and GSQL\u2014and you'll get hands-on experience writing real graph queries. All the concepts we introduced here (nodes, edges, traversal, pattern matching) will show up again in concrete query syntax.</p> <p>And you know what? When you see <code>(alice:Person)-[:FRIEND_OF]-&gt;(bob)</code> in the next chapter, it won't feel weird anymore. You'll think \"Okay, that's a Person node named alice, connected by a FRIEND_OF edge to another Person node named bob.\" It'll feel natural.</p> <p>That's the power of repetition and varied examples. The concepts we introduced here will keep showing up throughout the rest of this course, and each time you'll understand them a little better.</p>"},{"location":"chapters/03-labeled-property-graph-model/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>The Labeled Property Graph model has four components: nodes (entities), edges (relationships), properties (attributes), and labels (categories)</p> </li> <li> <p>Relationships are first-class citizens in graph databases\u2014they're real, queryable entities with their own properties and identities</p> </li> <li> <p>Index-free adjacency means nodes store direct pointers to connected nodes, enabling constant-time neighbor access regardless of graph size</p> </li> <li> <p>Schema flexibility lets you choose: schema-optional for agility and evolution, or schema-enforced for consistency and data quality</p> </li> <li> <p>Traversal and pattern matching make graph queries intuitive\u2014you describe the structure you're looking for, not how to find it</p> </li> <li> <p>Multi-hop queries are where graphs shine, maintaining performance even at 5+ hops while RDBMS queries timeout</p> </li> <li> <p>Validation and rules help maintain data quality as graphs grow, though approaches vary by database</p> </li> </ol> <p>Most importantly: Don't worry if this all feels overwhelming. These concepts will appear again and again throughout this course, in queries, in examples, in case studies. Each repetition will strengthen your understanding. By the end of the course, you'll be thinking in graphs naturally.</p> <p>Ready for the next step? Chapter 4 awaits, where all this theory becomes practice through actual query languages.</p> <p>Graph databases ask you to think differently about data, but that difference is actually more natural\u2014it's how you already think about the connected world around you. The learning curve isn't steep; it's just unfamiliar. Give it time, and it'll click.</p>"},{"location":"chapters/04-query-languages/","title":"Query Languages for Graph Databases","text":""},{"location":"chapters/04-query-languages/#summary","title":"Summary","text":"<p>This chapter provides comprehensive coverage of graph query languages including OpenCypher, GSQL, and the emerging GQL standard. You'll master Cypher syntax for pattern matching, learn how to construct complex graph queries with match, where, and return clauses, and explore GSQL's map-reduce pattern for distributed query processing. The chapter emphasizes both declarative and imperative query approaches, query optimization techniques, and performance considerations for production graph applications.</p>"},{"location":"chapters/04-query-languages/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 26 concepts from the learning graph:</p> <ol> <li>OpenCypher</li> <li>GSQL</li> <li>Statistical Query Tuning</li> <li>GQL</li> <li>Cypher Syntax</li> <li>Match Clause</li> <li>Where Clause</li> <li>Return Clause</li> <li>Create Statement</li> <li>Merge Statement</li> <li>Delete Statement</li> <li>Set Clause</li> <li>Graph Patterns</li> <li>Variable Length Paths</li> <li>Shortest Path</li> <li>All Paths</li> <li>Map-Reduce Pattern</li> <li>Accumulators</li> <li>Query Optimization</li> <li>Query Performance</li> <li>Query Latency</li> <li>Query Throughput</li> <li>Declarative Queries</li> <li>Imperative Queries</li> <li>Query Plans</li> <li>Shortest Path Algorithms</li> </ol>"},{"location":"chapters/04-query-languages/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 3: Labeled Property Graph Information Model</li> </ul>"},{"location":"chapters/04-query-languages/#the-elephant-in-the-room-or-should-we-say-the-ai-in-the-cloud","title":"The Elephant in the Room (Or Should We Say, the AI in the Cloud?)","text":"<p>Let's address something right up front: By the time you're reading this, AI capabilities are doubling roughly every seven months. There's a decent chance that by the time you're actually working with graph databases professionally, you'll just describe what you want in plain English, and an AI will write the query for you. \"Hey AI, find all customers who bought products similar to Alice's purchases in the last month.\" Done.</p> <p>So why are we about to spend an entire chapter learning Cypher syntax, GSQL patterns, and query optimization techniques?</p> <p>Here's the thing (and we're giving you a knowing wink here): Understanding what the code does is valuable even if you never write it yourself. When the AI generates a query that returns 10 million nodes instead of the 10 you expected, you'll want to know why. When a query that should take milliseconds is taking minutes, you'll need to spot the problem. When you're reviewing what the AI suggested and something looks... off... you'll want the knowledge to catch it.</p> <p>Think of it like learning to drive even though self-driving cars exist. Sure, the car might handle 99% of the driving, but you still want to know what's happening when you press the brake, right?</p> <p>So yes, AI might write most of your graph queries in the future. But this chapter will teach you to read them, understand them, debug them, and\u2014when necessary\u2014write them yourself. Consider it \"AI literacy for graph databases.\"</p> <p>Ready? Let's dive into graph query languages. And remember: every time you think \"I'll never write this manually,\" imagine your future self saying \"Thank goodness I learned this\" when the AI suggests a query that would accidentally delete your entire production database. (We're joking. Mostly.)</p>"},{"location":"chapters/04-query-languages/#query-languages-the-big-three-and-the-future","title":"Query Languages: The Big Three (and the Future)","text":"<p>Before we dive into syntax, let's survey the landscape. There are three major query languages you should know about, plus a fourth that's emerging as a standard.</p>"},{"location":"chapters/04-query-languages/#opencypher-the-peoples-champion","title":"OpenCypher: The People's Champion","text":"<p>OpenCypher is the most popular graph query language, originally developed by Neo4j and then open-sourced. It's declarative (you describe what you want, not how to get it), highly readable, and looks a bit like ASCII art of graphs.</p> <p>Why it's popular: - Visual syntax: <code>(alice:Person)-[:FRIEND_OF]-&gt;(bob)</code> literally looks like a graph - Declarative: Focus on the pattern you want, not the algorithm to find it - Wide adoption: Neo4j, Amazon Neptune, Memgraph, RedisGraph, and many others - Active community: Lots of resources, tutorials, and Stack Overflow answers</p> <p>Example: <pre><code>MATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nRETURN friend.name, friend.age\nORDER BY friend.age DESC;\n</code></pre></p> <p>Even if you've never seen Cypher before, you can probably guess what this does: Find Alice, find her friends, return their names and ages sorted by age.</p>"},{"location":"chapters/04-query-languages/#gsql-the-distributed-powerhouse","title":"GSQL: The Distributed Powerhouse","text":"<p>GSQL (Graph SQL) is TigerGraph's query language, designed for massive-scale distributed graph processing. While Cypher is declarative, GSQL blends declarative and imperative styles, giving you fine-grained control over execution.</p> <p>Why it matters: - Imperative control: You can specify exactly how to process the graph - Map-reduce pattern: Built for distributed computation across clusters - Accumulators: Powerful constructs for aggregating data during traversal - Performance tuning: Fine-grained control for optimizing complex queries</p> <p>Example: <pre><code>CREATE QUERY FindFriends(VERTEX&lt;Person&gt; inputPerson) {\n  Start = {inputPerson};\n  Friends = SELECT friend\n            FROM Start:s -(FRIEND_OF:e)- Person:friend\n            ORDER BY friend.age DESC;\n  PRINT Friends;\n}\n</code></pre></p> <p>GSQL looks more like traditional programming\u2014you define variables, specify control flow, and manage execution explicitly.</p>"},{"location":"chapters/04-query-languages/#gql-the-emerging-standard","title":"GQL: The Emerging Standard","text":"<p>GQL (Graph Query Language) is the ISO standard for graph queries, currently being developed by the same committee that created SQL. Think of it as \"SQL for graphs.\"</p> <p>Why you should care (eventually): - ISO standard: Official international standard, like SQL - Industry consensus: Major vendors collaborating on unified syntax - Future-proof: Learning GQL means learning the future lingua franca of graphs - SQL familiarity: Designed to feel familiar to SQL developers</p> <p>Current status: Still emerging (as of 2025). Neo4j, Oracle, and other vendors are implementing support, but it's not yet as mature as Cypher or GSQL.</p> <p>What it looks like: <pre><code>MATCH (alice:Person WHERE alice.name = 'Alice')-[:FRIEND_OF]-&gt;(friend:Person)\nRETURN friend.name, friend.age\nORDER BY friend.age DESC\n</code></pre></p> <p>Familiar, right? It's intentionally similar to Cypher but with SQL-like syntax elements.</p>"},{"location":"chapters/04-query-languages/#which-one-should-you-learn","title":"Which One Should You Learn?","text":"<p>Short answer: Start with Cypher. It's the most widely used, has the best tutorials, and concepts transfer easily to other languages.</p> <p>Long answer: Cypher will teach you graph thinking. GSQL will teach you performance optimization. GQL will prepare you for the future. Ideally, know enough Cypher to read and write basic queries, understand GSQL concepts for distributed systems, and keep an eye on GQL for standardization.</p> <p>And remember: The AI will probably write queries in whatever language your database supports. Your job is to understand what it wrote. \ud83d\ude09</p>"},{"location":"chapters/04-query-languages/#cypher-syntax-the-ascii-art-of-graph-queries","title":"Cypher Syntax: The ASCII Art of Graph Queries","text":"<p>Let's dive deep into Cypher, the most popular graph query language. We'll cover enough that when an AI (or a colleague, or your future self) writes a Cypher query, you'll know exactly what's happening.</p>"},{"location":"chapters/04-query-languages/#the-core-philosophy-drawing-graphs-with-text","title":"The Core Philosophy: Drawing Graphs with Text","text":"<p>Cypher's genius is visual syntax. Compare these:</p> <p>What you're thinking: <pre><code>Alice --[FRIEND_OF]--&gt; Bob\n</code></pre></p> <p>What you write: <pre><code>(alice:Person)-[:FRIEND_OF]-&gt;(bob:Person)\n</code></pre></p> <p>See the similarity? Nodes in parentheses <code>()</code>, relationships in brackets <code>[]</code>, arrows showing direction <code>-&gt;</code>. It's ASCII art that happens to be executable code.</p>"},{"location":"chapters/04-query-languages/#nodes-the-parentheses-pattern","title":"Nodes: The Parentheses Pattern","text":"<p>Nodes are always wrapped in parentheses:</p> <pre><code>()                           // Anonymous node (any node)\n(n)                          // Node bound to variable 'n'\n(:Person)                    // Node with label 'Person'\n(p:Person)                   // Person node bound to variable 'p'\n(alice:Person {name: \"Alice\"})  // Person named Alice\n(p:Person:Employee)          // Node with multiple labels\n</code></pre> <p>Breaking down the anatomy: - <code>(variable:Label {property: value})</code> - Variable (optional): Lets you refer to the node later in the query - Label (optional): The type/category of node - Properties (optional): Key-value pairs to match or filter</p>"},{"location":"chapters/04-query-languages/#relationships-the-bracket-and-arrow-pattern","title":"Relationships: The Bracket and Arrow Pattern","text":"<p>Relationships use brackets and arrows:</p> <pre><code>-[:FRIEND_OF]-&gt;              // Directed relationship, specific type\n-[:FRIEND_OF]-               // Undirected (matches either direction)\n&lt;-[:FRIEND_OF]-              // Relationship pointing left\n-[r:FRIEND_OF]-&gt;             // Relationship bound to variable 'r'\n-[:FRIEND_OF {since: 2020}]-&gt;  // Relationship with properties\n-[*1..3]-&gt;                   // Variable-length path (1 to 3 hops)\n-[:FRIEND_OF|COLLEAGUE]-&gt;    // Multiple relationship types (OR)\n</code></pre> <p>Direction matters (usually): - <code>(alice)-[:PURCHASED]-&gt;(product)</code> - Alice purchased product \u2705 - <code>(product)-[:PURCHASED]-&gt;(alice)</code> - Product purchased Alice? \u274c (semantically weird)</p> <p>But you can traverse backwards: - <code>(alice)&lt;-[:PURCHASED]-(product)</code> - Products purchased by Alice (same data, viewed backwards)</p>"},{"location":"chapters/04-query-languages/#the-five-essential-clauses","title":"The Five Essential Clauses","text":"<p>Cypher queries are built from clauses, like SQL. Here are the five you'll use constantly:</p> <ol> <li>MATCH - Find patterns in the graph</li> <li>WHERE - Filter results</li> <li>RETURN - Specify what to output</li> <li>CREATE - Add new data</li> <li>DELETE - Remove data</li> </ol> <p>Let's explore each in detail.</p>"},{"location":"chapters/04-query-languages/#match-clause-finding-patterns","title":"MATCH Clause: Finding Patterns","text":"<p>The MATCH clause is the heart of Cypher queries. It describes a pattern you want to find in the graph.</p> <p>Simple matching: <pre><code>// Find all Person nodes\nMATCH (p:Person)\nRETURN p;\n\n// Find all friendships\nMATCH (a:Person)-[:FRIEND_OF]-&gt;(b:Person)\nRETURN a.name, b.name;\n\n// Find Alice specifically\nMATCH (alice:Person {name: \"Alice\"})\nRETURN alice;\n</code></pre></p> <p>Multi-hop matching: <pre><code>// Find friends of friends (2 hops)\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;()-[:FRIEND_OF]-&gt;(fof)\nRETURN fof.name;\n\n// Find who Alice's friends follow (mixing relationship types)\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)-[:FOLLOWS]-&gt;(celebrity)\nRETURN celebrity.name;\n</code></pre></p> <p>Multiple patterns: <pre><code>// Find people who are both friends AND work at the same company\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nMATCH (alice)-[:WORKS_AT]-&gt;(company)&lt;-[:WORKS_AT]-(friend)\nRETURN friend.name, company.name;\n</code></pre></p> <p>Optional patterns: <pre><code>// Find Alice's friends, and their companies if they have them\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nOPTIONAL MATCH (friend)-[:WORKS_AT]-&gt;(company)\nRETURN friend.name, company.name;  // company.name might be null\n</code></pre></p> <p>The abstract concept: MATCH is declarative pattern matching. You describe the shape you want, and the query engine finds all instances of that shape in your graph.</p> <p>The practical reality: When you tell an AI \"find Alice's friends,\" it writes a MATCH clause. When the query takes too long, you'll look at the MATCH clause to see if it's searching too broadly.</p>"},{"location":"chapters/04-query-languages/#where-clause-filtering-results","title":"WHERE Clause: Filtering Results","text":"<p>The WHERE clause filters matches based on conditions, just like SQL.</p> <p>Property filtering: <pre><code>// Friends over 30\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nWHERE friend.age &gt; 30\nRETURN friend.name;\n\n// Multiple conditions\nMATCH (p:Person)\nWHERE p.age &gt; 25 AND p.age &lt; 40 AND p.city = \"Seattle\"\nRETURN p.name;\n\n// Pattern matching in strings\nMATCH (p:Person)\nWHERE p.email ENDS WITH \"@example.com\"\nRETURN p.name;\n</code></pre></p> <p>Relationship filtering: <pre><code>// Friendships formed after 2020\nMATCH (a:Person)-[r:FRIEND_OF]-&gt;(b:Person)\nWHERE r.since &gt; date(\"2020-01-01\")\nRETURN a.name, b.name, r.since;\n</code></pre></p> <p>Null checking: <pre><code>// People who have an email address\nMATCH (p:Person)\nWHERE p.email IS NOT NULL\nRETURN p;\n</code></pre></p> <p>List operations: <pre><code>// People in specific cities\nMATCH (p:Person)\nWHERE p.city IN [\"Seattle\", \"Portland\", \"San Francisco\"]\nRETURN p.name, p.city;\n</code></pre></p> <p>Pattern-based filtering: <pre><code>// Find people who have friends but don't work anywhere\nMATCH (p:Person)-[:FRIEND_OF]-&gt;()\nWHERE NOT (p)-[:WORKS_AT]-&gt;()\nRETURN p.name;\n</code></pre></p> <p>Pro tip: You can often put properties directly in MATCH <code>(p:Person {age: 30})</code> instead of using WHERE <code>WHERE p.age = 30</code>. They're equivalent, but WHERE is more flexible for complex conditions.</p>"},{"location":"chapters/04-query-languages/#return-clause-shaping-output","title":"RETURN Clause: Shaping Output","text":"<p>The RETURN clause specifies what data you want back from the query.</p> <p>Basic returns: <pre><code>// Return nodes\nMATCH (p:Person)\nRETURN p;\n\n// Return specific properties\nMATCH (p:Person)\nRETURN p.name, p.age;\n\n// Return with aliases\nMATCH (p:Person)\nRETURN p.name AS person_name, p.age AS person_age;\n</code></pre></p> <p>Returning relationships: <pre><code>// Return the whole pattern\nMATCH (a:Person)-[r:FRIEND_OF]-&gt;(b:Person)\nRETURN a, r, b;\n\n// Return relationship properties\nMATCH (a:Person)-[r:FRIEND_OF]-&gt;(b:Person)\nRETURN a.name, b.name, r.since;\n</code></pre></p> <p>Computed values: <pre><code>// Calculate on the fly\nMATCH (p:Person)\nRETURN p.name, p.age, (2025 - p.age) AS birth_year;\n\n// String concatenation\nMATCH (p:Person)\nRETURN p.name + \" (\" + p.city + \")\" AS person_description;\n</code></pre></p> <p>Aggregations: <pre><code>// Count friends\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nRETURN count(friend);\n\n// Average age of friends\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nRETURN avg(friend.age), min(friend.age), max(friend.age);\n\n// Group and count\nMATCH (p:Person)\nRETURN p.city, count(p) AS population\nORDER BY population DESC;\n</code></pre></p> <p>Sorting and limiting: <pre><code>// Sort by age\nMATCH (p:Person)\nRETURN p.name, p.age\nORDER BY p.age DESC;\n\n// Top 10 oldest people\nMATCH (p:Person)\nRETURN p.name, p.age\nORDER BY p.age DESC\nLIMIT 10;\n\n// Skip and limit (pagination)\nMATCH (p:Person)\nRETURN p.name\nORDER BY p.name\nSKIP 20\nLIMIT 10;  // Results 21-30\n</code></pre></p> <p>DISTINCT results: <pre><code>// Unique cities\nMATCH (p:Person)\nRETURN DISTINCT p.city;\n\n// Count unique cities\nMATCH (p:Person)\nRETURN count(DISTINCT p.city);\n</code></pre></p>"},{"location":"chapters/04-query-languages/#create-statement-adding-data","title":"CREATE Statement: Adding Data","text":"<p>The CREATE statement adds new nodes and relationships to the graph.</p> <p>Creating nodes: <pre><code>// Create a single node\nCREATE (alice:Person {name: \"Alice\", age: 28, city: \"Seattle\"});\n\n// Create multiple nodes at once\nCREATE\n  (alice:Person {name: \"Alice\", age: 28}),\n  (bob:Person {name: \"Bob\", age: 35}),\n  (techcorp:Company {name: \"TechCorp\"});\n</code></pre></p> <p>Creating relationships: <pre><code>// Create nodes and relationship in one statement\nCREATE (alice:Person {name: \"Alice\"})-[:FRIEND_OF {since: \"2020-05-12\"}]-&gt;(bob:Person {name: \"Bob\"});\n\n// Add relationship between existing nodes\nMATCH (alice:Person {name: \"Alice\"})\nMATCH (bob:Person {name: \"Bob\"})\nCREATE (alice)-[:FRIEND_OF {since: \"2020-05-12\"}]-&gt;(bob);\n</code></pre></p> <p>Creating patterns: <pre><code>// Create a whole social network structure\nCREATE\n  (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(bob:Person {name: \"Bob\"}),\n  (bob)-[:FRIEND_OF]-&gt;(charlie:Person {name: \"Charlie\"}),\n  (charlie)-[:FRIEND_OF]-&gt;(alice),\n  (alice)-[:WORKS_AT]-&gt;(techcorp:Company {name: \"TechCorp\"}),\n  (bob)-[:WORKS_AT]-&gt;(techcorp);\n</code></pre></p> <p>Returning created data: <pre><code>CREATE (alice:Person {name: \"Alice\", age: 28})\nRETURN alice;\n</code></pre></p> <p>Important warning: CREATE always creates new nodes/relationships, even if they already exist. If you run the same CREATE statement twice, you'll get duplicates. That's where MERGE comes in...</p>"},{"location":"chapters/04-query-languages/#merge-statement-create-or-match","title":"MERGE Statement: Create or Match","text":"<p>The MERGE statement is like \"create if it doesn't exist, otherwise match.\" It's idempotent\u2014running it multiple times has the same effect as running it once.</p> <p>Basic MERGE: <pre><code>// Create Alice if she doesn't exist\nMERGE (alice:Person {name: \"Alice\"})\nRETURN alice;\n\n// Run this 10 times - still only one Alice node\nMERGE (alice:Person {name: \"Alice\"})\nRETURN alice;\n</code></pre></p> <p>MERGE with ON CREATE: <pre><code>// Set properties only when creating new node\nMERGE (alice:Person {name: \"Alice\"})\nON CREATE SET alice.created = timestamp(), alice.age = 28\nRETURN alice;\n</code></pre></p> <p>MERGE with ON MATCH: <pre><code>// Update last_seen every time we match Alice\nMERGE (alice:Person {name: \"Alice\"})\nON MATCH SET alice.last_seen = timestamp()\nRETURN alice;\n</code></pre></p> <p>MERGE with both: <pre><code>MERGE (alice:Person {name: \"Alice\"})\nON CREATE SET alice.created = timestamp(), alice.age = 28\nON MATCH SET alice.last_seen = timestamp()\nRETURN alice;\n</code></pre></p> <p>MERGE relationships: <pre><code>// Ensure friendship exists (create if missing)\nMATCH (alice:Person {name: \"Alice\"})\nMATCH (bob:Person {name: \"Bob\"})\nMERGE (alice)-[r:FRIEND_OF]-&gt;(bob)\nON CREATE SET r.since = date()\nRETURN r;\n</code></pre></p> <p>Why MERGE matters: When loading data from external sources, you often don't know if nodes already exist. MERGE handles this gracefully\u2014no duplicates, no errors.</p> <p>When the AI uses MERGE: If you ask an AI to \"make sure Alice is friends with Bob,\" it should use MERGE, not CREATE. If it uses CREATE, you might end up with 50 duplicate FRIEND_OF relationships. Now you know to spot that!</p>"},{"location":"chapters/04-query-languages/#set-clause-updating-properties","title":"SET Clause: Updating Properties","text":"<p>The SET clause modifies properties on existing nodes and relationships.</p> <p>Setting properties: <pre><code>// Update a single property\nMATCH (alice:Person {name: \"Alice\"})\nSET alice.age = 29\nRETURN alice;\n\n// Update multiple properties\nMATCH (alice:Person {name: \"Alice\"})\nSET alice.age = 29, alice.city = \"Portland\", alice.updated = timestamp()\nRETURN alice;\n</code></pre></p> <p>Adding labels: <pre><code>// Add a label to a node\nMATCH (alice:Person {name: \"Alice\"})\nSET alice:Employee\nRETURN alice;  // Now alice has labels Person AND Employee\n</code></pre></p> <p>Copying properties: <pre><code>// Copy all properties from one node to another\nMATCH (alice:Person {name: \"Alice\"})\nMATCH (template:PersonTemplate)\nSET alice = template\nRETURN alice;\n</code></pre></p> <p>Conditional updates: <pre><code>// Update age only if current age is less than 30\nMATCH (p:Person)\nWHERE p.age &lt; 30\nSET p.age = p.age + 1\nRETURN p.name, p.age;\n</code></pre></p> <p>Updating relationship properties: <pre><code>MATCH (a:Person)-[r:FRIEND_OF]-&gt;(b:Person)\nWHERE a.name = \"Alice\" AND b.name = \"Bob\"\nSET r.closeness = 0.9, r.last_contact = date()\nRETURN r;\n</code></pre></p>"},{"location":"chapters/04-query-languages/#delete-statement-removing-data","title":"DELETE Statement: Removing Data","text":"<p>The DELETE statement removes nodes and relationships from the graph.</p> <p>Deleting relationships: <pre><code>// Delete a specific friendship\nMATCH (alice:Person {name: \"Alice\"})-[r:FRIEND_OF]-&gt;(bob:Person {name: \"Bob\"})\nDELETE r;\n\n// Delete all FRIEND_OF relationships\nMATCH ()-[r:FRIEND_OF]-&gt;()\nDELETE r;\n</code></pre></p> <p>Deleting nodes: <pre><code>// Delete a node (must delete its relationships first!)\nMATCH (alice:Person {name: \"Alice\"})\nDELETE alice;  // ERROR if Alice has any relationships\n\n// Delete node and all its relationships\nMATCH (alice:Person {name: \"Alice\"})\nDETACH DELETE alice;  // Deletes Alice and all connected relationships\n</code></pre></p> <p>Conditional deletion: <pre><code>// Delete inactive users\nMATCH (p:Person)\nWHERE p.last_seen &lt; date() - duration({months: 6})\nDETACH DELETE p;\n</code></pre></p> <p>Deleting all data (use with extreme caution!): <pre><code>// Delete everything in the database\nMATCH (n)\nDETACH DELETE n;  // \u26a0\ufe0f This removes EVERYTHING\n</code></pre></p> <p>Why DETACH DELETE exists: In graph databases, you can't have relationships pointing to non-existent nodes. If you try to DELETE a node that has relationships, the database will throw an error. DETACH DELETE removes the relationships first, then the node.</p> <p>When the AI might get this wrong: If the AI tries to DELETE a node without DETACH, the query will fail. Now you'll know to add DETACH. (See? Understanding syntax helps even when AI writes code!)</p>"},{"location":"chapters/04-query-languages/#graph-patterns-the-power-of-structure","title":"Graph Patterns: The Power of Structure","text":"<p>Graph patterns are the core of Cypher queries\u2014they describe shapes you want to find in your graph.</p> <p>Basic patterns: <pre><code>// Simple 1-hop pattern\n(a)-[:KNOWS]-&gt;(b)\n\n// 2-hop pattern\n(a)-[:KNOWS]-&gt;(b)-[:KNOWS]-&gt;(c)\n\n// Triangle pattern\n(a)-[:KNOWS]-&gt;(b)-[:KNOWS]-&gt;(c)-[:KNOWS]-&gt;(a)\n\n// Star pattern (central node with multiple connections)\n(center)-[:CONNECTED_TO]-&gt;(n1),\n(center)-[:CONNECTED_TO]-&gt;(n2),\n(center)-[:CONNECTED_TO]-&gt;(n3)\n</code></pre></p> <p>Pattern with multiple relationship types: <pre><code>// Alice's friends who work at companies Alice follows\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\n      -[:WORKS_AT]-&gt;(company)&lt;-[:FOLLOWS]-(alice)\nRETURN friend.name, company.name;\n</code></pre></p> <p>Patterns with constraints: <pre><code>// Find fraud rings: groups where everyone knows everyone (cliques)\nMATCH (a:Person)-[:TRANSFERRED_MONEY]-&gt;(b:Person)\n     -[:TRANSFERRED_MONEY]-&gt;(c:Person)-[:TRANSFERRED_MONEY]-&gt;(a)\nWHERE a &lt;&gt; b AND b &lt;&gt; c AND c &lt;&gt; a  // Ensure they're distinct\nRETURN a, b, c;\n</code></pre></p> <p>Anti-patterns (patterns that should NOT exist): <pre><code>// Find people who have friends but no job\nMATCH (p:Person)-[:FRIEND_OF]-&gt;()\nWHERE NOT (p)-[:WORKS_AT]-&gt;()\nRETURN p.name;\n</code></pre></p> <p>Why patterns matter: Patterns let you express complex graph queries concisely. Finding triangles (3-way relationships) in SQL would require multiple self-joins. In Cypher, it's one visual pattern.</p>"},{"location":"chapters/04-query-languages/#variable-length-paths-following-the-rabbit-hole","title":"Variable-Length Paths: Following the Rabbit Hole","text":"<p>Variable-length paths let you traverse relationships without knowing how many hops you need.</p> <p>Basic syntax: <pre><code>-[*]-&gt;           // Any number of hops (use carefully\u2014can be slow!)\n-[*1..3]-&gt;       // 1 to 3 hops\n-[*..5]-&gt;        // Up to 5 hops\n-[*2..]-&gt;        // At least 2 hops\n-[:FRIEND_OF*1..3]-&gt;  // 1-3 hops of FRIEND_OF relationships\n</code></pre></p> <p>Real examples: <pre><code>// Find everyone within 3 degrees of Alice\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF*1..3]-(connected)\nRETURN DISTINCT connected.name;\n\n// Find influence chains: who can reach CEO through management?\nMATCH (employee:Person)-[:REPORTS_TO*1..10]-&gt;(ceo:Person {title: \"CEO\"})\nRETURN employee.name, length(path) AS distance;\n\n// Find product recommendations: friends of friends who bought similar products\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF*2]-(fof)\n     -[:PURCHASED]-&gt;(product)\nWHERE NOT (alice)-[:PURCHASED]-&gt;(product)  // Alice hasn't bought it yet\nRETURN product.name, count(fof) AS friend_count\nORDER BY friend_count DESC\nLIMIT 10;\n</code></pre></p> <p>Why variable-length paths are powerful: They let you explore network effects, influence propagation, recommendation chains, and supply chain dependencies without knowing the exact number of hops beforehand.</p> <p>Performance warning: Variable-length paths can be expensive. <code>[:FRIEND_OF*]</code> with no upper limit might traverse millions of relationships. Always set an upper bound (<code>*1..5</code>) unless you have a very good reason not to.</p> <p>When the AI might mess this up: If the AI writes <code>[:FRIEND_OF*]</code> without a limit on a large graph, the query might run forever. Understanding this helps you spot the issue.</p>"},{"location":"chapters/04-query-languages/#shortest-path-finding-the-quickest-route","title":"Shortest Path: Finding the Quickest Route","text":"<p>Shortest path finds the minimal-hop route between two nodes.</p> <p>Basic shortest path: <pre><code>// Find shortest friendship chain between Alice and Bob\nMATCH path = shortestPath((alice:Person {name: \"Alice\"})\n                         -[:FRIEND_OF*]-(bob:Person {name: \"Bob\"}))\nRETURN path, length(path);\n</code></pre></p> <p>All shortest paths: <pre><code>// Find all shortest paths (there might be multiple routes with same length)\nMATCH paths = allShortestPaths((alice:Person {name: \"Alice\"})\n                              -[:FRIEND_OF*]-(bob:Person {name: \"Bob\"}))\nRETURN paths, length(paths);\n</code></pre></p> <p>Shortest path with relationship type filter: <pre><code>// Shortest professional connection (via WORKS_AT and PARTNER_WITH)\nMATCH path = shortestPath((alice:Person {name: \"Alice\"})\n                         -[:WORKS_AT|PARTNER_WITH*]-(bob:Person {name: \"Bob\"}))\nRETURN path;\n</code></pre></p> <p>Shortest path with max length: <pre><code>// Find shortest path within 5 hops (return null if no path exists)\nMATCH path = shortestPath((alice:Person {name: \"Alice\"})\n                         -[:FRIEND_OF*..5]-(bob:Person {name: \"Bob\"}))\nRETURN path;\n</code></pre></p> <p>Real-world use cases: - Social networks: Six degrees of separation, connection suggestions - Supply chains: Find fastest route from manufacturer to customer - Network routing: Shortest path between network nodes - Knowledge graphs: How are two concepts related?</p> <p>The abstract concept: Shortest path algorithms (Dijkstra's, BFS) find minimal-cost routes through graphs. Cypher abstracts this complexity into simple syntax.</p>"},{"location":"chapters/04-query-languages/#all-paths-when-you-need-every-route","title":"All Paths: When You Need Every Route","text":"<p>All paths returns every possible route between two nodes (use carefully\u2014can be huge!).</p> <p>Basic syntax: <pre><code>// Find all paths between Alice and Bob (up to 4 hops)\nMATCH paths = (alice:Person {name: \"Alice\"})\n             -[:FRIEND_OF*..4]-(bob:Person {name: \"Bob\"})\nRETURN paths;\n</code></pre></p> <p>Filtered paths: <pre><code>// Find all collaboration paths through projects\nMATCH paths = (alice:Person {name: \"Alice\"})\n             -[:WORKED_ON]-&gt;(:Project)&lt;-[:WORKED_ON*..3]-(bob:Person {name: \"Bob\"})\nWHERE length(paths) &gt; 1  // At least 2 hops\nRETURN paths\nLIMIT 100;  // Prevent returning millions of paths\n</code></pre></p> <p>Why you'd want all paths: - Redundancy analysis: How many ways can information flow? - Risk assessment: If one connection fails, what are the alternatives? - Network analysis: Understanding structural properties of graphs</p> <p>Why all paths is dangerous: On a densely connected graph, the number of paths can grow exponentially. ALWAYS use LIMIT and max-length constraints.</p> <p>When the AI might overuse this: If you ask \"how is Alice related to Bob,\" a naive AI might use all paths, returning millions of results. Shortest path is usually better.</p>"},{"location":"chapters/04-query-languages/#declarative-vs-imperative-queries","title":"Declarative vs. Imperative Queries","text":"<p>One of the key concepts in query languages is the difference between declarative and imperative approaches.</p> <p>Declarative queries (Cypher's style): - You describe what you want, not how to find it - The database figures out the optimal execution plan - Easier to write, harder to optimize manually</p> <p>Example: <pre><code>// Declarative: \"Find Alice's friends over 30\"\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nWHERE friend.age &gt; 30\nRETURN friend.name;\n</code></pre></p> <p>You didn't specify: - Which index to use - Which node to start from - What traversal algorithm to use - How to filter results</p> <p>The query planner handles all that.</p> <p>Imperative queries (GSQL's style): - You specify how to execute the query step-by-step - More control over execution, more code complexity - Useful for optimizing complex queries on massive graphs</p> <p>Example (GSQL): <pre><code>CREATE QUERY FindOlderFriends(VERTEX&lt;Person&gt; alice) {\n  SumAccum&lt;INT&gt; @@count;\n\n  Start = {alice};\n\n  Friends = SELECT friend\n            FROM Start:s -(FRIEND_OF:e)-&gt; Person:friend\n            WHERE friend.age &gt; 30\n            ACCUM @@count += 1;\n\n  PRINT Friends, @@count;\n}\n</code></pre></p> <p>Here you explicitly: - Define accumulators (<code>@@count</code>) - Specify traversal start (<code>Start = {alice}</code>) - Control execution flow</p> <p>Which is better? Neither\u2014they're different tools for different jobs: - Declarative for most queries, rapid development, standard use cases - Imperative for performance-critical queries, complex aggregations, distributed processing</p> <p>AI implication: Most AI systems will generate declarative queries (Cypher) because they're simpler and more portable. If you need imperative control (GSQL), you might need to guide the AI more specifically.</p>"},{"location":"chapters/04-query-languages/#gsql-and-the-map-reduce-pattern","title":"GSQL and the Map-Reduce Pattern","text":"<p>Let's talk about GSQL and why TigerGraph designed a different approach.</p>"},{"location":"chapters/04-query-languages/#why-gsql-exists","title":"Why GSQL Exists","text":"<p>Cypher is great for small-to-medium graphs (millions of nodes). But when you hit billions of nodes and trillions of relationships across distributed clusters, declarative queries can struggle with optimization. GSQL was designed for this scale.</p> <p>GSQL's map-reduce pattern processes graphs in stages:</p> <ol> <li>Map: Transform each vertex/edge</li> <li>Reduce: Aggregate results</li> <li>Repeat: Iterate until convergence</li> </ol> <p>This mirrors big data processing frameworks (Hadoop MapReduce, Spark), but optimized for graphs.</p>"},{"location":"chapters/04-query-languages/#accumulators-gsqls-secret-weapon","title":"Accumulators: GSQL's Secret Weapon","text":"<p>Accumulators are variables that collect data during graph traversal.</p> <p>Types of accumulators: - <code>SumAccum&lt;INT&gt;</code> - Sum integers - <code>AvgAccum</code> - Calculate averages - <code>MaxAccum</code> / <code>MinAccum</code> - Track max/min values - <code>ListAccum&lt;STRING&gt;</code> - Collect lists - <code>SetAccum&lt;VERTEX&gt;</code> - Collect unique vertices</p> <p>Example: <pre><code>CREATE QUERY CountFriendsByCity(VERTEX&lt;Person&gt; alice) {\n  MapAccum&lt;STRING, INT&gt; @@cityCount;\n\n  Start = {alice};\n\n  Friends = SELECT friend\n            FROM Start -(:FRIEND_OF)-&gt; Person:friend\n            ACCUM @@cityCount += (friend.city -&gt; 1);\n\n  PRINT @@cityCount;\n}\n\n// Might return: {\"Seattle\": 5, \"Portland\": 3, \"SF\": 2}\n</code></pre></p> <p>Why accumulators matter: They let you aggregate data during traversal, not after. This is much faster on distributed systems because you're not shuffling data across network.</p> <p>Cypher equivalent (less efficient at scale): <pre><code>MATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nRETURN friend.city, count(friend)\nGROUP BY friend.city;\n</code></pre></p> <p>Both produce the same result, but on a 10-billion-edge graph, GSQL's accumulator approach can be orders of magnitude faster.</p> <p>When you'd use GSQL: - Billion+ node graphs - Distributed processing across clusters - Complex multi-hop aggregations - Graph algorithms (PageRank, community detection, centrality) - Real-time fraud detection at scale</p> <p>When you'd stick with Cypher: - Graphs under 100 million nodes - Standard CRUD operations - Rapid development - Team familiarity with declarative SQL-like syntax</p>"},{"location":"chapters/04-query-languages/#query-optimization-making-queries-fast","title":"Query Optimization: Making Queries Fast","text":"<p>Understanding query optimization helps you read query plans and spot performance issues.</p>"},{"location":"chapters/04-query-languages/#how-query-planners-work","title":"How Query Planners Work","text":"<p>When you write a Cypher query, the database doesn't execute it literally. It:</p> <ol> <li>Parses the query into an abstract syntax tree</li> <li>Optimizes by rewriting into equivalent but faster forms</li> <li>Generates a query plan specifying execution order</li> <li>Executes the plan</li> </ol> <p>View the query plan: <pre><code>EXPLAIN\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nWHERE friend.age &gt; 30\nRETURN friend.name;\n</code></pre></p> <p>This shows you what the database will do without actually running the query.</p> <p>Analyze actual execution: <pre><code>PROFILE\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nWHERE friend.age &gt; 30\nRETURN friend.name;\n</code></pre></p> <p>PROFILE runs the query and shows actual row counts, execution time per step.</p>"},{"location":"chapters/04-query-languages/#common-optimizations","title":"Common Optimizations","text":"<p>1. Index usage: <pre><code>// Slow (table scan)\nMATCH (p:Person)\nWHERE p.name = \"Alice\"\nRETURN p;\n\n// Fast (index seek) if you've created an index\nCREATE INDEX person_name FOR (p:Person) ON (p.name);\n</code></pre></p> <p>2. Filter early: <pre><code>// Slower: match everything, then filter\nMATCH (p:Person)-[:FRIEND_OF]-&gt;(friend)\nWHERE p.name = \"Alice\" AND friend.age &gt; 30\nRETURN friend;\n\n// Faster: filter Alice first, then traverse\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nWHERE friend.age &gt; 30\nRETURN friend;\n</code></pre></p> <p>3. Avoid Cartesian products: <pre><code>// Slow (Cartesian product: all people \u00d7 all companies)\nMATCH (p:Person), (c:Company)\nWHERE p.name = \"Alice\" AND c.name = \"TechCorp\"\nRETURN p, c;\n\n// Fast (connected pattern)\nMATCH (p:Person {name: \"Alice\"})-[:WORKS_AT]-&gt;(c:Company {name: \"TechCorp\"})\nRETURN p, c;\n</code></pre></p> <p>4. Use LIMIT when exploring: <pre><code>// Risky (might return millions)\nMATCH (p:Person)\nRETURN p;\n\n// Safe (stops after 100)\nMATCH (p:Person)\nRETURN p\nLIMIT 100;\n</code></pre></p>"},{"location":"chapters/04-query-languages/#query-performance-metrics","title":"Query Performance Metrics","text":"<p>Query latency: How long does one query take? - Good: &lt; 100ms - Acceptable: 100ms - 1s - Slow: &gt; 1s - Fix it: &gt; 10s</p> <p>Query throughput: How many queries per second can the system handle? - Measure with: Queries per second (QPS) - Affected by: Concurrency, caching, index quality, hardware</p> <p>Statistical Query Tuning: Use PROFILE to identify bottlenecks: <pre><code>PROFILE\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF*1..3]-(connected)\nRETURN count(connected);\n</code></pre></p> <p>Look for: - High db hits: Operations scanning too many nodes/relationships - Large row counts: Intermediate results that should be filtered earlier - Missing index usage: Scans instead of index seeks</p>"},{"location":"chapters/04-query-languages/#shortest-path-algorithms-under-the-hood","title":"Shortest Path Algorithms: Under the Hood","text":"<p>You've used <code>shortestPath()</code> in Cypher, but what's actually happening?</p>"},{"location":"chapters/04-query-languages/#breadth-first-search-bfs","title":"Breadth-First Search (BFS)","text":"<p>How it works: 1. Start at source node 2. Explore all neighbors (1-hop away) 3. Explore all neighbors' neighbors (2-hops away) 4. Continue until target found</p> <p>Why it finds shortest paths: BFS explores layer by layer, so first time it reaches the target is guaranteed to be the shortest path (for unweighted graphs).</p> <p>Cypher uses BFS for: <pre><code>shortestPath((alice)-[:FRIEND_OF*]-(bob))\n</code></pre></p> <p>Time complexity: O(V + E) where V = vertices, E = edges</p>"},{"location":"chapters/04-query-languages/#dijkstras-algorithm","title":"Dijkstra's Algorithm","text":"<p>How it works: 1. Assign tentative distances to all nodes (infinity, except source = 0) 2. Visit unvisited node with smallest distance 3. Update distances to neighbors 4. Repeat until target reached</p> <p>When you'd use it: Weighted graphs where relationships have costs.</p> <p>Example: <pre><code>// If FRIEND_OF had a 'distance' property\nMATCH path = shortestPath((alice)-[:FRIEND_OF*]-(bob))\nRETURN reduce(dist = 0, r IN relationships(path) | dist + r.distance) AS totalDistance;\n</code></pre></p> <p>Time complexity: O((V + E) log V) with priority queue</p>"},{"location":"chapters/04-query-languages/#a-algorithm","title":"A* Algorithm","text":"<p>How it works: Like Dijkstra, but uses a heuristic (estimated cost to goal) to explore promising paths first.</p> <p>When you'd use it: Spatial graphs (geographic networks, routing) where you have coordinate data to estimate distances.</p> <p>Example use case: Finding shortest driving route on road network graph.</p> <p>Time complexity: Depends on heuristic quality, often much faster than Dijkstra in practice</p>"},{"location":"chapters/04-query-languages/#why-you-care","title":"Why You Care","text":"<p>When the AI generates: <pre><code>shortestPath((a)-[:FRIEND_OF*]-(b))\n</code></pre></p> <p>You now know it's running BFS. If the query is slow, you know: - BFS is O(V + E), so it might be traversing millions of relationships - You could limit max hops: <code>shortestPath((a)-[:FRIEND_OF*..6]-(b))</code> - Or you could check if an index on Person.name exists to find <code>a</code> and <code>b</code> quickly</p> <p>See? Understanding algorithms helps you debug AI-generated code!</p>"},{"location":"chapters/04-query-languages/#query-plans-reading-the-execution-blueprint","title":"Query Plans: Reading the Execution Blueprint","text":"<p>Query plans show exactly how the database will execute your query.</p> <p>Get the plan without running: <pre><code>EXPLAIN\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nWHERE friend.age &gt; 30\nRETURN friend.name\nORDER BY friend.name;\n</code></pre></p> <p>Typical plan operations:</p> Operation What It Does Performance <code>NodeByLabelScan</code> Scan all nodes with a label Slow (O(n)) <code>NodeIndexSeek</code> Use index to find nodes Fast (O(log n)) <code>Expand(All)</code> Follow all relationships O(degree) per node <code>Filter</code> Apply WHERE conditions Depends on selectivity <code>Sort</code> Order results O(n log n) <code>Limit</code> Take first N results Fast <code>Distinct</code> Remove duplicates O(n) <p>Reading a plan: <pre><code>Plan:\n+------------------+--------+--------+\n| Operator         | Rows   | DB Hits|\n+------------------+--------+--------+\n| ProduceResults   |   12   |    0   |\n| Sort             |   12   |   24   |\n| Filter           |   12   |   45   |\n| Expand(All)      |   45   |   90   |\n| NodeIndexSeek    |    1   |    2   |\n+------------------+--------+--------+\n</code></pre></p> <p>Interpretation: 1. NodeIndexSeek (bottom): Found 1 Alice node using index (2 db hits) 2. Expand(All): Followed FRIEND_OF edges, found 45 friends (90 db hits) 3. Filter: Checked age &gt; 30, kept 12 results (45 db hits) 4. Sort: Sorted 12 results by name (24 db hits) 5. ProduceResults (top): Returned 12 rows</p> <p>Red flags to look for: - \u274c <code>NodeByLabelScan</code> when you expected <code>NodeIndexSeek</code> (missing index!) - \u274c Huge row counts early in plan that filter down later (filter earlier!) - \u274c <code>CartesianProduct</code> (accidental cross join) - \u274c High DB hits relative to rows returned (inefficient access pattern)</p> <p>When the AI's query is slow: Look at the plan. You might see it's doing a label scan instead of an index seek, meaning you need to create an index. Or it's expanding too broadly before filtering. Understanding plans = debugging superpowers.</p>"},{"location":"chapters/04-query-languages/#bringing-it-all-together-a-realistic-example","title":"Bringing It All Together: A Realistic Example","text":"<p>Let's build a complete query using everything we've learned. Imagine you're building a social network feature: \"People you may know.\"</p> <p>Requirements: - Find people who are friends with your friends (2-hop) - But exclude people you're already friends with - Prioritize people in the same city - Show people with mutual friends count - Limit to top 10 suggestions</p> <p>The query: <pre><code>MATCH (me:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)-[:FRIEND_OF]-&gt;(suggestion)\nWHERE NOT (me)-[:FRIEND_OF]-(suggestion)  // Not already friends\n  AND suggestion &lt;&gt; me                     // Not myself\nWITH suggestion, count(DISTINCT friend) AS mutualFriends, suggestion.city AS city, me.city AS myCity\nORDER BY\n  CASE WHEN city = myCity THEN 1 ELSE 0 END DESC,  // Same city first\n  mutualFriends DESC                                // Then by mutual friends\nRETURN\n  suggestion.name AS name,\n  suggestion.city AS city,\n  mutualFriends,\n  CASE WHEN city = myCity THEN 'Same city!' ELSE '' END AS note\nLIMIT 10;\n</code></pre></p> <p>Breaking it down: 1. MATCH: Find 2-hop friends 2. WHERE: Filter out existing friends and self 3. WITH: Aggregate mutual friends count, prepare for sorting 4. ORDER BY: Same city first, then by mutual friend count 5. RETURN: Format output nicely 6. LIMIT: Top 10 suggestions</p> <p>What an AI might get wrong: - Forgetting <code>suggestion &lt;&gt; me</code> (suggesting yourself) - Not using <code>DISTINCT</code> in count (counting same mutual friend multiple times if multiple paths exist) - Inefficient pattern (could use variable-length path with max 2 hops for clarity)</p> <p>Optimized version: <pre><code>// Create index first for better performance\nCREATE INDEX person_name IF NOT EXISTS FOR (p:Person) ON (p.name);\n\n// Optimized query\nMATCH (me:Person {name: \"Alice\"})\nMATCH (me)-[:FRIEND_OF]-&gt;(friend)-[:FRIEND_OF]-&gt;(suggestion)\nWHERE NOT (me)-[:FRIEND_OF]-(suggestion)\n  AND suggestion &lt;&gt; me\nWITH suggestion,\n     count(DISTINCT friend) AS mutualFriends,\n     me.city = suggestion.city AS sameCity\nORDER BY sameCity DESC, mutualFriends DESC\nRETURN suggestion.name, suggestion.city, mutualFriends\nLIMIT 10;\n</code></pre></p> <p>Performance considerations: - Index on <code>Person.name</code> makes finding Alice fast - Filtering <code>NOT (me)-[:FRIEND_OF]-(suggestion)</code> happens during traversal (efficient) - <code>LIMIT 10</code> stops early\u2014doesn't need to find all suggestions - <code>DISTINCT</code> in count prevents duplicate counting</p> <p>Now you can read this query, understand it, optimize it, and explain it\u2014even if an AI wrote it.</p>"},{"location":"chapters/04-query-languages/#the-bottom-line-why-learning-this-matters","title":"The Bottom Line: Why Learning This Matters","text":"<p>We opened this chapter with a wink: \"AI will probably write your queries.\" And that's likely true. But here's what we've learned:</p> <ol> <li>Reading code is a superpower: When the AI generates a query, you can understand what it does</li> <li>Debugging is essential: When queries fail or are slow, you can spot the issue</li> <li>Optimization requires knowledge: You can't tune what you don't understand</li> <li>Communication improves: \"Use MERGE, not CREATE\" is faster than explaining duplicates</li> <li>Trust but verify: You can review AI-generated code for correctness and efficiency</li> </ol> <p>Think of this chapter as query language literacy. You might not write Cypher from scratch every day, but you'll read it, review it, debug it, and optimize it. And when the AI suggests something that looks wrong, you'll have the knowledge to catch it.</p> <p>Final thought: AI is doubling every seven months, yes. But so is the amount of data we're storing and querying. The problems are growing as fast as the solutions. Understanding graph query languages isn't about whether AI can write them\u2014it's about understanding what needs to be written, why it works (or doesn't), and how to make it better.</p> <p>Plus, honestly? There's something deeply satisfying about reading a complex Cypher query and thinking, \"Yeah, I know exactly what that does.\" That's worth learning, AI or no AI. \ud83d\ude0a</p>"},{"location":"chapters/04-query-languages/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>OpenCypher is the people's champion: Most popular, visual syntax, widely adopted</li> <li>GSQL is for scale: Map-reduce patterns and accumulators for billion-node graphs</li> <li>GQL is the future: ISO standard emerging, SQL-like, future-proof</li> <li>MATCH finds patterns: Declarative pattern matching is Cypher's superpower</li> <li>WHERE filters, RETURN shapes: Basic clauses you'll see everywhere</li> <li>CREATE adds, MERGE upserts: CREATE makes duplicates, MERGE doesn't</li> <li>Variable-length paths are powerful but dangerous: Always set max length</li> <li>Shortest path uses BFS: Understanding algorithms helps debug performance</li> <li>Query plans reveal execution: EXPLAIN and PROFILE are your debugging friends</li> <li>Declarative vs imperative: Different tools for different scales</li> <li>Accumulators enable distributed aggregation: GSQL's secret sauce for massive graphs</li> <li>Optimization matters: Indexes, early filtering, avoiding Cartesian products</li> <li>AI will write queries, but you need to read them: Literacy &gt; authorship</li> </ol> <p>Now go forth and read graph queries with confidence! And when the AI inevitably tries to use <code>CREATE</code> where it should use <code>MERGE</code>, you'll catch it. \ud83d\ude09</p> <p>Remember: The best code is code you understand\u2014whether you wrote it or an AI did. This chapter gave you the tools to understand graph query languages. Use them wisely (and when the AI messes up, you know we told you so!)</p>"},{"location":"chapters/05-performance-metrics-benchmarking/","title":"Performance, Metrics, and Benchmarking","text":""},{"location":"chapters/05-performance-metrics-benchmarking/#summary","title":"Summary","text":"<p>This chapter explores the performance characteristics that make graph databases excel at relationship-heavy workloads. You'll learn about index-free adjacency's constant-time neighbor access, understand key performance metrics like hop count and node degree, and master indexing strategies including vector indexes and composite indexes. The chapter covers industry-standard benchmarks like LDBC SNB and Graph 500, teaching you how to measure query latency, throughput, and scalability while comparing traversal costs against traditional join operations.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 20 concepts from the learning graph:</p> <ol> <li>Hop Count</li> <li>Degree of Node</li> <li>Indegree</li> <li>Outdegree</li> <li>Edge-to-Node Ratio</li> <li>Graph Indexes</li> <li>Vector Indexes</li> <li>Full-Text Search</li> <li>Composite Indexes</li> <li>Graph Metrics</li> <li>Performance Benchmarking</li> <li>Synthetic Benchmarks</li> <li>Single-Node Benchmarks</li> <li>Multi-Node Benchmarks</li> <li>LDBC SNB Benchmark</li> <li>Graph 500</li> <li>Query Cost Analysis</li> <li>Join Operations</li> <li>Traversal Cost</li> <li>Scalability</li> </ol>"},{"location":"chapters/05-performance-metrics-benchmarking/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 3: Labeled Property Graph Information Model</li> <li>Chapter 4: Query Languages for Graph Databases</li> </ul>"},{"location":"chapters/05-performance-metrics-benchmarking/#show-me-the-numbers-why-skepticism-makes-you-a-better-engineer","title":"Show Me the Numbers: Why Skepticism Makes You a Better Engineer","text":"<p>Let's get something straight: If your colleagues are skeptical when you propose using a graph database, that's a good sign. It means you work with real engineers who don't just chase the latest trend or trust vendor marketing. Skepticism is the foundation of good engineering.</p> <p>You've probably heard the claims: \"Graph databases are 1000\u00d7 faster for connected data!\" \"Traversals that take minutes in SQL run in milliseconds!\" \"This will revolutionize our data layer!\"</p> <p>And you should absolutely, positively, 100% question those claims.</p> <p>Here's the thing though: good engineers don't just stay skeptical\u2014they test. They measure. They benchmark. They gather data. And then they make informed decisions based on evidence, not hype.</p> <p>This chapter is about becoming that engineer. The one who walks into a meeting with actual benchmark results. The one who can say, \"I tested this on our data model, and here's what I found.\" The one whose opinion carries weight because it's backed by numbers, not vendor promises.</p> <p>When you propose a graph database (or any technology) and someone says, \"Prove it,\" you'll be able to say, \"I did. Here are the benchmarks.\" That's how you build a reputation as someone who evaluates objectively, measures rigorously, and makes data-driven decisions.</p> <p>Your skeptical colleagues will respect that. And honestly? You should be skeptical too. Trust, but verify. Or better yet: verify, then trust.</p> <p>Ready to learn how to benchmark graph databases properly? Let's dive into the metrics that matter, the tests that prove (or disprove) the claims, and how to build your reputation as the engineer who brings receipts.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#graph-metrics-the-numbers-that-tell-the-story","title":"Graph Metrics: The Numbers That Tell the Story","text":"<p>Before we can benchmark anything, we need to understand what to measure. Graph databases have some unique performance characteristics tied to graph structure. Let's break down the key metrics.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#hop-count-measuring-relationship-distance","title":"Hop Count: Measuring Relationship Distance","text":"<p>Hop count is the number of edges you traverse to get from one node to another. It's one of the most important performance metrics for graph databases.</p> <p>Why it matters: In graph databases, query performance is heavily influenced by how many hops you need to traverse. Remember the performance cliff from Chapter 1? That's hop count in action.</p> <p>Examples: - 1-hop: Alice's direct friends \u2192 <code>(alice)-[:FRIEND_OF]-&gt;(friend)</code> - 2-hop: Friends of friends \u2192 <code>(alice)-[:FRIEND_OF]-&gt;()-[:FRIEND_OF]-&gt;(fof)</code> - 3-hop: Friends of friends of friends \u2192 <code>(alice)-[:FRIEND_OF*3]-(connection)</code></p> <p>Performance relationship: - Graph databases: Linear or near-linear scaling with hop count - RDBMS with JOINs: Exponential degradation with hop count</p> <p>Measuring hop count in queries: <pre><code>// Find connections up to 3 hops away and count path lengths\nMATCH path = (alice:Person {name: \"Alice\"})-[:FRIEND_OF*1..3]-(connected)\nRETURN length(path) AS hops, count(connected) AS connections\nORDER BY hops;\n</code></pre></p> <p>Benchmark insight: When you benchmark, always measure performance across different hop counts (1, 2, 3, 4, 5). This shows whether the database maintains linear performance or hits the exponential cliff.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#degree-of-node-measuring-connectivity","title":"Degree of Node: Measuring Connectivity","text":"<p>Degree is the number of relationships connected to a node. High-degree nodes (hubs) can significantly impact query performance.</p> <p>Why it matters: Traversing from a high-degree node can be expensive if you're following all relationships. A celebrity with 10 million followers is a very different query than a regular user with 200 friends.</p> <p>Types of degree:</p> <p>Degree (total): <pre><code>// Count all relationships (incoming + outgoing)\nMATCH (alice:Person {name: \"Alice\"})-[r]-()\nRETURN count(r) AS degree;\n</code></pre></p> <p>Use case: Understanding overall connectivity, finding hub nodes, identifying influencers</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#indegree-incoming-relationships","title":"Indegree: Incoming Relationships","text":"<p>Indegree is the number of incoming relationships (edges pointing to the node).</p> <pre><code>// Count incoming relationships\nMATCH (alice:Person {name: \"Alice\"})&lt;-[r]-()\nRETURN count(r) AS indegree;\n</code></pre> <p>Examples: - Social media: Follower count (indegree) vs following count (outdegree) - Citation graphs: How many papers cite this paper (indegree) - Supply chain: How many suppliers feed into this factory (indegree)</p> <p>Benchmark insight: High-indegree nodes can slow down certain queries. When benchmarking, test queries that traverse from high-indegree nodes to see if performance degrades.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#outdegree-outgoing-relationships","title":"Outdegree: Outgoing Relationships","text":"<p>Outdegree is the number of outgoing relationships (edges pointing from the node).</p> <pre><code>// Count outgoing relationships\nMATCH (alice:Person {name: \"Alice\"})-[r]-&gt;()\nRETURN count(r) AS outdegree;\n</code></pre> <p>Examples: - Social media: Following count - Citation graphs: How many papers this paper cites - Supply chain: How many customers this supplier serves</p> <p>Performance consideration: Expanding from a high-outdegree node (e.g., a user who follows 50,000 people) can be slow if you need to process all relationships.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#degree-distribution-the-shape-of-your-graph","title":"Degree Distribution: The Shape of Your Graph","text":"<p>Real-world graphs often follow a power-law distribution: most nodes have low degree, a few nodes have very high degree (the \"hubs\").</p> <p>Why this matters for benchmarking: - Queries that hit low-degree nodes perform differently than queries hitting hubs - Benchmarks should test both average-degree and high-degree scenarios - Your synthetic test data should match the degree distribution of your real data</p> <p>Finding high-degree nodes: <pre><code>// Find top 10 most connected people\nMATCH (p:Person)-[r]-()\nRETURN p.name, count(r) AS degree\nORDER BY degree DESC\nLIMIT 10;\n</code></pre></p> <p>Benchmark best practice: When generating synthetic data, make sure degree distribution matches production. If your production graph has power-law distribution, your benchmark should too.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#edge-to-node-ratio-graph-density","title":"Edge-to-Node Ratio: Graph Density","text":"<p>Edge-to-node ratio measures how densely connected your graph is.</p> <p>Formula: <pre><code>Edge-to-Node Ratio = Total Edges / Total Nodes\n</code></pre></p> <p>Examples: - Sparse graph: E/N = 2 (average node has 2 connections) - Medium density: E/N = 10 (average node has 10 connections) - Dense graph: E/N = 100 (average node has 100 connections)</p> <p>Why it matters: Dense graphs behave differently than sparse graphs. Variable-length path queries on dense graphs can explode combinatorially.</p> <p>Measuring your graph: <pre><code>// Calculate edge-to-node ratio\nMATCH (n)\nWITH count(n) AS nodeCount\nMATCH ()-[r]-&gt;()\nRETURN count(r) AS edgeCount,\n       nodeCount,\n       count(r) * 1.0 / nodeCount AS ratio;\n</code></pre></p> <p>Benchmark insight: Always report edge-to-node ratio when publishing benchmark results. A benchmark on a sparse graph (E/N = 2) vs dense graph (E/N = 50) tells very different stories.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#graph-indexes-making-queries-fast","title":"Graph Indexes: Making Queries Fast","text":"<p>Indexes are critical for graph database performance. You can't benchmark properly without understanding how indexes work.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#graph-indexes-finding-starting-points","title":"Graph Indexes: Finding Starting Points","text":"<p>Graph indexes help you find nodes quickly to start your traversal. Without indexes, you're scanning every node\u2014slow and painful.</p> <p>Why they matter: Most Cypher queries start with a MATCH that finds specific nodes: <pre><code>MATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF]-&gt;(friend)\nRETURN friend;\n</code></pre></p> <p>That <code>{name: \"Alice\"}</code> lookup needs an index, or the database scans all Person nodes.</p> <p>Creating indexes in Neo4j: <pre><code>// Index on Person.name\nCREATE INDEX person_name FOR (p:Person) ON (p.name);\n\n// Index on Person.email\nCREATE INDEX person_email FOR (p:Person) ON (p.email);\n\n// Index on Product.sku\nCREATE INDEX product_sku FOR (p:Product) ON (p.sku);\n</code></pre></p> <p>Benchmark impact: - Without index: Query scans all nodes \u2192 O(n) time - With index: Query uses index seek \u2192 O(log n) time</p> <p>On a 10-million-node graph: - Without index: ~5 seconds - With index: ~5 milliseconds</p> <p>That's a 1000\u00d7 difference!</p> <p>Benchmark best practice: When comparing graph databases to RDBMS, make sure both have appropriate indexes. An unfair comparison (indexed RDBMS vs non-indexed graph DB) proves nothing.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#vector-indexes-similarity-search-at-scale","title":"Vector Indexes: Similarity Search at Scale","text":"<p>Vector indexes enable similarity search and nearest-neighbor queries, crucial for recommendation systems and AI applications.</p> <p>What they do: Store high-dimensional vectors (embeddings) and enable fast similarity search.</p> <p>Example use case: <pre><code>// Find products similar to what Alice bought (using vector embeddings)\nMATCH (alice:Person {name: \"Alice\"})-[:PURCHASED]-&gt;(product:Product)\nCALL db.index.vector.queryNodes('productEmbeddings', 5, product.embedding)\nYIELD node AS similar, score\nRETURN similar.name, score\nLIMIT 10;\n</code></pre></p> <p>Why this matters for graph databases: Modern applications combine graph relationships with vector similarity. You might: - Find friends who like similar content (graph + vectors) - Recommend products based on purchase patterns AND product similarity - Detect fraud rings using network structure AND behavioral embeddings</p> <p>Benchmark consideration: If your use case involves similarity search, benchmark vector index performance alongside graph traversal. Many graph databases now support vector indexes natively (Neo4j, Amazon Neptune, TigerGraph).</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#full-text-search-finding-nodes-by-content","title":"Full-Text Search: Finding Nodes by Content","text":"<p>Full-text search indexes allow searching node properties using text matching, wildcards, and fuzzy search.</p> <p>Creating full-text index: <pre><code>// Create full-text index on Person name and bio\nCREATE FULLTEXT INDEX personSearch FOR (p:Person) ON EACH [p.name, p.bio];\n</code></pre></p> <p>Using full-text search: <pre><code>// Find people whose name or bio contains \"graph database\"\nCALL db.index.fulltext.queryNodes('personSearch', 'graph database')\nYIELD node, score\nRETURN node.name, node.bio, score\nORDER BY score DESC;\n</code></pre></p> <p>Benchmark scenario: If your application needs text search (e.g., \"find all customers who mentioned 'defect' in support tickets\"), include full-text queries in your benchmark suite.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#composite-indexes-multi-property-lookups","title":"Composite Indexes: Multi-Property Lookups","text":"<p>Composite indexes index multiple properties together, enabling fast lookups on property combinations.</p> <p>Example: <pre><code>// Composite index on city and age\nCREATE INDEX person_city_age FOR (p:Person) ON (p.city, p.age);\n\n// Fast lookup: people in Seattle aged 30-40\nMATCH (p:Person)\nWHERE p.city = \"Seattle\" AND p.age &gt;= 30 AND p.age &lt;= 40\nRETURN p.name, p.age;\n</code></pre></p> <p>Why composite indexes matter: Single-property indexes only help when filtering on that property. If you filter on city AND age, a composite index can use both properties for the lookup.</p> <p>Benchmark insight: When benchmarking, test realistic query patterns. If your application frequently filters on multiple properties, composite indexes can dramatically improve performance.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#performance-benchmarking-measuring-what-matters","title":"Performance Benchmarking: Measuring What Matters","text":"<p>Now we get to the heart of this chapter: actually measuring performance. This is where you stop trusting vendor claims and start generating your own data.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#why-benchmark","title":"Why Benchmark?","text":"<p>Good reasons to benchmark: 1. Verify vendor claims: Is it really 1000\u00d7 faster? Let's find out. 2. Compare alternatives: RDBMS vs Graph DB vs Document DB\u2014which fits your data? 3. Capacity planning: How many QPS can this system handle? 4. Optimize queries: Which query pattern is fastest? 5. Build credibility: Walk into meetings with data, not opinions</p> <p>Bad reasons to benchmark: 1. \u274c \"I want graphs to win\" (confirmation bias) 2. \u274c \"I need to justify a decision I already made\" (motivated reasoning) 3. \u274c \"The vendor showed me a benchmark\" (trusting without verifying)</p> <p>The mindset: A good benchmark should be designed to find the truth, even if that truth is \"graph databases aren't right for this use case.\"</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#what-to-measure","title":"What to Measure","text":"<p>Key performance metrics:</p> <ol> <li>Query latency: How long does one query take?</li> <li>Median (p50): Typical case</li> <li>95th percentile (p95): Most users' experience</li> <li>99th percentile (p99): Worst typical case</li> <li> <p>Max: Absolute worst case</p> </li> <li> <p>Query throughput: How many queries per second (QPS) can the system handle?</p> </li> <li>Under light load (10 concurrent users)</li> <li>Under realistic load (100 concurrent users)</li> <li> <p>Under stress (1000+ concurrent users)</p> </li> <li> <p>Scalability: How does performance change as data grows?</p> </li> <li>1 million nodes</li> <li>10 million nodes</li> <li>100 million nodes</li> <li> <p>1 billion nodes</p> </li> <li> <p>Resource usage:</p> </li> <li>CPU utilization</li> <li>Memory consumption</li> <li>Disk I/O</li> <li>Network bandwidth (for distributed systems)</li> </ol> <p>Measuring latency in Cypher: <pre><code>// Use PROFILE to see actual execution time\nPROFILE\nMATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF*2]-(fof)\nRETURN count(DISTINCT fof);\n</code></pre></p> <p>External benchmarking: For accurate measurements, use external tools that measure end-to-end latency including network overhead: - Apache JMeter - Gatling - Custom scripts with time measurements</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#synthetic-benchmarks-controlled-testing","title":"Synthetic Benchmarks: Controlled Testing","text":"<p>Synthetic benchmarks use generated data and predefined queries to test specific performance characteristics.</p> <p>Advantages: - Controlled: You control data size, structure, and query patterns - Reproducible: Others can replicate your results - Scalable: Easy to generate 1M, 10M, 100M node datasets - Comparable: Industry-standard benchmarks let you compare across systems</p> <p>Disadvantages: - Artificial: May not reflect your real workload - Optimizable: Vendors can tune for specific benchmarks - Misleading: High synthetic performance doesn't guarantee real-world performance</p> <p>When to use synthetic benchmarks: - Comparing different graph databases - Testing specific features (shortest path, variable-length paths) - Capacity planning (how does it scale?) - Proving/disproving specific performance claims</p> <p>Example synthetic workload: <pre><code>Dataset: 10 million Person nodes, 100 million FRIEND_OF edges\nQueries:\n  Q1: Find person by name (index lookup)\n  Q2: Find person's friends (1-hop traversal)\n  Q3: Find friends of friends (2-hop traversal)\n  Q4: Find shortest path between two people\n  Q5: Count mutual friends between two people\n  Q6: Find people within 3 hops\n</code></pre></p>"},{"location":"chapters/05-performance-metrics-benchmarking/#single-node-benchmarks-testing-one-server","title":"Single-Node Benchmarks: Testing One Server","text":"<p>Single-node benchmarks test performance on a single database server (not distributed).</p> <p>What they measure: - Raw query performance - Index efficiency - Memory management - Single-machine limits</p> <p>Example setup: - Hardware: 64GB RAM, 16 CPU cores, SSD storage - Database: Neo4j Community Edition, single instance - Dataset: LDBC SNB SF10 (10GB scale factor) - Workload: Interactive queries (1-7 hops)</p> <p>Why single-node benchmarks matter: Most applications start with a single database server. Understanding single-node performance helps you know when you need to scale out.</p> <p>Benchmark best practice: Always specify hardware specs when publishing results. \"Graph DB is faster\" means nothing without context. \"Graph DB on 64GB machine vs PostgreSQL on 64GB machine\" is actionable data.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#multi-node-benchmarks-testing-distributed-systems","title":"Multi-Node Benchmarks: Testing Distributed Systems","text":"<p>Multi-node benchmarks test performance on distributed database clusters.</p> <p>What they measure: - Horizontal scalability (add more servers = more performance?) - Network overhead - Distributed query coordination - Fault tolerance</p> <p>Example setup: - Cluster: 10 nodes \u00d7 64GB RAM each - Database: TigerGraph distributed cluster - Dataset: Graph500 scale 30 (1 billion edges) - Workload: BFS, PageRank, Connected Components</p> <p>Key metrics: - Speedup: How much faster is 10 nodes vs 1 node?   - Linear speedup: 10\u00d7 faster (ideal)   - Sub-linear: 5\u00d7 faster (realistic)   - Worse: 2\u00d7 faster (poor scaling)</p> <ul> <li>Efficiency: Speedup / Number of nodes</li> <li>100% efficiency: 10 nodes = 10\u00d7 faster</li> <li>50% efficiency: 10 nodes = 5\u00d7 faster</li> <li>20% efficiency: 10 nodes = 2\u00d7 faster (not good)</li> </ul> <p>Why multi-node benchmarks matter: If you're considering a distributed graph database for billion-node graphs, you need to know if it actually scales or just adds operational complexity.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#industry-standard-benchmarks-the-tests-everyone-uses","title":"Industry-Standard Benchmarks: The Tests Everyone Uses","text":"<p>Instead of creating benchmarks from scratch, you can use industry-standard benchmarks that have been carefully designed and widely adopted.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#ldbc-snb-the-social-network-benchmark","title":"LDBC SNB: The Social Network Benchmark","text":"<p>LDBC Social Network Benchmark (SNB) is the gold standard for graph database benchmarking. It simulates a social network like Facebook or LinkedIn.</p> <p>What it includes: - Data generator: Creates synthetic social network data at various scales - Workloads:   - Interactive (BI): Short, frequent queries (OLTP-style)   - Business Intelligence: Complex analytical queries (OLAP-style)   - Graph Analytics: Algorithm benchmarks</p> <p>Scale factors: - SF1: ~1GB data (~1M persons) - SF10: ~10GB data (~10M persons) - SF100: ~100GB data (~100M persons) - SF1000: ~1TB data (~1B persons)</p> <p>Example queries: - IC1: Find friends and recent posts (1-2 hops) - IC2: Find recent messages from friends (1-hop + filtering) - IC3: Find friends who know specific people in specific locations (2-3 hops) - IC13: Find shortest path between two people</p> <p>Why LDBC SNB is valuable: 1. Realistic: Based on actual social network patterns 2. Comprehensive: Tests many query types 3. Scalable: Multiple scale factors 4. Comparable: Published results from Neo4j, TigerGraph, Amazon Neptune, etc. 5. Audited: Results can be officially audited for fairness</p> <p>Running LDBC SNB: <pre><code># Generate SF10 dataset\n./ldbc_snb_datagen --scale-factor 10\n\n# Load into Neo4j\n./ldbc_snb_loader --database neo4j --data-dir ./sf10\n\n# Run interactive workload\n./ldbc_snb_driver --database neo4j --workload interactive\n</code></pre></p> <p>Benchmark credibility: If you publish LDBC SNB results, people will take them seriously. It's the industry standard.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#graph-500-the-supercomputer-benchmark","title":"Graph 500: The Supercomputer Benchmark","text":"<p>Graph 500 is a benchmark for very large-scale graph processing, originally designed for supercomputers and HPC systems.</p> <p>What it measures: - Breadth-First Search (BFS): Starting from a source vertex, visit all reachable vertices - Single-Source Shortest Paths (SSSP): Find shortest paths from one vertex to all others - Performance metric: Traversed Edges Per Second (TEPS)</p> <p>Scale: - Scale 20: ~1 million vertices, ~10 million edges - Scale 30: ~1 billion vertices, ~17 billion edges - Scale 40: ~1 trillion vertices, ~17 trillion edges</p> <p>Example problem: <pre><code>Given: Graph with 1 billion vertices, 17 billion edges\nTask: Perform BFS from a random starting vertex\nMetric: How many edges per second can you traverse?\n\nTop systems: ~100 billion TEPS\nTypical graph DB: ~1-10 million TEPS (very different scale!)\n</code></pre></p> <p>Why Graph 500 matters: It shows the absolute limits of graph processing. Supercomputers achieve billions of TEPS. Graph databases achieve millions of TEPS but with much more query flexibility.</p> <p>When to use Graph 500: - Testing distributed graph databases - Comparing graph algorithms (BFS, PageRank, etc.) - Understanding theoretical limits - Not useful for typical CRUD operations</p> <p>Benchmark insight: Graph 500 shows that specialized HPC systems are faster for pure graph algorithms, but graph databases offer rich query languages, ACID transactions, and operational features that HPC systems don't.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#query-cost-analysis-understanding-performance-trade-offs","title":"Query Cost Analysis: Understanding Performance Trade-offs","text":"<p>Let's get quantitative about why graph databases outperform RDBMS for relationship queries.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#join-operations-the-rdbms-approach","title":"Join Operations: The RDBMS Approach","text":"<p>Join operations are how RDBMS systems traverse relationships. Each hop requires a JOIN.</p> <p>Example: 3-hop friend query in SQL <pre><code>-- Find friends of friends of friends\nSELECT DISTINCT p4.name\nFROM persons p1\nJOIN friendships f1 ON p1.id = f1.person1_id\nJOIN persons p2 ON f1.person2_id = p2.id\nJOIN friendships f2 ON p2.id = f2.person1_id\nJOIN persons p3 ON f2.person2_id = p3.id\nJOIN friendships f3 ON p3.id = f3.person1_id\nJOIN persons p4 ON f3.person2_id = p4.id\nWHERE p1.name = 'Alice';\n</code></pre></p> <p>Cost analysis:</p> <p>Per JOIN operation: 1. Find rows in first table: O(log n) with index, O(n) without 2. For each row, look up matching rows in second table: O(m log k) where m = rows from first table, k = rows in second table 3. Build result set: O(m \u00d7 r) where r = average matches per row</p> <p>For 3 hops: - First JOIN: O(n\u2081 log n\u2082) - Second JOIN: O(intermediate\u2081 log n\u2083) - Third JOIN: O(intermediate\u2082 log n\u2084)</p> <p>Problem: Intermediate result sets grow exponentially with hops. On a social network: - 1 hop: 200 friends (200 rows) - 2 hops: 200 \u00d7 200 = 40,000 rows (intermediate set) - 3 hops: 40,000 \u00d7 200 = 8,000,000 rows (intermediate set)</p> <p>Database must: - Build intermediate result sets in memory (or spill to disk\u2014even slower) - Perform nested loop joins or hash joins (both expensive) - Filter duplicates at the end</p> <p>Why it's slow: The work grows exponentially with hops, even with indexes.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#traversal-cost-the-graph-database-approach","title":"Traversal Cost: The Graph Database Approach","text":"<p>Traversal in graph databases follows pointers directly from node to node.</p> <p>Same 3-hop query in Cypher: <pre><code>MATCH (alice:Person {name: \"Alice\"})-[:FRIEND_OF*3]-(connection)\nRETURN DISTINCT connection.name;\n</code></pre></p> <p>Cost analysis:</p> <p>Per hop: 1. Find starting node: O(log n) with index 2. Read relationship pointers: O(1) per relationship (index-free adjacency) 3. Follow pointer to next node: O(1) per hop 4. Check filters: O(1) per node</p> <p>For 3 hops with 200 friends per person: - Find Alice: O(log n) \u2248 20 operations (with index on 1M nodes) - Traverse 1st hop: 200 pointer lookups = 200 operations - Traverse 2nd hop: 200 \u00d7 200 = 40,000 operations - Traverse 3rd hop: 40,000 \u00d7 200 = 8,000,000 operations</p> <p>Wait, that's the same number! Why is it faster?</p> <p>The difference: 1. No intermediate result sets: Graph DBs don't build 40,000-row tables in memory 2. Pointer lookups vs index lookups: O(1) pointer dereference vs O(log n) index seek 3. Locality of reference: Graph data stored together, better cache performance 4. No JOIN overhead: No hash tables, no nested loops, no sort-merge</p> <p>In practice: - RDBMS 3-hop: 3-10 seconds (with indexes) - Graph DB 3-hop: 10-50 milliseconds</p> <p>That's 100-1000\u00d7 faster.</p> <p>Why? Constant factors matter. O(1) pointer lookup vs O(log n) index lookup, multiplied by millions of operations, creates huge differences.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#comparative-benchmark-rdbms-vs-graph-db","title":"Comparative Benchmark: RDBMS vs Graph DB","text":"<p>Let's look at real benchmark numbers (these are representative of published results):</p> <p>Dataset: Social network, 10 million people, 100 million friendships</p> <p>Query: Find friends up to N hops away</p> Hops RDBMS (PostgreSQL) Graph DB (Neo4j) Speedup 1 12 ms 5 ms 2.4\u00d7 2 185 ms 7 ms 26\u00d7 3 3,400 ms 11 ms 309\u00d7 4 58,000 ms 14 ms 4,142\u00d7 5 timeout (&gt;10 min) 18 ms &gt;33,000\u00d7 <p>Observations: 1. 1-hop: Graph DB only 2\u00d7 faster (both are fast) 2. 2-3 hops: Graph DB 10-300\u00d7 faster (noticeable difference) 3. 4+ hops: Graph DB 1000s of times faster (RDBMS becomes unusable)</p> <p>The performance cliff: Around 2-3 hops, RDBMS performance falls off a cliff. Graph databases maintain near-constant performance.</p> <p>Benchmark credibility: These numbers match published LDBC results and vendor benchmarks. You can replicate them yourself.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#scalability-how-systems-grow","title":"Scalability: How Systems Grow","text":"<p>Scalability measures how performance changes as data size or load increases.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#types-of-scalability","title":"Types of Scalability","text":"<p>Vertical scaling (scale up): - Add more resources to one server (CPU, RAM, SSD) - Limits: Single machine limits (~1TB RAM, ~100 CPU cores) - Cost: Expensive (high-end servers cost $$$$)</p> <p>Horizontal scaling (scale out): - Add more servers to a cluster - Limits: Coordination overhead, network bandwidth - Cost: Cheaper per node, but operational complexity</p> <p>Graph databases support both: - Single-node: Neo4j, Amazon Neptune (serverless), Memgraph - Distributed: TigerGraph, Neo4j Aura Enterprise, JanusGraph, Dgraph</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#measuring-scalability","title":"Measuring Scalability","text":"<p>Ideal scaling: - 2\u00d7 data \u2192 same query time (constant performance) - 2\u00d7 servers \u2192 2\u00d7 throughput (linear horizontal scaling)</p> <p>Real-world scaling: - 2\u00d7 data \u2192 1.5\u00d7 query time (sub-linear degradation\u2014acceptable) - 2\u00d7 servers \u2192 1.6\u00d7 throughput (80% efficiency\u2014good)</p> <p>Scalability benchmark: <pre><code>Test: Measure query latency as data grows\n\nDataset sizes: 1M, 10M, 100M, 1B nodes\nQuery: Find friends within 3 hops\n\nResults:\n  1M nodes: 5 ms\n  10M nodes: 8 ms (1.6\u00d7 slower for 10\u00d7 data\u2014excellent)\n  100M nodes: 15 ms (3\u00d7 slower for 100\u00d7 data\u2014good)\n  1B nodes: 45 ms (9\u00d7 slower for 1000\u00d7 data\u2014acceptable)\n</code></pre></p> <p>Log-scale growth: Query time grows logarithmically with data size (due to index lookups). This is excellent scalability.</p> <p>Poor scalability example: <pre><code>RDBMS 3-hop query as data grows:\n\n  1M nodes: 200 ms\n  10M nodes: 3,000 ms (15\u00d7 slower)\n  100M nodes: 60,000 ms (300\u00d7 slower)\n  1B nodes: timeout\n</code></pre></p> <p>Linear growth: Query time grows linearly or worse with data size. This doesn't scale.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#benchmark-best-practices-doing-it-right","title":"Benchmark Best Practices: Doing It Right","text":"<p>Now that you understand the metrics and benchmarks, let's talk about how to run fair, credible benchmarks.</p> <p>1. Match the hardware: - Same CPU, RAM, SSD across all systems tested - Document specs: \"64GB RAM, 16 cores, 1TB NVMe SSD\"</p> <p>2. Use appropriate indexes: - Don't compare indexed Graph DB to non-indexed RDBMS - Create indexes that match query patterns - Report which indexes you created</p> <p>3. Warm up the cache: - Run queries multiple times before measuring - Report cold-cache and warm-cache performance separately</p> <p>4. Measure realistic workloads: - Don't just test best-case queries - Include complex queries that stress the system - Test at realistic concurrency levels</p> <p>5. Report distributions, not just averages: - Median (p50), 95th percentile (p95), 99th percentile (p99) - Max latency matters for user experience</p> <p>6. Test at realistic data sizes: - If production will have 100M nodes, test at 100M nodes - Synthetic benchmarks at 1M nodes don't predict 100M node performance</p> <p>7. Document everything: - Database versions - Configuration settings (cache size, query timeout, etc.) - Data model (schema, indexes) - Query text - Hardware specs - Methodology (how you generated load, measured latency, etc.)</p> <p>8. Share your results: - Publish to GitHub - Write a blog post - Present at meetups - Let others replicate and verify</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#building-your-reputation-the-data-driven-engineer","title":"Building Your Reputation: The Data-Driven Engineer","text":"<p>Here's why all this benchmarking knowledge matters for your career:</p> <p>Scenario 1: The Skeptical Tech Lead</p> <p>You propose using a graph database for the new recommendation engine.</p> <p>Tech Lead: \"Graph databases are just hype. SQL works fine.\"</p> <p>You (without benchmarks): \"But... the vendor said it's 1000\u00d7 faster...\"</p> <p>Result: Dismissed as naive.</p> <p>You (with benchmarks): \"I benchmarked our data model\u201410M users, 100M relationships. Here's what I found:\"</p> Query PostgreSQL Neo4j Speedup 2-hop friends 180 ms 7 ms 25\u00d7 3-hop recommendations 3.2 sec 11 ms 290\u00d7 <p>\"I ran LDBC SNB at SF10 scale. Graph DB stays under 50ms for all queries. PostgreSQL timeouts at 3 hops. Here's the GitHub repo with my test scripts.\"</p> <p>Result: Tech lead respects you. You get to run a proof-of-concept.</p> <p>Scenario 2: The Budget Discussion</p> <p>CFO: \"Why do we need to spend $50k/year on a graph database?\"</p> <p>You (without data): \"It's faster...?\"</p> <p>Result: Budget denied.</p> <p>You (with data): \"Our current recommendation engine runs overnight batch jobs\u201412-hour runtime. Users see stale recommendations. I benchmarked a graph database: same recommendations in 50 milliseconds, real-time. We can retire 20 batch processing servers ($80k/year cost). ROI is positive year one.\"</p> <p>Result: Budget approved. You're a hero.</p> <p>Scenario 3: The Conference Talk</p> <p>You: \"I compared graph databases to relational databases for social network queries. Here are my LDBC SNB results, reproduced on identical hardware. You can replicate my benchmarks using this GitHub repo.\"</p> <p>Result: Your talk gets accepted. Your blog post gets shared. Recruiters message you. Your reputation as a rigorous, data-driven engineer grows.</p> <p>The pattern: Benchmarking isn't just about measuring databases. It's about building credibility. Engineers who bring data to discussions are respected. Engineers who trust vendor marketing are not.</p> <p>Your reputation is built on moments like these. When you can say, \"I tested it, here are the numbers,\" you become someone whose opinion matters.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#running-your-own-benchmarks-a-practical-guide","title":"Running Your Own Benchmarks: A Practical Guide","text":"<p>Ready to actually run some benchmarks? Here's how to get started.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#step-1-define-your-questions","title":"Step 1: Define Your Questions","text":"<p>Don't benchmark randomly. Start with specific questions:</p> <p>Good questions: - \"Can a graph database handle our 3-hop recommendation query in under 100ms?\" - \"How does Neo4j compare to PostgreSQL for our specific data model?\" - \"Will query performance degrade as we grow from 10M to 100M nodes?\"</p> <p>Bad questions: - \"Which database is faster?\" (too vague) - \"Is Neo4j good?\" (not measurable)</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#step-2-model-your-data","title":"Step 2: Model Your Data","text":"<p>Create a representative data model:</p> <pre><code>// Example: E-commerce graph\nCREATE (:Customer {id: 1, name: \"Alice\", city: \"Seattle\"})\nCREATE (:Product {id: 101, name: \"Laptop\", price: 899})\nCREATE (:Category {id: 1, name: \"Electronics\"})\n\nCREATE (c:Customer {id: 1})-[:PURCHASED {date: \"2024-01-15\", price: 899}]-&gt;(p:Product {id: 101})\nCREATE (p)-[:IN_CATEGORY]-&gt;(cat:Category {id: 1})\n</code></pre> <p>Scale it: Use LDBC data generator or write scripts to generate millions of nodes.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#step-3-write-benchmark-queries","title":"Step 3: Write Benchmark Queries","text":"<pre><code>// Q1: Find customer by ID (index lookup)\nMATCH (c:Customer {id: $customerId})\nRETURN c;\n\n// Q2: Find customer's purchases (1-hop)\nMATCH (c:Customer {id: $customerId})-[:PURCHASED]-&gt;(p:Product)\nRETURN p;\n\n// Q3: Find similar customers (2-hop)\nMATCH (c:Customer {id: $customerId})-[:PURCHASED]-&gt;(p:Product)&lt;-[:PURCHASED]-(similar)\nRETURN similar, count(p) AS commonProducts\nORDER BY commonProducts DESC\nLIMIT 10;\n\n// Q4: Product recommendations (3-hop)\nMATCH (c:Customer {id: $customerId})-[:PURCHASED]-&gt;(:Product)&lt;-[:PURCHASED]-(similar)\n     -[:PURCHASED]-&gt;(rec:Product)\nWHERE NOT (c)-[:PURCHASED]-&gt;(rec)\nRETURN rec, count(similar) AS score\nORDER BY score DESC\nLIMIT 10;\n</code></pre>"},{"location":"chapters/05-performance-metrics-benchmarking/#step-4-measure-performance","title":"Step 4: Measure Performance","text":"<p>Use Python with the Neo4j driver:</p> <pre><code>from neo4j import GraphDatabase\nimport time\nimport statistics\n\ndriver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n\ndef benchmark_query(query, params, iterations=100):\n    latencies = []\n\n    with driver.session() as session:\n        # Warm up\n        for _ in range(10):\n            session.run(query, params)\n\n        # Measure\n        for _ in range(iterations):\n            start = time.time()\n            session.run(query, params)\n            latencies.append((time.time() - start) * 1000)  # ms\n\n    return {\n        \"p50\": statistics.median(latencies),\n        \"p95\": statistics.quantiles(latencies, n=20)[18],  # 95th percentile\n        \"p99\": statistics.quantiles(latencies, n=100)[98],  # 99th percentile\n        \"mean\": statistics.mean(latencies)\n    }\n\n# Run benchmark\nparams = {\"customerId\": 12345}\nresults = benchmark_query(\"MATCH (c:Customer {id: $customerId})-[:PURCHASED]-&gt;(p) RETURN p\", params)\nprint(f\"P50: {results['p50']:.2f}ms, P95: {results['p95']:.2f}ms, P99: {results['p99']:.2f}ms\")\n</code></pre>"},{"location":"chapters/05-performance-metrics-benchmarking/#step-5-compare-alternatives","title":"Step 5: Compare Alternatives","text":"<p>Run the same queries on PostgreSQL:</p> <pre><code>import psycopg2\nimport time\n\nconn = psycopg2.connect(\"dbname=ecommerce user=postgres\")\ncursor = conn.cursor()\n\ndef benchmark_sql(query, params, iterations=100):\n    latencies = []\n\n    # Warm up\n    for _ in range(10):\n        cursor.execute(query, params)\n        cursor.fetchall()\n\n    # Measure\n    for _ in range(iterations):\n        start = time.time()\n        cursor.execute(query, params)\n        cursor.fetchall()\n        latencies.append((time.time() - start) * 1000)\n\n    return {\n        \"p50\": statistics.median(latencies),\n        \"p95\": statistics.quantiles(latencies, n=20)[18],\n        \"mean\": statistics.mean(latencies)\n    }\n\n# Same query in SQL\nsql = \"\"\"\n    SELECT p.* FROM customers c\n    JOIN purchases pur ON c.id = pur.customer_id\n    JOIN products p ON pur.product_id = p.id\n    WHERE c.id = %s\n\"\"\"\nresults = benchmark_sql(sql, (12345,))\nprint(f\"PostgreSQL P50: {results['p50']:.2f}ms, P95: {results['p95']:.2f}ms\")\n</code></pre>"},{"location":"chapters/05-performance-metrics-benchmarking/#step-6-analyze-and-report","title":"Step 6: Analyze and Report","text":"<p>Create a comparison table:</p> Query PostgreSQL P50 Neo4j P50 Speedup Find customer 2.1 ms 1.8 ms 1.2\u00d7 Customer purchases (1-hop) 8.5 ms 3.2 ms 2.7\u00d7 Similar customers (2-hop) 145 ms 6.8 ms 21\u00d7 Recommendations (3-hop) 2,800 ms 12 ms 233\u00d7 <p>Visualize the results: - Graph showing latency vs hop count - Bar chart comparing databases - Line graph showing scalability (latency vs data size)</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#step-7-share-your-findings","title":"Step 7: Share Your Findings","text":"<p>Write it up: - Blog post - GitHub repo with scripts - Internal tech doc</p> <p>Include: - Methodology (how you tested) - Data model (schema) - Hardware specs - Database versions and configuration - Full results (not just cherry-picked wins) - Limitations (what you didn't test)</p> <p>Be honest: If RDBMS won on some queries, say so. Credibility comes from honesty, not cherry-picking.</p>"},{"location":"chapters/05-performance-metrics-benchmarking/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Skepticism is good engineering: Question claims, test assumptions, measure reality</li> <li>Hop count predicts performance: Graph DBs scale linearly, RDBMS hits exponential cliff at 2-3 hops</li> <li>Degree matters: High-degree nodes (hubs) can slow queries\u2014test both average and extreme cases</li> <li>Indexes are critical: Don't benchmark without appropriate indexes on both systems</li> <li>Use standard benchmarks: LDBC SNB and Graph 500 provide credible, comparable results</li> <li>Join operations are expensive: O(n log n) per hop with growing intermediate sets</li> <li>Traversal is constant-time: O(1) pointer lookups via index-free adjacency</li> <li>Measure distributions, not averages: P50, P95, P99 tell the real story</li> <li>Test at realistic scale: 1M node benchmarks don't predict 100M node performance</li> <li>Document everything: Reproducibility = credibility</li> <li>Build your reputation: Data-driven engineers earn respect</li> <li>Share your results: Transparency builds trust</li> </ol> <p>The bottom line: Don't trust vendor claims. Don't trust this textbook. Run your own benchmarks. Measure your data, your queries, your workload. Make decisions based on evidence, not marketing.</p> <p>When you walk into a meeting with benchmark results\u2014your own data, reproducible methodology, honest reporting\u2014people listen. That's how you build a reputation as an engineer who makes smart, data-driven decisions.</p> <p>And that's worth more than any technology choice.</p> <p>Remember: The best engineers aren't the ones who know the right answer. They're the ones who know how to find the right answer through rigorous testing and measurement. Go benchmark something.</p>"},{"location":"chapters/06-graph-algorithms/","title":"Graph Algorithms","text":""},{"location":"chapters/06-graph-algorithms/#summary","title":"Summary","text":"<p>This chapter covers essential graph algorithms that power modern graph analytics and machine learning applications. You'll learn classic search algorithms like breadth-first and depth-first search, explore pathfinding techniques including A-star and the traveling salesman problem, and master centrality measures that identify important nodes in networks. The chapter progresses to advanced topics including PageRank, community detection, graph neural networks, and graph embeddings that enable machine learning on graph-structured data.</p>"},{"location":"chapters/06-graph-algorithms/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 18 concepts from the learning graph:</p> <ol> <li>Breadth-First Search</li> <li>Depth-First Search</li> <li>A-Star Algorithm</li> <li>Pathfinding</li> <li>Traveling Salesman Problem</li> <li>PageRank</li> <li>Community Detection</li> <li>Centrality Measures</li> <li>Betweenness Centrality</li> <li>Closeness Centrality</li> <li>Graph Embeddings</li> <li>Graph Neural Networks</li> <li>Link Prediction</li> <li>Graph Clustering</li> <li>Connected Components</li> <li>Strongly Connected Components</li> <li>Weakly Connected Components</li> <li>Node Classification</li> </ol>"},{"location":"chapters/06-graph-algorithms/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 3: Labeled Property Graph Information Model</li> <li>Chapter 4: Query Languages for Graph Databases</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/07-social-network-modeling/","title":"Social Network Modeling","text":""},{"location":"chapters/07-social-network-modeling/#summary","title":"Summary","text":"<p>This chapter applies graph database concepts to social network applications, demonstrating how to model friend graphs, influence networks, and organizational structures. You'll learn to represent complex social relationships including followers, activity streams, and user profiles while exploring advanced applications like sentiment analysis integration and fake account detection. The chapter extends to human resources applications including org chart modeling, skill management systems, and task assignment workflows.</p>"},{"location":"chapters/07-social-network-modeling/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 15 concepts from the learning graph:</p> <ol> <li>Social Networks</li> <li>Friend Graphs</li> <li>Influence Graphs</li> <li>Follower Networks</li> <li>Activity Streams</li> <li>User Profiles</li> <li>Relationship Types</li> <li>Sentiment Analysis</li> <li>Natural Language Processing</li> <li>Fake Account Detection</li> <li>Human Resources Modeling</li> <li>Org Chart Models</li> <li>Skill Management</li> <li>Task Assignment</li> <li>Backlog Management</li> </ol>"},{"location":"chapters/07-social-network-modeling/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 3: Labeled Property Graph Information Model</li> <li>Chapter 4: Query Languages for Graph Databases</li> </ul>"},{"location":"chapters/07-social-network-modeling/#introduction-social-networks-are-everywhere-yes-even-where-you-dont-expect-them","title":"Introduction: Social Networks Are Everywhere (Yes, Even Where You Don't Expect Them)","text":"<p>When you hear \"social network,\" you probably think of Instagram, TikTok, or X (formerly Twitter). But here's the plot twist: social network patterns show up almost everywhere in our digital lives, often hiding in plain sight. That Amazon product with 4.7 stars? It's a social network of reviewers with varying credibility based on their review history. Your GitHub profile showing contributions and merged pull requests? That's reputation tracking in a developer social network. Stack Overflow karma points? Reddit upvotes? Yelp reviewer badges? All social networks.</p> <p>Even your school or workplace runs on social network principles. When teachers assign group projects, they're creating task networks. When your manager checks who has skills in Python before assigning a coding project, they're querying a skill network. Any system where people have reputations, write comments, give ratings, or connect with each other is fundamentally a social network\u2014and graph databases excel at modeling these patterns.</p> <p>In this chapter, you'll learn how to model social networks using graph databases, from simple friend connections to complex systems involving influence, sentiment analysis, and fake account detection. We'll explore how the same patterns that power Facebook also power human resources systems, project management tools, and content moderation platforms. By the end, you'll see social network patterns everywhere you look (you're welcome for that superpower).</p>"},{"location":"chapters/07-social-network-modeling/#what-makes-something-a-social-network","title":"What Makes Something a Social Network?","text":"<p>At its core, a social network is just people (or accounts, or profiles) connected by relationships. But modern social networks are way more interesting than that simple definition suggests. They track who influences whom, what content people create, how others react to that content, and even whether accounts are real humans or bots pretending to be humans.</p> <p>The magic of graph databases is that they can model all these dimensions naturally. While a traditional database would store user data in one table, friendships in another table, posts in a third table, and comments in a fourth table (requiring complex joins to answer simple questions like \"show me my friends' recent posts\"), a graph database stores everything as an interconnected network that mirrors how we actually think about social relationships.</p> <p>Let's break down the key components that make up modern social networks:</p> <ul> <li>User Profiles: The people (or accounts) in the network, with their personal information, preferences, and history</li> <li>Connections: Relationships between users, which can be symmetric (mutual friends) or directed (followers)</li> <li>Content: Posts, comments, reviews, photos, videos, or any other user-generated material</li> <li>Interactions: Likes, shares, comments, reactions, and other ways people engage with content</li> <li>Reputation Signals: Karma scores, review ratings, follower counts, verification badges, and trust indicators</li> <li>Metadata: When things happened, where they happened, what device was used, and other contextual information</li> </ul>"},{"location":"chapters/07-social-network-modeling/#user-profiles-more-than-just-names-and-photos","title":"User Profiles: More Than Just Names and Photos","text":"<p>Every social network starts with user profiles\u2014the nodes that represent people in the graph. But unlike a simple contact list, social network profiles are rich with information that helps the network function better. Your profile isn't just your name and photo; it's a collection of attributes that helps the system understand who you are, what you care about, and how trustworthy you are.</p> <p>In a graph database, each user profile is a node with properties. Some properties are basic (name, email, birth date), while others emerge from user behavior (reputation score, activity level, account age). The beauty of graph databases' flexible schema is that different users can have different properties\u2014power users might have badges, verified accounts might have verification timestamps, and content creators might have additional metadata about their posting history.</p> <p>Here's what a typical user profile node might contain:</p> Property Category Examples Purpose Basic Identity username, email, display_name, profile_photo Identify and display the user Demographics age, location, language, timezone Personalize content and connections Account Metadata created_date, last_login, account_status Track account lifecycle Reputation Signals reputation_score, verified_badge, strike_count Indicate trustworthiness Privacy Settings profile_visibility, messaging_allowed Control user experience Behavioral Metrics posts_count, avg_response_time, activity_level Understand user engagement User Profile Graph Model Visualization     Type: graph-model      Purpose: Show how user profile nodes connect to other entities in a social network graph      Node types:     1. User (blue circles)        - Properties: username, email, reputation_score, created_date, verified        - Example nodes: \"Alice\" (verified, reputation: 892), \"Bob\" (reputation: 234), \"Charlie\" (verified, reputation: 1,450)      2. Post (light green squares)        - Properties: content, timestamp, post_type, visibility        - Example nodes: \"Alice's vacation photo\", \"Bob's tech question\", \"Charlie's tutorial video\"      3. Topic (purple triangles)        - Properties: topic_name, category        - Example nodes: \"Photography\", \"Web Development\", \"Travel\"      4. Badge (gold stars)        - Properties: badge_name, criteria, rarity        - Example nodes: \"Top Contributor\", \"Verified Expert\", \"Early Adopter\"      Edge types:     1. FOLLOWS (solid blue arrows)        - Properties: since_date, notification_settings        - Directed: User \u2192 User        - Example: Bob FOLLOWS Alice, Charlie FOLLOWS Alice      2. CREATED (solid green arrows)        - Properties: created_timestamp        - Directed: User \u2192 Post        - Example: Alice CREATED \"vacation photo\"      3. INTERESTED_IN (dashed purple arrows)        - Properties: interest_level (0-10), added_date        - Directed: User \u2192 Topic        - Example: Alice INTERESTED_IN \"Photography\" (level: 9)      4. EARNED (dotted gold arrows)        - Properties: earned_date, achievement_context        - Directed: User \u2192 Badge        - Example: Charlie EARNED \"Top Contributor\" (2023-05-15)      Sample data:     - Alice (verified, reputation: 892)       \u251c\u2500 CREATED \u2192 \"Sunset photography tips\" (Post)       \u251c\u2500 INTERESTED_IN \u2192 \"Photography\" (level: 9)       \u251c\u2500 INTERESTED_IN \u2192 \"Travel\" (level: 7)       \u251c\u2500 EARNED \u2192 \"Top Contributor\" badge       \u2514\u2500 Followed by Bob and Charlie      - Bob (reputation: 234)       \u251c\u2500 FOLLOWS \u2192 Alice       \u251c\u2500 FOLLOWS \u2192 Charlie       \u251c\u2500 CREATED \u2192 \"React hooks question\" (Post)       \u2514\u2500 INTERESTED_IN \u2192 \"Web Development\" (level: 6)      - Charlie (verified, reputation: 1,450)       \u251c\u2500 FOLLOWS \u2192 Alice       \u251c\u2500 CREATED \u2192 \"Advanced CSS tutorial\" (Post)       \u251c\u2500 INTERESTED_IN \u2192 \"Web Development\" (level: 10)       \u2514\u2500 EARNED \u2192 \"Verified Expert\" badge      Layout: Force-directed with users forming a central cluster, content and topics radiating outward      Interactive features:     - Hover over user node: Show full profile summary (username, reputation, badges, join date)     - Click user node: Highlight all posts created by that user and all users they follow     - Double-click user: Show expanded network (followers of followers)     - Hover over edge: Show relationship details (follow date, interaction frequency)     - Click badge: Show all users who have earned that badge     - Filter controls: Show/hide different relationship types      Visual styling:     - User node size based on reputation score (larger = higher reputation)     - Verified users have gold border     - Edge thickness based on interaction frequency     - Color saturation indicates activity level (brighter = more active)      Legend:     - Node shapes: Circle (User), Square (Post), Triangle (Topic), Star (Badge)     - Edge styles: Solid (action), Dashed (preference), Dotted (achievement)     - Border colors: Gold (verified), Gray (regular)      Implementation: vis-network JavaScript library     Canvas size: 900x700px with zoom and pan controls  <p>One key insight: reputation isn't stored in a single property\u2014it emerges from the graph structure. A user's reputation might be calculated from how many followers they have, how many of their posts get liked, how many verified users follow them, and how long their account has existed. This is where graphs shine: they make these relationship-based calculations fast and natural.</p>"},{"location":"chapters/07-social-network-modeling/#friend-graphs-the-foundation-of-social-connections","title":"Friend Graphs: The Foundation of Social Connections","text":"<p>The simplest social network pattern is the friend graph: nodes representing people, connected by \"friend\" relationships. This is how Facebook started\u2014just college students and their friendships. But even this seemingly simple pattern has interesting complexity when you look closely.</p> <p>In most friendship systems, the relationship is mutual (symmetric): if Alice is friends with Bob, then Bob is friends with Alice. This differs from \"following\" relationships, which we'll explore next. The FRIENDS_WITH relationship is undirected, meaning it works both ways automatically.</p> <p>Creating a friend connection in a graph database is remarkably simple compared to the relational database equivalent. Instead of inserting a row into a \"friendships\" junction table with two foreign keys, you simply create an edge between two user nodes. Queries like \"show me Alice's friends\" or \"how many mutual friends do Alice and Bob have?\" become one-line graph traversals instead of multi-table joins.</p> <p>Here are some common friend graph queries and why they matter:</p> <ul> <li>Direct friends: <code>MATCH (user)-[:FRIENDS_WITH]-(friend)</code> - The foundation of any social experience</li> <li>Friends of friends: <code>MATCH (user)-[:FRIENDS_WITH*2]-(fof)</code> - Potential new connections and recommendations</li> <li>Mutual friends: <code>MATCH (user1)-[:FRIENDS_WITH]-(mutual)-[:FRIENDS_WITH]-(user2)</code> - Social proof for new connections</li> <li>Friend groups: <code>MATCH (user)-[:FRIENDS_WITH]-(friend)-[:FRIENDS_WITH]-(friendOfFriend)</code> - Discovering communities</li> </ul> Friend Recommendation MicroSim     Type: microsim      Learning objective: Demonstrate how friend-of-friend recommendations work in social networks and how different algorithms can prioritize recommendations      Canvas layout (1000x700px):     - Left side (700x700): Network visualization showing user nodes and connections     - Right side (300x700): Control panel and recommendation results      Visual elements in network view:     - Current user (you) shown as large blue circle in center     - Direct friends shown as medium green circles     - Friends-of-friends shown as smaller orange circles     - Strangers (not connected) shown as tiny gray circles     - Edges between nodes shown as thin gray lines     - When recommendation is selected, highlight path from you \u2192 friend \u2192 recommended person in yellow      Interactive controls:     - Button: \"Generate Network\" (creates random social graph with 40-60 nodes)     - Slider: \"Your friend count\" (5-20 friends, default: 10)     - Dropdown: \"Recommendation algorithm\"       * Mutual Friends Count (default)       * Common Interests       * Network Centrality       * Activity Level     - Button: \"Find Recommendations\"     - Display panel: Top 5 friend recommendations with reasons      Network generation parameters:     - Total nodes: 50 (1 you + 49 others)     - Your direct friends: controlled by slider     - Average friends per person: 8-12 (random)     - Each person has 2-5 random interests from pool (Sports, Music, Tech, Art, Travel, Food, Gaming, Reading)     - Activity level: random 1-10 per person      Recommendation algorithms:     1. Mutual Friends Count: Ranks friends-of-friends by number of mutual connections        - Formula: COUNT(mutual_friends)        - Reasoning: \"You have 4 mutual friends with this person\"      2. Common Interests: Ranks by shared interest tags        - Formula: COUNT(shared_interests) / TOTAL(your_interests)        - Reasoning: \"You both like Tech, Gaming, and Music\"      3. Network Centrality: Recommends well-connected people        - Formula: COUNT(their_total_friends) - penalize if already connected        - Reasoning: \"This person knows a lot of people (18 friends)\"      4. Activity Level: Recommends active users        - Formula: activity_score * mutual_connection_bonus        - Reasoning: \"Very active user (activity: 9/10) with 2 mutual friends\"      Behavior:     - On \"Generate Network\": Create random graph, position nodes using force-directed layout     - On \"Find Recommendations\": Run selected algorithm, display top 5 results     - Hover over recommended person: Highlight all paths from you to them through friends     - Click recommended person: Show detailed profile (friend count, interests, activity level, mutual friends list)     - Animation: When showing recommendation, animate yellow highlight along the connection path      Visual feedback:     - Recommendation results show as cards with: name, reason, mutual friends count, shared interests, connection strength bar     - Color code connection strength: Green (strong), Yellow (medium), Orange (weak)      Educational value:     - Students see different algorithms produce different recommendations     - Visualizes the \"friends of friends\" concept     - Shows why mutual friends is a strong social signal     - Demonstrates trade-offs in recommendation strategies      Implementation: p5.js with force-directed graph layout (or use simple geometric positioning)     Default state: Start with network already generated so students can immediately explore  <p>The friend graph becomes more interesting when we calculate metrics like clustering coefficient (how many of your friends are friends with each other) or identify tightly-knit groups (cliques). These patterns help social networks suggest new friends, detect spam accounts (real people have clustered friend networks, bots have random connections), and organize content (show posts from friend groups differently).</p>"},{"location":"chapters/07-social-network-modeling/#follower-networks-when-relationships-arent-symmetric","title":"Follower Networks: When Relationships Aren't Symmetric","text":"<p>While friend relationships are mutual, many social networks use asymmetric follower relationships instead. On X (Twitter), Instagram, or TikTok, you can follow someone without them following you back. This creates a directed graph where edges have direction and meaning.</p> <p>The FOLLOWS relationship is fundamentally different from FRIENDS_WITH. When Alice follows Bob, she sees Bob's posts, but Bob doesn't necessarily see Alice's posts unless he follows her back. This asymmetry enables influencer dynamics, fan relationships, and information propagation patterns that couldn't exist in symmetric friend networks.</p> <p>Directed graphs enable questions that have no meaning in undirected graphs:</p> <ul> <li>Followers: Who follows me? <code>MATCH (follower)-[:FOLLOWS]-&gt;(me)</code></li> <li>Following: Who do I follow? <code>MATCH (me)-[:FOLLOWS]-&gt;(following)</code></li> <li>Mutual follows: Who do I follow who also follows me back? (These are your \"mutuals\")</li> <li>Follow ratio: Following count / Follower count (a metric some use to judge account quality)</li> <li>Reach: How many people could potentially see my post? (followers + their followers + ...)</li> </ul> Follower Network Visualization Diagram     Type: diagram      Purpose: Illustrate the difference between symmetric friendships and asymmetric follower relationships      Layout: Two side-by-side network diagrams for comparison      LEFT SIDE: \"Symmetric Friend Network (Facebook-style)\"     - 5 user nodes arranged in a circle: Alice, Bob, Carol, Dave, Eve     - Undirected edges (lines without arrows) between nodes:       * Alice \u2190\u2192 Bob       * Alice \u2190\u2192 Carol       * Bob \u2190\u2192 Dave       * Carol \u2190\u2192 Dave       * Dave \u2190\u2192 Eve     - All edges are same thickness, colored blue     - Label: \"All friendships are mutual\"     - Annotation: \"If Alice is friends with Bob, Bob is automatically friends with Alice\"      RIGHT SIDE: \"Asymmetric Follower Network (Twitter/Instagram-style)\"     - Same 5 user nodes: Alice, Bob, Carol, Dave, Eve     - Directed edges (arrows) showing follow relationships:       * Alice \u2192 Bob (Alice follows Bob)       * Bob \u2192 Alice (Bob follows Alice) [these two create a mutual]       * Alice \u2192 Carol (Alice follows Carol)       * Carol \u2192 Bob (Carol follows Bob, but Bob doesn't follow back)       * Dave \u2192 Bob (Dave follows Bob)       * Dave \u2192 Eve (Dave follows Eve)       * Eve \u2192 Bob (Eve follows Bob)       * Bob \u2192 Carol (Bob follows Carol)     - Color coding for edges:       * Green thick arrows: Mutual follows (bidirectional)       * Gray thin arrows: One-way follows     - Label: \"Follows can be one-directional\"     - Annotation: \"Bob has 4 followers (Alice, Carol, Dave, Eve) but only follows 2 people back (Alice, Carol)\"      Key insights callout (bottom):     - \"In follower networks, you can have INFLUENCERS (many followers, few following)\"     - \"Mutual follows indicate stronger relationships\"     - \"One-way follows create information hierarchies\"      Visual style: Clean network diagram with circular nodes labeled with names     Node size: All equal size     Color scheme: Blue for left diagram (friendship), Green/Gray for right diagram (followers)     Include legend explaining arrow colors on right side      Implementation: SVG diagram or simple illustration tool  <p>The follower model creates interesting dynamics. Users with many followers but few following are influencers. Users with roughly equal followers and following are regular community participants. Accounts that follow thousands but have few followers are often spam or growth-hacking bots. These patterns would be invisible in a symmetric friend model.</p> <p>Follower networks also enable information cascades: when an influencer posts content, it can reach thousands directly, then spread further as followers share with their followers. Graph databases can model these propagation patterns and even predict which content is likely to go viral based on who shares it first.</p>"},{"location":"chapters/07-social-network-modeling/#activity-streams-adding-time-to-the-social-graph","title":"Activity Streams: Adding Time to the Social Graph","text":"<p>Social networks aren't static\u2014they're constantly flowing with new content. Activity streams capture this temporal dimension by tracking who posts what, when. Every tweet, Instagram photo, Facebook status, Reddit comment, or product review is a node in the graph, connected to its creator and timestamp.</p> <p>In graph databases, activity stream nodes typically have a CREATED_BY edge pointing to the user who created them, and often a POSTED_IN or TAGGED_WITH edge connecting them to topics, groups, or categories. The timestamp property on the post node enables time-based queries: \"show me posts from the last hour\" or \"what were the trending topics last Tuesday?\"</p> <p>The activity stream pattern appears everywhere:</p> <ul> <li>Social media posts: Twitter tweets, Facebook updates, Instagram photos</li> <li>Reviews and ratings: Amazon product reviews, Yelp restaurant reviews, App Store ratings</li> <li>Comments and discussions: Reddit comments, YouTube video comments, blog post comments</li> <li>Work activity: GitHub commits, Jira ticket updates, Slack messages</li> <li>Event logs: User logins, page views, button clicks</li> </ul> <p>What makes activity streams interesting in graph databases is how they connect to other entities. A post isn't just created by a user\u2014it might mention other users, belong to a topic, include location data, link to external content, and receive reactions from other users. Each of these is a different relationship type in the graph.</p> <p>Here's a query pattern you'll use constantly: \"Show me recent posts from people I follow, about topics I care about, sorted by relevance.\" In a relational database, this requires joining user tables, follower tables, post tables, topic tables, and user interest tables\u2014five or more joins. In a graph database, it's a simple pattern match that follows edges from you to people you follow to their recent posts that connect to your interest topics.</p> Activity Stream Timeline Visualization     Type: timeline      Purpose: Show how activity streams work in a social network, with posts distributed over time and connected to creators, topics, and interactions      Time period: Last 7 days (display by day)     Orientation: Vertical timeline (top = most recent)      Timeline items (sample data):      Day 1 (Today):     - 10:30 AM: Alice posted photo \"Morning coffee \u2615\"       * Tagged: Coffee, Photography       * Reactions: 12 likes, 3 comments       * Reach: 234 followers      - 2:15 PM: Bob posted question \"Best JavaScript frameworks in 2024?\"       * Tagged: Programming, Web Development       * Reactions: 45 likes, 28 comments, 8 shares       * Reach: 156 followers      Day 2 (Yesterday):     - 9:00 AM: Charlie posted tutorial \"How to center a div (seriously)\"       * Tagged: CSS, Web Development, Humor       * Reactions: 234 likes, 67 comments, 89 shares       * Reach: 1,450 followers       * Went viral: +2,340 indirect impressions      - 4:30 PM: Alice commented on Charlie's tutorial       * Comment: \"The classic problem! Great explanation\"       * Reactions: 5 likes      Day 3:     - 1:00 PM: Dave posted code snippet \"Python one-liner for sorting\"       * Tagged: Python, Programming Tips       * Reactions: 34 likes, 12 comments       * Reach: 567 followers      Day 4:     - 11:15 AM: Eve posted review \"Amazing pizza at Luigi's! \ud83c\udf55 5\u2b50\"       * Tagged: Food, Restaurant Reviews, New York       * Reactions: 23 likes, 7 comments       * Reach: 198 followers      Day 5:     - 3:45 PM: Bob shared Charlie's tutorial       * Added comment: \"This saved me hours of frustration!\"       * Reactions: 15 likes       * Extended reach: +156 people      Day 6:     - 8:00 AM: Charlie posted poll \"Which do you prefer: Tabs or Spaces?\"       * Tagged: Programming, Debates       * Reactions: 456 votes, 89 comments, 34 shares       * Reach: 1,450 followers      Day 7:     - 5:30 PM: Alice posted travel photo \"Sunset in Santorini\"       * Tagged: Travel, Photography, Greece       * Reactions: 89 likes, 12 comments       * Reach: 234 followers      Visual style:     - Vertical timeline with date markers on left     - Post cards showing: creator profile pic, content preview, tags, reaction counts     - Color coding by content type:       * Blue: Regular posts       * Green: Questions       * Purple: Tutorials/educational       * Orange: Reviews       * Pink: Shared content     - Connection lines showing: Alice \u2192 Charlie (comment), Bob \u2192 Charlie (share)      Interactive features:     - Hover over post: Show full content and engagement metrics     - Click post: Expand to show all comments and reactions with timestamps     - Click user: Filter timeline to show only that user's activity     - Click tag: Filter timeline to show posts with that tag     - Slider at top: Adjust time window (last 24 hours, 7 days, 30 days)     - Toggle: Show only posts from followed users vs. all posts      Educational value:     - Visualizes how content flows through time     - Shows viral spread (Charlie's tutorial)     - Demonstrates engagement patterns (comments, shares, likes)     - Illustrates how shares extend reach     - Shows clustering of topics (programming posts cluster together)      Implementation: vis-timeline library with custom styling     Canvas size: 1000x800px with scrollable content     Default view: Shows last 7 days, scrollable to see older content  <p>Activity streams also enable algorithmic ranking. Instead of showing posts in simple reverse-chronological order, modern social networks rank posts by predicted relevance using graph signals: Is the creator someone you interact with frequently? Do you engage with this topic often? Have your friends engaged with this post? These graph-based features feed machine learning models that personalize everyone's feed.</p>"},{"location":"chapters/07-social-network-modeling/#relationship-types-not-all-connections-are-equal","title":"Relationship Types: Not All Connections Are Equal","text":"<p>Real social networks are messy and multi-dimensional. The relationship between you and your mom is different from your relationship with your boss, which is different from your relationship with your favorite YouTuber, which is different from your relationship with your ex. Graph databases shine here because they support multiple relationship types between the same two nodes.</p> <p>Instead of a single generic \"connected to\" relationship, graph databases let you model the actual nature of connections:</p> <ul> <li>Family relationships: PARENT_OF, CHILD_OF, SIBLING_OF, MARRIED_TO</li> <li>Professional relationships: WORKS_WITH, MANAGES, REPORTS_TO, COLLABORATES_WITH</li> <li>Social relationships: FRIENDS_WITH, FOLLOWS, BLOCKS, MUTED</li> <li>Content relationships: CREATED, COMMENTED_ON, LIKED, SHARED, BOOKMARKED</li> <li>Affiliation relationships: MEMBER_OF, MODERATOR_OF, OWNS, SUBSCRIBES_TO</li> </ul> <p>The power of typed relationships is that you can query specific relationship patterns. \"Show me people I work with who also share my interest in photography\" is a query that combines professional and interest relationships. \"Find experts on machine learning who are within 3 degrees of separation from me\" traverses your network while filtering by expertise tags.</p> <p>Relationship types can also have properties that add even more nuance:</p> Relationship Type Common Properties Why They Matter FOLLOWS followed_since, notification_setting, follow_reason Track when connection started, how engaged user is WORKS_WITH start_date, end_date, role, department Distinguish current and past colleagues COMMENTED_ON comment_text, timestamp, sentiment_score Understand nature of engagement MEMBER_OF joined_date, membership_level, participation_score Identify active vs. inactive members TRUSTS trust_level (1-10), endorsement_count Model reputation networks Multi-Relationship Network Graph Model     Type: graph-model      Purpose: Demonstrate how the same people can have multiple different relationship types connecting them, creating a rich multi-dimensional social network      Node types:     1. Person (large circles with profile pictures/icons)        - Properties: name, job_title, interests[], location        - Example nodes:          * \"Sarah\" (Software Engineer, interests: [coding, hiking, photography])          * \"Mike\" (Product Manager, interests: [product design, running, cooking])          * \"Lisa\" (UX Designer, interests: [design, photography, travel])          * \"James\" (Data Scientist, interests: [ML, coding, gaming])          * \"Emma\" (Marketing Lead, interests: [marketing, travel, food])      2. Company (square nodes)        - Properties: company_name, industry        - Example: \"TechCorp Inc.\" (Software)      3. Interest Group (triangle nodes)        - Properties: group_name, member_count        - Examples: \"Photography Club\" (128 members), \"Hiking Enthusiasts\" (234 members)      Edge types (color-coded):     1. WORKS_WITH (solid blue arrows)        - Properties: since_date, department, project        - Examples:          * Sarah WORKS_WITH Mike (since: 2022, dept: Product, project: \"App Redesign\")          * Sarah WORKS_WITH James (since: 2023, dept: Engineering, project: \"ML Features\")          * Mike WORKS_WITH Lisa (since: 2021, dept: Product)          * Mike WORKS_WITH Emma (since: 2022, cross-dept collaboration)      2. FRIENDS_WITH (solid green lines, undirected)        - Properties: friends_since, friendship_strength (1-10)        - Examples:          * Sarah FRIENDS_WITH Lisa (since: 2020, strength: 9)          * Mike FRIENDS_WITH Emma (since: 2019, strength: 8)          * James FRIENDS_WITH Sarah (since: 2023, strength: 6)      3. FOLLOWS (dashed purple arrows)        - Properties: followed_since, notification_on (true/false)        - Examples:          * Lisa FOLLOWS Sarah (since: 2021, notifications: true)          * Emma FOLLOWS Sarah (since: 2022, notifications: false)          * James FOLLOWS Mike (since: 2023, notifications: true)      4. SHARES_INTEREST (dotted orange lines)        - Properties: common_interests[]        - Examples:          * Sarah SHARES_INTEREST Lisa [photography, travel]          * Sarah SHARES_INTEREST James [coding]          * Mike SHARES_INTEREST Emma [travel, food]      5. MENTORS (thick gold arrows)        - Properties: mentorship_since, focus_area        - Examples:          * Mike MENTORS Sarah (since: 2022, focus: \"product thinking\")          * Sarah MENTORS James (since: 2023, focus: \"software engineering\")      6. EMPLOYED_BY (gray arrows)        - Properties: role, start_date        - All five people EMPLOYED_BY \"TechCorp Inc.\"      7. MEMBER_OF (light blue arrows)        - Properties: joined_date, participation_level        - Examples:          * Sarah MEMBER_OF \"Photography Club\" (joined: 2021, level: active)          * Lisa MEMBER_OF \"Photography Club\" (joined: 2020, level: organizer)          * Sarah MEMBER_OF \"Hiking Enthusiasts\" (joined: 2022, level: occasional)      Sample complex query visualization:     - When user clicks \"Find mentors who share my interests\":       * Highlight Sarah (user)       * Trace SHARES_INTEREST edges to find Lisa and James       * Trace MENTORS edges to find potential mentors       * Result: Mike mentors Sarah and shares interests indirectly through friendship       * Show path: Sarah \u2192 FRIENDS_WITH \u2192 Lisa \u2192 SHARES_INTEREST (photography) \u2192 Sarah                    Sarah \u2192 MENTORED_BY \u2192 Mike      Layout: Force-directed with people nodes in center, company and groups on periphery      Interactive features:     - Hover over person: Show all their properties and relationships summary     - Click person: Filter view to show only relationships involving that person     - Hover over edge: Show relationship type and properties in tooltip     - Click edge: Highlight all edges of that type in the graph     - Filter panel:       * Checkboxes to show/hide relationship types       * \"Show only professional\" (WORKS_WITH, MENTORS, EMPLOYED_BY)       * \"Show only social\" (FRIENDS_WITH, FOLLOWS, SHARES_INTEREST)       * \"Show only affiliations\" (MEMBER_OF, EMPLOYED_BY)     - Double-click person: Show expanded network (include their connections not visible)     - Query builder: Allow user to construct graph patterns       * Example: \"Find people who WORKS_WITH me AND SHARES_INTEREST with me\"      Visual styling:     - Person node size based on total number of connections     - Edge thickness based on relationship strength/importance     - Edge color: Blue (professional), Green (friendship), Purple (following), Orange (shared interest), Gold (mentorship), Gray (employment), Light blue (membership)     - Animated highlight when hovering or selecting     - Semi-transparent edges when filtered out      Legend (always visible):     - Relationship types with color coding     - Node shapes (Circle: Person, Square: Company, Triangle: Group)     - Edge styles (Solid: strong connection, Dashed: follow, Dotted: shared attribute)      Educational value:     - Shows same people connected in multiple ways     - Demonstrates how different relationships serve different purposes     - Illustrates multi-dimensional social graphs     - Shows why graph databases excel at relationship-rich data      Implementation: vis-network JavaScript library     Canvas size: 1000x800px with zoom, pan, and filter controls     Default state: All relationships visible, force-directed layout stabilized  <p>The multi-relationship approach also solves privacy problems elegantly. You might want to share vacation photos with friends but not coworkers. In a graph database, you can traverse FRIENDS_WITH relationships but not WORKS_WITH relationships when deciding who sees personal posts. LinkedIn leverages this: your work history is visible to professional connections (WORKS_WITH) but maybe not to everyone who follows you.</p>"},{"location":"chapters/07-social-network-modeling/#influence-graphs-measuring-who-matters","title":"Influence Graphs: Measuring Who Matters","text":"<p>Not all users in a social network have equal impact. Some people have thousands of followers, high engagement rates, and content that gets shared widely. Others are casual users who rarely post. Influence graphs capture these dynamics by adding weight to connections and calculating centrality metrics.</p> <p>Influence can be measured in various ways:</p> <ul> <li>Follower count: Raw number of people who see your content (reach)</li> <li>Engagement rate: Percentage of followers who like, comment, or share (quality vs. quantity)</li> <li>Share cascades: How often your content gets re-shared by others (virality)</li> <li>Network position: How central you are in the social graph (betweenness centrality)</li> <li>PageRank: The algorithm Google uses to rank web pages, adapted for social networks</li> </ul> <p>PageRank is particularly interesting for social networks because it doesn't just count followers\u2014it weights followers by their own importance. Having 100 followers who are themselves influencers is more valuable than having 1,000 followers who are inactive accounts. PageRank iteratively calculates importance by following the graph structure.</p> <p>Graph databases can calculate influence metrics efficiently because they're optimized for traversing relationships. Finding someone's followers is a one-hop traversal. Finding followers-of-followers is a two-hop traversal. Calculating PageRank requires iterating over the entire graph multiple times, which graph databases handle well.</p> <p>Influence graphs appear in unexpected places:</p> <ul> <li>Academic citations: Papers with many citations from highly-cited papers are more important</li> <li>Code repositories: GitHub repos with stars from active developers rank higher</li> <li>Review platforms: Reviews from verified, high-reputation reviewers carry more weight</li> <li>Answer forums: Stack Overflow karma reflects your influence in the developer community</li> </ul> Influence Propagation MicroSim     Type: microsim      Learning objective: Visualize how influence spreads through a social network when an influencer posts content, and compare different propagation patterns      Canvas layout (1200x800px):     - Left side (850x800): Network visualization showing users and influence propagation     - Right side (350x800): Control panel, metrics dashboard, and propagation statistics      Visual elements in network view:     - User nodes sized by follower count (larger = more followers)     - Node colors indicate influence level:       * Red (large): Influencers (1000+ followers)       * Orange (medium): Mid-tier (100-999 followers)       * Yellow (small): Regular users (10-99 followers)       * Gray (tiny): Casual users (&lt;10 followers)     - Edges: FOLLOWS relationships shown as light gray arrows     - When propagation runs: Nodes light up as content reaches them     - Pulse animation shows content spreading through network      Interactive controls:     - Dropdown: \"Select initial poster\"       * Random casual user       * Random regular user       * Random mid-tier user       * Random influencer       * [Specific user selection]     - Slider: \"Content quality\" (1-10, affects share probability)       * Low quality (1-3): 5% share rate       * Medium quality (4-7): 15% share rate       * High quality (8-10): 30% share rate     - Slider: \"Simulation speed\" (50-2000ms per step)     - Button: \"Start Propagation\"     - Button: \"Reset Network\"     - Button: \"Generate New Network\"     - Checkbox: \"Show engagement details\"     - Checkbox: \"Highlight influencer paths\"      Network generation:     - Total users: 100     - Influencers: 3 (1000-5000 followers each)     - Mid-tier: 15 (100-500 followers each)     - Regular users: 32 (10-99 followers each)     - Casual users: 50 (&lt;10 followers each)     - Follow pattern: Preferential attachment (popular users get more followers)     - Influencers have higher engagement multiplier (2x share rate)      Propagation algorithm:     1. Initial poster creates content at time T=0     2. All followers see content (immediate reach)     3. Each follower decides whether to share based on:        - Content quality (affects base share probability)        - Their engagement level (random personality factor)        - Whether they're an influencer (2x share rate)     4. If they share, their followers see it (T=1, T=2, etc.)     5. Track: unique reach, total impressions, shares, propagation depth      Metrics dashboard (right panel):     - Initial reach: [number] (direct followers)     - Current total reach: [number] (unique users who saw it)     - Total impressions: [number] (including duplicates)     - Times shared: [number]     - Propagation depth: [number] hops     - Virality score: [calculated metric]     - Engagement rate: [percentage]     - Time elapsed: [seconds] in simulation      Comparison table (shows after propagation completes):     | Initial Poster Type | Avg Reach | Avg Shares | Avg Depth |     |---------------------|-----------|------------|-----------|     | Casual User         | 45        | 3          | 2.1       |     | Regular User        | 178       | 12         | 3.4       |     | Mid-tier User       | 634       | 45         | 4.8       |     | Influencer          | 2,890     | 234        | 6.2       |      Behavior:     - On \"Start Propagation\":       * Selected user node pulses (creates content)       * Follower nodes light up green (saw content)       * Some followers pulse and share (become orange briefly)       * Their followers light up green       * Animation continues until no more shares occur     - Hover over any node during propagation: Show when they saw content, whether they shared, why they shared/didn't share     - Click node: Show their influence metrics (followers, engagement rate, shares given)     - \"Highlight influencer paths\": Show in bright yellow any propagation path that went through an influencer      Visual feedback:     - Green glow: User saw content     - Orange pulse: User shared content     - Yellow highlight: Influencer-mediated propagation     - Edge thickness increases when content flows through that connection     - Counter in corner shows current reach and shares in real-time      Educational value:     - Demonstrates network effects and viral spread     - Shows why influencers are valuable (reach multiplication)     - Illustrates that content quality matters (affects share rate)     - Reveals how some content fizzles out vs. goes viral     - Shows random variation (same conditions = different outcomes sometimes)     - Demonstrates exponential growth in well-connected networks      Implementation: p5.js with custom propagation simulation     Physics: Use force-directed layout for positioning (or static hierarchical layout)     Default state: Network pre-generated, ready for user to select poster and start     Animation: Smooth transitions, clear visual feedback for each propagation step  <p>Influence graphs raise interesting ethical questions. Should platforms amplify already-influential users (making the rich richer), or should they boost emerging voices? Should influence be calculated transparently, or is it a proprietary algorithm? These aren't just technical questions\u2014they shape how information flows through society.</p>"},{"location":"chapters/07-social-network-modeling/#sentiment-analysis-understanding-the-emotional-network","title":"Sentiment Analysis: Understanding the Emotional Network","text":"<p>Social networks aren't just about who connects to whom\u2014they're about what people say and how others react. Sentiment analysis adds an emotional dimension to the social graph by analyzing whether posts, comments, and reviews express positive, negative, or neutral feelings.</p> <p>In a graph database, sentiment can be stored as a property on content nodes or relationship edges. A review node might have a <code>sentiment_score</code> property ranging from -1 (very negative) to +1 (very positive). A COMMENTED_ON relationship might have a <code>comment_sentiment</code> property indicating whether the comment was supportive, critical, or neutral.</p> <p>Why does sentiment matter in social graphs?</p> <ul> <li>Product reviews: Aggregate sentiment helps buyers decide (\"4.5 stars but recent reviews are negative? Something changed.\")</li> <li>Brand monitoring: Companies track sentiment about their products across social media</li> <li>Content moderation: Negative sentiment clusters might indicate harassment or toxic behavior</li> <li>Political analysis: Sentiment about candidates or policies reveals public opinion</li> <li>Customer support: Negative sentiment triggers escalation to human agents</li> </ul> <p>Modern sentiment analysis uses Natural Language Processing (NLP) models that go beyond simple keyword matching. Instead of just looking for \"good\" or \"bad\" words, these models understand context, sarcasm, and subtle emotional cues. \"This product is surprisingly decent\" is mildly positive despite the word \"surprisingly\" often appearing in negative contexts.</p> <p>Here's how sentiment analysis integrates with graph databases:</p> <ol> <li>Content creation: User posts comment or review (node created)</li> <li>Sentiment extraction: NLP model analyzes text, assigns sentiment score</li> <li>Property storage: Score stored as property on content node</li> <li>Aggregation queries: Calculate average sentiment for products, topics, users</li> <li>Trend detection: Track sentiment changes over time (\"product rating dropping\")</li> <li>Network effects: Positive content from influencers spreads more than negative content</li> </ol> Sentiment Analysis Flow Workflow Diagram     Type: workflow      Purpose: Show how sentiment analysis integrates into a social network content pipeline, from user posting to moderation actions      Visual style: Flowchart with process rectangles, decision diamonds, and data stores      Steps:      1. Start: \"User Submits Post/Comment/Review\"        Hover text: \"User types content and clicks submit button\"        Shape: Rounded rectangle (start/end)        Color: Green      2. Process: \"Store Content Node in Graph\"        Hover text: \"Create node with properties: content_text, timestamp, author_id, content_type\"        Shape: Rectangle        Color: Light blue        Details: Node created with initial properties, relationship created: User -[:CREATED]-&gt; Content      3. Process: \"Send Text to NLP Sentiment Model\"        Hover text: \"Call external API or local model (e.g., VADER, TextBlob, or transformer-based model)\"        Shape: Rectangle        Color: Light blue        Details: Text preprocessing: remove URLs, mentions, hashtags; Tokenization and embedding      4. Process: \"Calculate Sentiment Scores\"        Hover text: \"Model returns: overall score (-1 to +1), confidence level, emotion tags (joy, anger, sadness, etc.)\"        Shape: Rectangle        Color: Light blue        Details: Returns JSON: {sentiment: 0.65, confidence: 0.89, emotions: [\"joy\", \"excitement\"]}      5. Process: \"Update Content Node with Sentiment Properties\"        Hover text: \"Add properties: sentiment_score, sentiment_confidence, emotion_tags[], analyzed_timestamp\"        Shape: Rectangle        Color: Light blue        Details: Graph updated with sentiment metadata      6. Decision: \"Sentiment Highly Negative?\"        Hover text: \"Check if sentiment_score &lt; -0.7 AND confidence &gt; 0.8\"        Shape: Diamond        Color: Yellow        Details: Threshold for flagging potentially harmful content      7a. Process: \"Flag for Moderation\" (if YES to negative sentiment)         Hover text: \"Create FLAGGED_FOR_REVIEW relationship to ModQueue node, assign priority based on severity\"         Shape: Rectangle         Color: Orange         Details: Human moderator will review within 15 minutes      7b. Process: \"Publish Content\" (if NO to negative sentiment)         Hover text: \"Make content visible to followers/network, trigger notification system\"         Shape: Rectangle         Color: Green      8. Process: \"Update Aggregate Sentiment Metrics\"        Hover text: \"If content is about a product/topic, update rolling average sentiment for that entity\"        Shape: Rectangle        Color: Light blue        Details: Product node gets updated: avg_sentiment, recent_sentiment_trend, total_reviews      9. Process: \"Check for Sentiment Trends\"        Hover text: \"Detect sudden sentiment changes (e.g., product ratings dropping rapidly)\"        Shape: Rectangle        Color: Light blue        Details: Compare last 24h average vs. last 30d average      10. Decision: \"Significant Negative Trend?\"         Hover text: \"Has average sentiment dropped &gt;0.3 points in 24 hours?\"         Shape: Diamond         Color: Yellow      11a. Process: \"Alert Brand/Product Team\" (if YES to trend)          Hover text: \"Send notification to product owners about sentiment drop\"          Shape: Rectangle          Color: Red          Details: Dashboard alert created, email sent to stakeholders      11b. Process: \"Standard Analytics Update\" (if NO to trend)          Hover text: \"Update dashboards and reports with latest sentiment data\"          Shape: Rectangle          Color: Light blue      12. End: \"Content Published &amp; Analyzed\"         Hover text: \"Process complete, content live in network with sentiment metadata\"         Shape: Rounded rectangle (start/end)         Color: Green      Color coding:     - Green: Start/end points and successful outcomes     - Light blue: Standard processing steps     - Yellow: Decision points     - Orange: Warning/flagging actions     - Red: Alert/escalation actions      Swimlanes (horizontal lanes):     - User Layer (top): User interaction     - Application Layer: Content storage and processing     - Analysis Layer: NLP and sentiment calculation     - Moderation Layer: Flagging and review     - Analytics Layer (bottom): Trend detection and reporting      Annotations:     - Arrow from \"Calculate Sentiment Scores\" pointing to external box: \"External NLP API (OpenAI, Google Cloud NLP, or local model)\"     - Arrow from \"Update Aggregate Metrics\" pointing to data store icon: \"Product/Topic nodes in graph database\"     - Dashed line around moderation steps (7a, 11a) with label: \"Automated moderation pipeline\"      Implementation: Mermaid.js flowchart or Lucidchart-style diagram     Size: 1000x1400px to accommodate vertical flow and multiple decision branches  <p>Sentiment analysis becomes even more powerful when combined with graph structure. A product with mostly positive reviews but negative reviews from high-reputation reviewers might have quality issues that casual reviewers didn't notice. A political post with positive sentiment from your filter bubble but negative sentiment from outside your network might indicate polarization. Graph databases make these cross-dimensional analyses possible.</p>"},{"location":"chapters/07-social-network-modeling/#natural-language-processing-making-sense-of-unstructured-text","title":"Natural Language Processing: Making Sense of Unstructured Text","text":"<p>Sentiment is just one application of Natural Language Processing (NLP) in social networks. Modern NLP extracts structured information from unstructured text, turning messy human language into graph-friendly data. This bridges the gap between how humans communicate (long-form text) and how computers organize information (structured graphs).</p> <p>Common NLP tasks in social networks include:</p> <ul> <li>Entity extraction: Identifying people, places, organizations, products mentioned in text</li> <li>Topic modeling: Determining what subjects a post discusses (sports, politics, technology, etc.)</li> <li>Hashtag parsing: Extracting and linking to topic nodes</li> <li>Mention detection: Finding @mentions and creating relationships between users</li> <li>Link extraction: Identifying URLs and creating relationships to external content</li> <li>Spam detection: Recognizing patterns in text that indicate spam or bot behavior</li> <li>Content categorization: Auto-tagging posts for better discovery and recommendations</li> </ul> <p>Here's a powerful pattern: extract entities from text and create them as nodes in the graph. If someone posts \"Just visited the Louvre in Paris, amazing experience!\", NLP can extract: - Location entity: \"Louvre\" (museum) - Location entity: \"Paris\" (city) - Sentiment: Positive - Topic: Travel, Art</p> <p>These become relationships in the graph: User -[:VISITED]-&gt; Louvre -[:LOCATED_IN]-&gt; Paris, with properties like timestamp and sentiment. Now you can query \"show me people in my network who visited museums in Europe with positive sentiment\"\u2014a question that would be impossible without NLP extracting structure from unstructured text.</p> <p>Modern social networks use NLP constantly:</p> Platform NLP Application Graph Impact Twitter/X Trending topics extraction Creates TRENDING_NOW nodes, links to topic nodes LinkedIn Skills extraction from profiles Creates SKILLED_IN relationships to skill nodes YouTube Auto-generated video tags Creates TAGGED_WITH relationships to topic nodes Facebook Auto-tagging friends in photos Creates TAGGED_IN relationships to user nodes Reddit Subreddit recommendation Analyzes post content to suggest related communities Instagram Location tags and hashtags Creates POSTED_FROM and TAGGED relationships NLP Entity Extraction and Graph Building Diagram     Type: diagram      Purpose: Illustrate how NLP processes unstructured text from social media posts and extracts structured entities to add to the graph database      Layout: Left-to-right pipeline showing transformation from raw text to graph structure      STAGE 1 (Left): Raw User Post     - Visual: Speech bubble or post card containing sample text:       \"Just had an amazing lunch at @JoePizza in Brooklyn! Best margherita pizza \ud83c\udf55 in NYC. Highly recommend! #foodie #nycfood #pizza\"     - Label: \"Raw Unstructured Text Input\"     - Color: Light gray background      STAGE 2 (Center-left): NLP Processing     - Visual: Flowchart box labeled \"NLP Pipeline\" with substeps:       1. Tokenization: Break into words/tokens       2. Named Entity Recognition (NER): Identify entities       3. Sentiment Analysis: Determine emotional tone       4. Hashtag/Mention Extraction: Find tags and references       5. Category Classification: Determine topics     - Color: Blue background     - Arrows showing flow through each substep      STAGE 3 (Center-right): Extracted Entities     - Visual: List of structured data extracted:       * Mentioned Business: \"@JoePizza\" \u2192 Entity: \"Joe's Pizza\" (Restaurant)       * Location: \"Brooklyn\" \u2192 Entity: \"Brooklyn, NY\" (Place)       * Location: \"NYC\" \u2192 Entity: \"New York City\" (City)       * Product: \"margherita pizza\" \u2192 Entity: \"Margherita Pizza\" (Menu Item)       * Sentiment: Positive (score: 0.92)       * Topics: #foodie \u2192 \"Food\" topic                #nycfood \u2192 \"NYC Food\" topic                #pizza \u2192 \"Pizza\" topic       * Recommendation: \"Highly recommend\" \u2192 Intent: RECOMMENDS     - Color: Green background     - Label: \"Structured Entities &amp; Metadata\"      STAGE 4 (Right): Graph Database Updates     - Visual: Graph structure showing:        Central node: User \"Sarah\"        New relationships created:       1. Sarah -[:POSTED]-&gt; Post (content: \"Just had amazing lunch...\", timestamp: 2024-01-15, sentiment: 0.92)       2. Post -[:MENTIONS]-&gt; Restaurant \"Joe's Pizza\"       3. Post -[:LOCATED_IN]-&gt; Place \"Brooklyn\"       4. Post -[:TAGGED_WITH]-&gt; Topic \"Food\"       5. Post -[:TAGGED_WITH]-&gt; Topic \"NYC Food\"       6. Post -[:TAGGED_WITH]-&gt; Topic \"Pizza\"       7. Sarah -[:RECOMMENDS]-&gt; Restaurant \"Joe's Pizza\" (based_on_post: [post_id], sentiment: 0.92)       8. Restaurant \"Joe's Pizza\" -[:LOCATED_IN]-&gt; Place \"Brooklyn\"       9. Brooklyn -[:PART_OF]-&gt; City \"New York City\"        New/updated nodes:       - Restaurant node: \"Joe's Pizza\" (type: restaurant, cuisine: Italian, rating updates)       - Place nodes: \"Brooklyn\", \"New York City\"       - Topic nodes: \"Food\", \"NYC Food\", \"Pizza\"       - Post node: (sentiment: 0.92, timestamp, content)      - Color: Purple/pink background     - Label: \"Graph Database (Updated)\"     - Visual style: Network diagram with nodes and labeled edges      CALLOUT BOX (Bottom):     \"Enabled Queries After Processing:\"     - \"Find restaurants in Brooklyn recommended by people I follow\"     - \"Show me posts about pizza with positive sentiment\"     - \"Which NYC neighborhoods have the most food recommendations?\"     - \"Find users who share my interest in Italian food\"     - \"Alert Joe's Pizza to this positive mention\"      Visual style: Modern pipeline diagram with clear stages     Color scheme: Gray \u2192 Blue \u2192 Green \u2192 Purple (showing progression)     Include icons: Text icon, Brain icon (NLP), Database icon (graph)     Arrows between stages: Bold, directional, labeled with data type      Annotations:     - Note on NLP stage: \"Powered by models like spaCy, Stanford NER, or GPT\"     - Note on graph stage: \"Automated relationship creation enables rich queries\"     - Highlight: \"Same text \u2192 Multiple dimensions in graph\"      Implementation: Vector graphics (SVG) or diagramming tool     Size: 1200x700px for clear horizontal flow  <p>The combination of NLP and graphs creates powerful emergent properties. As more posts are analyzed, the graph learns patterns: certain hashtags cluster together, certain locations correlate with positive sentiment, certain users always mention the same topics. These patterns enable better recommendations, better search, and better content moderation\u2014all because NLP transformed unstructured chaos into structured knowledge.</p>"},{"location":"chapters/07-social-network-modeling/#fake-account-detection-fighting-bots-with-graph-patterns","title":"Fake Account Detection: Fighting Bots with Graph Patterns","text":"<p>Social networks face a constant battle against fake accounts: bots, spam accounts, impersonators, and coordinated inauthentic behavior. Traditional detection methods check individual account characteristics (new account, no profile photo, etc.), but graph-based detection is far more powerful because it analyzes relationship patterns.</p> <p>Real humans create organic social graphs with specific patterns: - Friends tend to cluster (your friends know each other) - Connections form gradually over time, not all at once - Activity patterns vary (some days active, some days quiet) - Content is diverse, not repetitive - Interactions are reciprocal (people reply to you, you reply to them)</p> <p>Fake accounts create different patterns: - Many connections made simultaneously (bulk following) - Connections are random, not clustered (follow anyone who follows back) - Identical or near-identical posts across multiple accounts (coordinated behavior) - One-way relationships (they follow thousands, few follow back) - Amplification networks (bots like and share each other's content)</p> <p>Graph databases excel at detecting these patterns because they can efficiently analyze network structure, not just node properties. Here are some graph-based detection signals:</p> <p>Clustering coefficient: Measures how interconnected a user's friends are. Real people: 0.3-0.7 (some friends know each other). Bots: often near 0 (random connections) or near 1 (tight bot network).</p> <p>Follow ratio: Following/Followers ratio. Real people: usually 0.5-2.0. Bots: often &gt;10 (follow thousands, get few followers back).</p> <p>Account age vs. activity: Real accounts gradually increase connections. Suspicious: brand new account with 1000 followers.</p> <p>Reciprocity rate: Percentage of mutual relationships. Real people: 40-70%. Bots: often &lt;10% (one-way follows).</p> <p>Content similarity: Posting identical or near-identical content as other accounts. Strong signal of coordination.</p> <p>Amplification network detection: Graph analysis can find clusters of accounts that always like/share each other's content\u2014a sign of coordinated inauthentic behavior.</p> Fake Account Detection Pattern MicroSim     Type: microsim      Learning objective: Visualize different network patterns created by real users vs. fake accounts, and let students explore detection algorithms      Canvas layout (1200x700px):     - Left side (800x700): Network visualization showing users and connections     - Right side (400x700): Detection controls, metrics, and results panel      Visual elements:     - User nodes colored by detection score:       * Dark green: Definitely real (score: 0.9-1.0)       * Light green: Probably real (score: 0.7-0.89)       * Yellow: Suspicious (score: 0.4-0.69)       * Orange: Likely fake (score: 0.2-0.39)       * Red: Almost certainly fake (score: 0.0-0.19)     - Node size based on follower count     - Edges show FOLLOWS relationships (gray arrows)     - When account selected, highlight its connections and show metrics      Interactive controls:     - Button: \"Generate Mixed Network\" (creates network with ~70% real, ~30% fake accounts)     - Button: \"Generate All Real Network\" (comparison baseline)     - Button: \"Generate Bot Network\" (extreme case)     - Dropdown: \"Detection Algorithm\"       * Clustering Coefficient Analysis       * Follow Ratio Analysis       * Account Age vs. Activity       * Content Similarity Detection       * Combined Score (default)     - Button: \"Run Detection\"     - Slider: \"Suspicion Threshold\" (0.0-1.0, default: 0.5)       * Accounts below threshold flagged as suspicious     - Checkbox: \"Show only flagged accounts\"     - Checkbox: \"Highlight bot networks\"      Network generation:      Real accounts (70%):     - Friends-of-friends connection pattern (clustering)     - Account age: 180-1800 days (random distribution)     - Follow ratio: 0.5-2.0 (balanced following/followers)     - Follower count: 50-500 (log-normal distribution)     - Following count: Similar to follower count (\u00b130%)     - Activity: Variable (10-100 posts, random intervals)     - Content: Unique text for each post     - Clustering coefficient: 0.3-0.7      Fake accounts (30%):     - Random connection pattern OR tight cluster (bot networks)     - Account age: 1-60 days (recently created)     - Follow ratio: 5-50 (following &gt;&gt; followers)     - Follower count: 10-100 (low)     - Following count: 500-5000 (very high)     - Activity: High volume in short time (100 posts in 7 days)     - Content: Repetitive or copied from other accounts     - Clustering coefficient: &lt;0.1 or &gt;0.9 (extremes)      Detection algorithms:      1. Clustering Coefficient:        - Calculate for each user: (# of connections between their friends) / (# possible connections)        - Suspicion score = distance from normal range (0.3-0.7)        - Display: \"Clustering: 0.05 (SUSPICIOUS - random connections)\"      2. Follow Ratio:        - Calculate: following_count / follower_count        - Suspicion score: penalize ratio &gt;3 or &lt;0.3        - Display: \"Follow ratio: 12.5 (SUSPICIOUS - following many, few followers)\"      3. Account Age vs. Activity:        - Calculate: posts_per_day = total_posts / account_age_days        - Suspicion if: new account (&lt;30 days) with high activity (&gt;5 posts/day)        - Display: \"7 days old, 89 posts (SUSPICIOUS - abnormal activity)\"      4. Content Similarity:        - Compare post content between accounts using simple text similarity        - Flag accounts with &gt;70% similar content to other accounts        - Display: \"83% content match with 4 other accounts (SUSPICIOUS - coordinated)\"      5. Combined Score:        - Weighted average of all metrics        - Weights: Clustering (25%), Follow Ratio (25%), Age/Activity (25%), Content (25%)        - Display: \"Overall suspicion: 0.82 (LIKELY FAKE)\"      Metrics panel (shown when account selected):     - Account age: [X] days     - Followers: [X]     - Following: [X]     - Follow ratio: [X]     - Total posts: [X]     - Posts per day: [X]     - Clustering coefficient: [X]     - Content uniqueness: [X]%     - Suspicion score: [X]     - Classification: REAL / SUSPICIOUS / LIKELY FAKE      Results panel (after running detection):     - Total accounts analyzed: [X]     - Flagged as suspicious: [X] ([X]%)     - True positives: [X] (correctly identified fakes)     - False positives: [X] (real accounts flagged incorrectly)     - Detection accuracy: [X]%     - Precision: [X]%     - Recall: [X]%      Behavior:     - On \"Generate Mixed Network\": Create graph with real and fake patterns     - On \"Run Detection\": Calculate scores for all accounts, color nodes by score     - Hover over node: Show quick metrics tooltip     - Click node: Show detailed metrics panel     - \"Show only flagged\": Hide green nodes, show only yellow/orange/red     - \"Highlight bot networks\": If multiple fake accounts all follow each other, draw thick red border around that cluster      Educational value:     - Shows visually how bot networks have different structure than real networks     - Demonstrates that no single metric catches all fakes (need combined approach)     - Illustrates false positives (some real accounts look suspicious)     - Shows how graph structure reveals patterns individual account properties miss     - Students can experiment with different thresholds and see precision/recall trade-offs      Special feature: \"Bot Network Visualization\"     - When detected, draw red outline around clusters where accounts:       * All created within 7 days of each other       * All follow each other (&gt;80% mutual follows within cluster)       * Post similar content (&gt;70% similarity)       * Have similar follow patterns     - Label: \"Coordinated inauthentic behavior detected\"      Implementation: p5.js with force-directed graph layout     Default state: Mixed network pre-generated, ready for detection to run     Animation: When running detection, show score calculation progress, then color nodes based on results  <p>Social networks combine multiple detection signals to calculate a \"fake account probability\" score. Accounts above a threshold get flagged for manual review or automatic restrictions. The beauty of graph-based detection is that it's harder to fake than individual account properties. Bots can add a profile photo and bio, but they can't easily create organic relationship patterns that evolve over years.</p> <p>This cat-and-mouse game continues: bot creators try to mimic real patterns, and detection systems adapt using machine learning on graph features. The next frontier is detecting AI-generated content that's grammatically perfect and topically diverse, making content-based detection harder. Graph structure remains a robust signal.</p>"},{"location":"chapters/07-social-network-modeling/#from-social-to-professional-human-resources-modeling","title":"From Social to Professional: Human Resources Modeling","text":"<p>All the patterns we've explored\u2014profiles, relationships, activity streams, influence, sentiment\u2014apply beyond social media. Human Resources departments face remarkably similar challenges: tracking who knows whom, who has what skills, who influences whom, and how to assign tasks effectively. The graph patterns are identical, just with different labels.</p> <p>Instead of friends and followers, HR deals with: - Org chart relationships: REPORTS_TO, MANAGES, WORKS_WITH - Skill networks: SKILLED_IN, REQUIRES_SKILL, ENDORSED_BY - Project relationships: ASSIGNED_TO, COLLABORATES_ON, OWNS - Expertise networks: EXPERT_IN, LEARNING, MENTORS</p> <p>An organization is fundamentally a social network with formal structure added on top. The org chart is the official directed graph (employee REPORTS_TO manager), but the actual work network is far more complex and organic\u2014people collaborate across departments, seek advice from unofficial mentors, and have expertise that doesn't match their job titles.</p> <p>Graph databases excel at modeling both the formal and informal organization simultaneously. You can query \"find someone with Python skills in the marketing department within 2 degrees of separation from me\" (combining skill network, org chart, and social network in one query). Try that with separate HR systems for org charts, skill databases, and project assignments!</p>"},{"location":"chapters/07-social-network-modeling/#org-chart-models-beyond-the-hierarchy","title":"Org Chart Models: Beyond the Hierarchy","text":"<p>Traditional org charts are trees: each employee has one manager, managers have one manager above them, up to the CEO at the root. But modern organizations are more complex. You might have a direct manager for performance reviews, a project lead for day-to-day work, and a mentor for career development. Graph databases handle this multi-dimensional reporting structure naturally.</p> <p>Here's what an org chart graph model includes:</p> <p>Node types: - Employee (with properties: name, title, department, hire_date, employee_id) - Department (with properties: name, cost_center, location) - Team (with properties: team_name, project, start_date) - Role (with properties: role_title, level, salary_band)</p> <p>Relationship types: - REPORTS_TO: Direct management chain (official hierarchy) - MANAGES: Inverse of REPORTS_TO, useful for manager-focused queries - MEMBER_OF: Department or team membership - COLLABORATES_WITH: Cross-functional working relationships - MENTORS: Informal development relationships - HAS_ROLE: Current role assignment - PREVIOUSLY_HELD: Historical roles (career progression)</p> Multi-Dimensional Org Chart Graph Model     Type: graph-model      Purpose: Show how modern organizations have multiple overlapping hierarchies (management reporting, project teams, mentorship) that are naturally represented in graph databases      Node types:      1. Employee (medium blue circles with initials)        - Properties: name, employee_id, title, email, hire_date, location        - Example nodes:          * \"Alice Chen\" (VP Engineering, hire_date: 2018)          * \"Bob Martinez\" (Senior Engineer, hire_date: 2020)          * \"Carol Johnson\" (Engineering Manager, hire_date: 2019)          * \"David Kim\" (Junior Engineer, hire_date: 2023)          * \"Eve Williams\" (Product Manager, hire_date: 2021)          * \"Frank Thompson\" (CTO, hire_date: 2017)          * \"Grace Lee\" (HR Director, hire_date: 2019)      2. Department (large purple squares)        - Properties: dept_name, budget, headcount        - Example nodes: \"Engineering\", \"Product\", \"HR\"      3. Project (green hexagons)        - Properties: project_name, status, deadline        - Example nodes: \"Mobile App Redesign\", \"API v2.0\", \"Q4 Infrastructure\"      4. Team (orange rounded rectangles)        - Properties: team_name, focus_area        - Example nodes: \"Backend Team\", \"Frontend Team\", \"Platform Team\"      5. Skill (small yellow triangles)        - Properties: skill_name, category        - Example nodes: \"Python\", \"Leadership\", \"System Design\", \"React\"      Edge types:      1. REPORTS_TO (solid thick blue arrows, hierarchical)        - Properties: since_date, reporting_type (direct/dotted-line)        - Management hierarchy:          * Alice \u2192 Frank (VP Engineering \u2192 CTO)          * Carol \u2192 Alice (Eng Manager \u2192 VP Engineering)          * Bob \u2192 Carol (Senior Eng \u2192 Eng Manager)          * David \u2192 Carol (Junior Eng \u2192 Eng Manager)          * Eve \u2192 Alice (Product Manager \u2192 VP Engineering, dotted-line)          * Grace \u2192 Frank (HR Director \u2192 CTO)      2. MEMBER_OF (solid purple arrows to departments)        - Properties: since_date, allocation_percentage        - Examples:          * Alice, Carol, Bob, David \u2192 Engineering (100%)          * Eve \u2192 Product (100%)          * Grace \u2192 HR (100%)          * Frank \u2192 Executive (100%)      3. ASSIGNED_TO (dashed green arrows to projects)        - Properties: role_on_project, allocation_percentage, start_date        - Examples:          * Bob ASSIGNED_TO \"API v2.0\" (Tech Lead, 80%)          * David ASSIGNED_TO \"API v2.0\" (Developer, 100%)          * Eve ASSIGNED_TO \"API v2.0\" (Product Owner, 50%)          * Carol ASSIGNED_TO \"Mobile App Redesign\" (Engineering Lead, 40%)      4. BELONGS_TO (solid orange arrows to teams)        - Properties: team_role, since_date        - Examples:          * Bob BELONGS_TO \"Backend Team\" (Senior Member)          * David BELONGS_TO \"Backend Team\" (Member)          * Carol BELONGS_TO \"Backend Team\" (Team Lead)      5. MENTORS (dotted gold arrows, employee to employee)        - Properties: mentorship_since, focus_areas[], meeting_frequency        - Examples:          * Alice MENTORS Eve (since: 2021, focus: [leadership, product strategy])          * Carol MENTORS David (since: 2023, focus: [technical skills, code review])          * Frank MENTORS Alice (since: 2019, focus: [executive leadership])      6. COLLABORATES_WITH (light blue undirected lines)        - Properties: projects_together[], interaction_frequency        - Examples:          * Bob \u2194 Eve (projects: [\"API v2.0\"], frequency: daily)          * Bob \u2194 David (projects: [\"API v2.0\"], frequency: daily)          * Carol \u2194 Alice (projects: [\"Mobile App\"], frequency: weekly)      7. SKILLED_IN (thin yellow arrows to skills)        - Properties: proficiency_level (1-10), years_experience, certified        - Examples:          * Bob SKILLED_IN \"Python\" (proficiency: 9, years: 8)          * Bob SKILLED_IN \"System Design\" (proficiency: 8, years: 6)          * David SKILLED_IN \"Python\" (proficiency: 5, years: 1)          * Eve SKILLED_IN \"Product Strategy\" (proficiency: 8, years: 4)          * Alice SKILLED_IN \"Leadership\" (proficiency: 9, years: 12)      Sample data focus: Bob Martinez (Senior Engineer)     - Reports to: Carol (Manager)     - Department: Engineering     - Project: API v2.0 (Tech Lead, 80% time)     - Team: Backend Team (Senior Member)     - Skills: Python (9/10), System Design (8/10), React (6/10)     - Collaborates with: Eve (Product Manager on API v2.0), David (mentoring relationship)     - Career path visible: Junior Eng (2020) \u2192 Mid-level Eng (2021) \u2192 Senior Eng (2023)      Layout: Hierarchical layout for management chain (top-down), with additional layers showing:     - Top: CTO (Frank)     - Second level: VP Engineering (Alice), HR Director (Grace)     - Third level: Engineering Manager (Carol), Product Manager (Eve - dotted line to Alice)     - Bottom: Engineers (Bob, David)     - Side clusters: Projects (connected to assigned employees)     - Side clusters: Skills (connected to skilled employees)      Interactive features:     - Hover over employee: Show summary (name, title, manager, department, current projects)     - Click employee: Highlight all their relationships (reports, projects, skills, collaborations, mentorships)     - Double-click employee: Show \"People finder\" - find paths to other employees     - Hover over project: Show all assigned employees and their roles     - Click skill: Highlight all employees with that skill, color by proficiency level     - Filter controls:       * \"Show only management chain\" (REPORTS_TO relationships only)       * \"Show only project teams\" (ASSIGNED_TO relationships only)       * \"Show collaboration network\" (COLLABORATES_WITH only)       * \"Show mentorship network\" (MENTORS only)       * \"Show skill network\" (SKILLED_IN only)     - Search: Find employee by name, skill, or project     - Query builder:       * \"Find Python experts in Engineering department\"       * \"Show reporting chain from David to Frank\"       * \"Find who Bob collaborates with regularly\"       * \"Show all people on API v2.0 project\"      Visual styling:     - Employee node size based on org level (larger = more senior)     - REPORTS_TO edges thicker and darker (emphasize hierarchy)     - MENTORS edges gold and dotted (warm, supportive relationship)     - ASSIGNED_TO edges dashed green (temporary project assignments)     - Color coding: Blue (org structure), Green (projects), Orange (teams), Yellow (skills), Gold (mentorship)     - Highlight path when showing relationships between two people      Legend (always visible):     - Node shapes: Circle (Employee), Square (Department), Hexagon (Project), Rounded rect (Team), Triangle (Skill)     - Edge types: Solid thick (reports), Solid thin (membership), Dashed (project), Dotted (mentorship), Undirected (collaboration)     - Color meanings for each relationship type      Educational value:     - Shows organizations are multi-dimensional, not just hierarchical trees     - Demonstrates how graph databases handle multiple simultaneous relationship types     - Illustrates informal networks (mentorship, collaboration) vs. formal (reporting)     - Shows how skills and projects cross organizational boundaries     - Demonstrates complex HR queries that would require many table joins in RDBMS      Implementation: vis-network JavaScript library with hierarchical layout option     Canvas size: 1200x900px with zoom, pan, and extensive filter controls     Default state: Show all relationship types, hierarchical layout centered on CTO  <p>The power of the graph model is you can traverse multiple relationship types in a single query. \"Find all engineers who report to someone I've worked with on a project, who have Python skills, and are not currently assigned to a project\" combines: - Social network (people I've worked with) - Org chart (who reports to them) - Skill network (Python filter) - Project network (current availability)</p> <p>This is the kind of query that would be a nightmare in traditional HR systems with separate databases for org charts, skills, and project assignments. In a graph database, it's elegant and fast.</p>"},{"location":"chapters/07-social-network-modeling/#skill-management-connecting-people-to-expertise","title":"Skill Management: Connecting People to Expertise","text":"<p>Organizations need to know who can do what. Traditional HR systems have employees select skills from a dropdown menu during annual reviews, resulting in stale, self-reported data that's often inaccurate. Graph-based skill management is far more dynamic and trustworthy.</p> <p>In a skill graph: - Skills are nodes (Python, Project Management, Customer Service, etc.) - Employees are nodes - Relationships include: SKILLED_IN, LEARNING, WANTS_TO_LEARN, ENDORSED_BY, TAUGHT</p> <p>The ENDORSED_BY relationship is particularly powerful. Like LinkedIn's skill endorsements, colleagues can endorse your skills, creating social proof. \"Bob says you're good at Python\" is more credible than you just claiming it yourself. Endorsements from senior engineers or people you've worked with on projects carry even more weight.</p> <p>Skill graphs enable sophisticated queries:</p> <ul> <li>Find experts: \"Show me the top 3 Python experts in the company (by endorsement count and years of experience)\"</li> <li>Find teachers: \"Who can teach React to our new hires? (people with high React proficiency who have the TEACHES relationship)\"</li> <li>Succession planning: \"If Alice leaves, who could take over her responsibilities? (people with overlapping skill sets)\"</li> <li>Team assembly: \"Build a team for a mobile app project (need: Swift, UI design, backend API, project management)\"</li> <li>Gap analysis: \"What skills do we need for our AI initiative that we don't currently have in-house?\"</li> <li>Learning paths: \"What skills should I learn next to move from Junior to Senior Engineer? (analyze skill patterns of current Senior Engineers)\"</li> </ul> <p>Skills can also have attributes like proficiency level (1-10), years of experience, certifications earned, and last used date. This turns \"Alice knows Python\" into \"Alice has 8/10 proficiency in Python with 5 years of experience, last used 2 months ago, with AWS Python certification.\"</p> <p>Modern organizations increasingly use skill graphs for talent mobility: instead of posting jobs and waiting for applications, they query the skill graph to find people who already have 80% of the required skills and might be interested in growing into the role. This makes career development more proactive and data-driven.</p>"},{"location":"chapters/07-social-network-modeling/#task-assignment-optimizing-project-workflows","title":"Task Assignment: Optimizing Project Workflows","text":"<p>The final piece of the social network puzzle is task assignment: matching work to people based on skills, availability, interests, and relationships. This is where all the patterns we've explored come together into a practical application.</p> <p>Task assignment graphs connect: - Tasks (nodes with properties: description, required_skills[], estimated_hours, priority, deadline) - People (employees with skills, availability, workload) - Projects (collections of tasks) - Teams (groups working together)</p> <p>The assignment process is a graph matching problem: 1. Task requires skills [Python, API design, testing] 2. Query graph for people with those skills who aren't overloaded 3. Consider preferences (who wants to work on backend projects?) 4. Consider relationships (who has worked together successfully before?) 5. Create ASSIGNED_TO relationship between person and task</p> Task Assignment Optimization Workflow     Type: workflow      Purpose: Show how task assignment in project management systems uses graph databases to match tasks with team members based on skills, availability, preferences, and team dynamics      Visual style: Swimlane flowchart with decision points and optimization steps      Swimlanes:     - Project Manager (top)     - Assignment System (middle - main process flow)     - Graph Database (queries and analysis)     - Team Members (bottom - notifications)      Process Flow:      1. Start: \"New Task Created\" (Project Manager lane)        Hover text: \"PM creates task: 'Implement user authentication API', priority: high, deadline: 2 weeks\"        Shape: Rounded rectangle        Color: Green      2. Process: \"Extract Task Requirements\" (Assignment System)        Hover text: \"Parse task description, identify required skills, estimate effort hours\"        Details:          - Required skills: [Python, API design, Security, Testing]          - Estimated hours: 40          - Priority: High          - Deadline: 14 days      3. Process: \"Query Skill Graph\" (Graph Database)        Hover text: \"MATCH (person:Employee)-[s:SKILLED_IN]-&gt;(skill:Skill) WHERE skill.name IN ['Python', 'API design', 'Security', 'Testing'] RETURN person, skill, s.proficiency\"        Details: Returns candidates with skill matches and proficiency levels      4. Process: \"Check Availability\" (Graph Database)        Hover text: \"MATCH (person)-[:ASSIGNED_TO]-&gt;(task) RETURN person, SUM(task.remaining_hours) as current_workload\"        Details: Calculate current workload for each candidate      5. Process: \"Calculate Match Scores\" (Assignment System)        Hover text: \"Score each candidate based on: skill match (40%), availability (30%), past performance (20%), preferences (10%)\"        Details:          - Skill match: How many required skills they have, at what proficiency          - Availability: Current workload vs capacity (40h/week)          - Past performance: Success rate on similar tasks          - Preferences: Interest in this type of work      6. Decision: \"Clear Best Match?\" (Assignment System)        Hover text: \"Is there one candidate with score &gt;0.85 and available capacity?\"        Shape: Diamond        Color: Yellow      7a. Process: \"Auto-Assign to Best Match\" (if YES)         Hover text: \"Create ASSIGNED_TO relationship, update task status, send notification\"         Color: Green      7b. Process: \"Generate Candidate List\" (if NO - tie or no clear winner)         Hover text: \"Create ranked list of top 3-5 candidates with scores and reasoning\"         Color: Orange      8b. Process: \"Present Options to PM\" (Assignment System \u2192 Project Manager)         Hover text: \"Show: Candidate A (score: 0.78, available next week), Candidate B (score: 0.75, available now but less experience), etc.\"      9b. Decision: \"PM Selects Candidate\" (Project Manager)         Hover text: \"PM reviews options and makes final choice based on strategic considerations\"      10. Process: \"Create Assignment\" (Assignment System)         Hover text: \"Create ASSIGNED_TO relationship in graph database\"         Details:           - Edge properties: assigned_date, estimated_hours, priority, deadline           - Update person's workload           - Update task status to 'assigned'      11. Process: \"Update Team Network\" (Graph Database)         Hover text: \"Strengthen COLLABORATES_WITH relationships between assigned person and project team members\"         Details: Increment collaboration counter, update last_collaboration_date      12. Process: \"Check Team Balance\" (Assignment System)         Hover text: \"Analyze workload distribution across team to prevent burnout and ensure fairness\"         Metrics:           - Workload variance across team           - Skills being utilized vs. sitting idle           - New vs. routine work distribution      13. Decision: \"Team Imbalanced?\" (Assignment System)         Hover text: \"Is anyone &gt;120% capacity or &gt;50% team idle?\"         Shape: Diamond      14a. Process: \"Suggest Rebalancing\" (if YES to imbalance)          Hover text: \"Generate suggestions: 'Consider moving Task X from Bob to Carol to balance workload'\"          Color: Orange      14b. Process: \"Continue Monitoring\" (if NO to imbalance)          Color: Green      15. Process: \"Notify Assigned Person\" (Team Members lane)         Hover text: \"Send notification: 'You've been assigned to task: Implement user auth API, deadline: Nov 1'\"         Details: Include context, required skills, related team members, priority      16. Process: \"Log Assignment for Learning\" (Graph Database)         Hover text: \"Record assignment for future analysis of assignment patterns and outcomes\"         Details: Used to improve matching algorithm over time      17. End: \"Task Assigned &amp; Tracked\" (Assignment System)         Hover text: \"Task actively assigned, progress tracking begins\"         Shape: Rounded rectangle         Color: Green      PARALLEL PROCESS (runs continuously):     - \"Monitor Task Progress\" \u2192 \"Update Skill Proficiency Based on Performance\"     - Hover text: \"As tasks complete, update person's skill proficiency and track success/failure patterns\"      Annotations:      - Arrow from \"Calculate Match Scores\" to callout box:       \"Scoring Formula Example:        Score = (skill_match \u00d7 0.4) + (availability \u00d7 0.3) + (past_performance \u00d7 0.2) + (interest \u00d7 0.1)         Candidate Alice:        - Skills: Python (9/10), API (8/10), Security (7/10), Testing (6/10) \u2192 0.90        - Availability: 10h/40h used this week \u2192 0.75        - Past performance: 8/10 similar tasks succeeded \u2192 0.80        - Interest: Expressed interest in security work \u2192 0.90        TOTAL: (0.90\u00d70.4) + (0.75\u00d70.3) + (0.80\u00d70.2) + (0.90\u00d70.1) = 0.845\"      - Arrow from \"Check Team Balance\" to metrics dashboard:       \"Team Workload Dashboard:        Alice: 35h (88% capacity) \u2713        Bob: 52h (130% capacity) \u26a0\ufe0f OVERLOADED        Carol: 18h (45% capacity) \u2713        David: 40h (100% capacity) \u2713        \u2192 Suggestion: Reassign 12h from Bob to Carol\"      Color coding:     - Green: Successful completion, optimal state     - Yellow: Decision points     - Orange: Manual intervention needed, warnings     - Blue: Graph database operations     - Purple: Notifications and communication      Visual elements:     - Graph database icon next to query boxes     - Person icons in Team Members lane     - Calendar icon for deadline checks     - Scale/balance icon for workload balancing     - Bell icon for notifications      Implementation: Flowchart with swimlanes (BPMN style)     Size: 1400x1000px to accommodate detailed workflow and annotations  <p>Good task assignment systems also learn from history. If Alice and Bob have successfully collaborated on 5 previous projects, the system might prioritize assigning them to work together again. If Carol struggled with a backend task last quarter, maybe don't assign her to another backend task until she's completed some training.</p> <p>The graph also reveals patterns like: \"Data science tasks take 30% longer when assigned to people outside the data science team\" or \"urgent bugs get fixed faster when assigned to the person who originally wrote that code (found via AUTHORED relationship).\"</p>"},{"location":"chapters/07-social-network-modeling/#backlog-management-prioritizing-the-work-queue","title":"Backlog Management: Prioritizing the Work Queue","text":"<p>Every development team has a backlog: a list of tasks, features, and bugs waiting to be addressed. Traditional backlogs are flat lists sorted by priority, but graph-based backlog management captures the rich dependencies and relationships between tasks.</p> <p>Tasks can have relationships like: - DEPENDS_ON: Task B can't start until Task A is complete - BLOCKS: Task A is blocking Task B (inverse of DEPENDS_ON) - RELATED_TO: Tasks share common themes or components - DUPLICATES: Tasks are essentially the same work - SUBTASK_OF: Task hierarchy for breaking down large features</p> <p>With these relationships, the graph reveals: - Critical path: Which tasks block the most other work? - Parallel work: Which tasks can be done simultaneously? - Bottlenecks: Which task dependencies create waiting time? - Orphaned tasks: Which tasks have no dependencies and could be done anytime? - Impact radius: If we do this task, how many blocked tasks become unblocked?</p> <p>Graph visualization makes backlog prioritization much clearer. Instead of staring at a flat list of 200 tasks wondering what to do first, you see a network diagram showing that fixing bug #423 would unblock 15 other tasks, making it high-priority even though it seemed minor in isolation.</p> <p>Modern backlog management also incorporates: - User story relationships (epic \u2192 feature \u2192 task) - Skill requirements (to suggest who should work on what) - Customer impact (which features affect the most users) - Technical debt tracking (which components are fragile and need refactoring)</p> <p>All of this is naturally modeled in a graph, where nodes are tasks and edges are dependencies, relationships, and impacts.</p>"},{"location":"chapters/07-social-network-modeling/#putting-it-all-together-the-social-graph-powers-everything","title":"Putting It All Together: The Social Graph Powers Everything","text":"<p>We started this chapter talking about Instagram and X, but by now you've seen that social network patterns appear everywhere: - Product review platforms (users, reviews, products, reputation) - GitHub (developers, repositories, commits, stars, followers) - HR systems (employees, skills, projects, org charts) - Project management (tasks, assignments, dependencies, teams) - Content platforms (creators, content, comments, likes, shares)</p> <p>The common thread is people connected by relationships, creating content, building reputation, and influencing others. Graph databases excel at modeling these patterns because they mirror how humans naturally think about social connections.</p> <p>When you're building any system where people interact, consider these questions: - What types of users exist in my system? - How do they connect to each other? - What content do they create? - How do others react to that content? - How is reputation and trust established? - How do I detect bad actors or fake accounts? - How do I make good recommendations?</p> <p>If you can answer these questions by drawing nodes and edges, you're thinking in graphs\u2014and you're ready to build modern social systems that scale gracefully and query efficiently.</p> <p>The things you learned in this chapter apply to any system where reputation matters, where people comment on things, where social proof influences decisions, where expertise needs to be found, or where relationships drive value. That's not just social media\u2014that's most of the modern digital world.</p> <p>Welcome to seeing social networks everywhere. You're welcome (or sorry, depending on your perspective).</p>"},{"location":"chapters/08-knowledge-representation-management/","title":"Knowledge Representation and Management","text":""},{"location":"chapters/08-knowledge-representation-management/#summary","title":"Summary","text":"<p>This chapter explores how graph databases excel at representing and managing knowledge structures including ontologies, taxonomies, and concept dependency graphs. You'll learn the Simple Knowledge Organization System (SKOS) for managing preferred and alternate labels, create controlled vocabularies and glossaries, and design curriculum graphs that model learning dependencies. The chapter covers knowledge management at multiple scales from personal knowledge graphs and note-taking systems to enterprise-wide knowledge capture and management platforms.</p>"},{"location":"chapters/08-knowledge-representation-management/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 20 concepts from the learning graph:</p> <ol> <li>Concept Dependency Graphs</li> <li>Curriculum Graphs</li> <li>Ontologies</li> <li>SKOS</li> <li>Preferred Labels</li> <li>Alternate Labels</li> <li>Acronym Lists</li> <li>Glossaries</li> <li>Controlled Vocabularies</li> <li>Taxonomies</li> <li>Enterprise Knowledge</li> <li>Department Knowledge</li> <li>Project Knowledge</li> <li>Personal Knowledge Graphs</li> <li>Note-Taking Systems</li> <li>Knowledge Capture</li> <li>Tacit Knowledge</li> <li>Codifiable Knowledge</li> <li>Knowledge Management</li> <li>Action Item Extraction</li> </ol>"},{"location":"chapters/08-knowledge-representation-management/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 3: Labeled Property Graph Information Model</li> <li>Chapter 4: Query Languages for Graph Databases</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/09-modeling-patterns-data-loading/","title":"Graph Modeling Patterns and Data Loading","text":""},{"location":"chapters/09-modeling-patterns-data-loading/#summary","title":"Summary","text":"<p>This chapter covers essential design patterns and anti-patterns for graph data modeling, helping you create maintainable and performant graph schemas. You'll explore subgraphs, supernodes, hyperedges, and multi-edges while learning time-based modeling patterns for temporal data and IoT events. The chapter provides comprehensive coverage of data loading strategies including ETL pipelines, CSV and JSON import techniques, and bulk versus incremental loading approaches, along with schema evolution and migration best practices.</p>"},{"location":"chapters/09-modeling-patterns-data-loading/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 18 concepts from the learning graph:</p> <ol> <li>Subgraphs</li> <li>Supernodes</li> <li>Anti-Patterns</li> <li>Hyperedges</li> <li>Multi-Edges</li> <li>Time-Based Modeling</li> <li>IoT Event Modeling</li> <li>Bitemporal Models</li> <li>Graph Quality Metrics</li> <li>Model Validation</li> <li>Schema Evolution</li> <li>Data Migration</li> <li>ETL Pipelines</li> <li>CSV Import</li> <li>JSON Import</li> <li>Data Loading</li> <li>Bulk Loading</li> <li>Incremental Loading</li> </ol>"},{"location":"chapters/09-modeling-patterns-data-loading/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 3: Labeled Property Graph Information Model</li> <li>Chapter 5: Performance, Metrics, and Benchmarking</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/10-commerce-supply-chain-it/","title":"Commerce, Supply Chain, and IT Infrastructure","text":""},{"location":"chapters/10-commerce-supply-chain-it/#summary","title":"Summary","text":"<p>This chapter demonstrates graph database applications in e-commerce, supply chain management, and IT infrastructure. You'll learn to model web storefronts with product catalogs, design recommendation engines using graph algorithms, and manage complex bill-of-materials structures for manufacturing. The chapter extends to IT asset management, network topology modeling, configuration management, and critical operational applications including impact analysis and root cause analysis for infrastructure troubleshooting.</p>"},{"location":"chapters/10-commerce-supply-chain-it/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 12 concepts from the learning graph:</p> <ol> <li>Web Storefront Models</li> <li>Product Catalogs</li> <li>Recommendation Engines</li> <li>Bill of Materials</li> <li>Complex Parts</li> <li>Supply Chain Modeling</li> <li>IT Asset Management</li> <li>Dependency Graphs</li> <li>Network Topology</li> <li>Configuration Management</li> <li>Impact Analysis</li> <li>Root Cause Analysis</li> </ol>"},{"location":"chapters/10-commerce-supply-chain-it/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 3: Labeled Property Graph Information Model</li> <li>Chapter 6: Graph Algorithms</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/11-financial-healthcare-regulatory/","title":"Financial, Healthcare, and Regulatory Applications","text":""},{"location":"chapters/11-financial-healthcare-regulatory/#summary","title":"Summary","text":"<p>This chapter explores graph database applications in highly regulated industries including finance and healthcare. You'll learn to model financial transaction networks, implement fraud detection systems using community detection algorithms, and build anti-money laundering (AML) and know-your-customer (KYC) compliance systems. The chapter covers healthcare-specific applications including provider-patient graphs, electronic health record modeling, and clinical pathway optimization, while addressing regulatory compliance, data lineage tracking, and master data management requirements common across regulated industries.</p>"},{"location":"chapters/11-financial-healthcare-regulatory/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 13 concepts from the learning graph:</p> <ol> <li>Financial Transactions</li> <li>Fraud Detection</li> <li>Anti-Money Laundering</li> <li>Know Your Customer</li> <li>Account Networks</li> <li>Healthcare Graphs</li> <li>Provider-Patient Graphs</li> <li>Electronic Health Records</li> <li>Clinical Pathways</li> <li>Regulatory Compliance</li> <li>Data Lineage</li> <li>Master Data Management</li> <li>Reference Data Models</li> </ol>"},{"location":"chapters/11-financial-healthcare-regulatory/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 3: Labeled Property Graph Information Model</li> <li>Chapter 6: Graph Algorithms</li> <li>Chapter 7: Social Network Modeling</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/12-advanced-topics-distributed-systems/","title":"Advanced Topics and Distributed Systems","text":""},{"location":"chapters/12-advanced-topics-distributed-systems/#summary","title":"Summary","text":"<p>This capstone chapter covers advanced graph database concepts including distributed architectures, real-time analytics, and visualization techniques. You'll explore graph partitioning and sharding strategies for horizontal scalability, understand replication and consistency models in distributed systems, and learn to design interactive graph visualizations. The chapter culminates with capstone project design guidance, helping you synthesize all course concepts into a complete end-to-end graph application that demonstrates mastery of graph database modeling, querying, performance optimization, and real-world application development.</p>"},{"location":"chapters/12-advanced-topics-distributed-systems/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 10 concepts from the learning graph:</p> <ol> <li>Distributed Graph Databases</li> <li>Graph Partitioning</li> <li>Sharding Strategies</li> <li>Replication</li> <li>Consistency Models</li> <li>Graph Visualization</li> <li>Interactive Queries</li> <li>Real-Time Analytics</li> <li>Batch Processing</li> <li>Capstone Project Design</li> </ol>"},{"location":"chapters/12-advanced-topics-distributed-systems/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Database Systems and NoSQL</li> <li>Chapter 3: Labeled Property Graph Information Model</li> <li>Chapter 4: Query Languages for Graph Databases</li> <li>Chapter 5: Performance, Metrics, and Benchmarking</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"learning-graph/","title":"Learning Graph for Introduction to Graph Databases","text":"<p>This section contains the learning graph for this textbook.  A learning graph is a graph of concepts used in this textbook.  Each concept is represented by a node in a network graph.  Concepts are connected by directed edges that indicate what concepts each node depends on before that concept is understood by the student.</p> <p>A learning graph is the foundational data structure for intelligent textbooks that can recommend learning paths. A learning graph is like a roadmap of concepts to help students arrive at their learning goals.</p> <p>At the left of the learning graph are prerequisite or foundational concepts.  They have no outbound edges.  They only have inbound edges for other concepts that depend on understanding these foundational prerequisite concepts.  At the far right we have the most advanced concepts in the course.  To master these concepts you must understand all the concepts that they point to.</p> <p>Here are other files used by the learning graph.</p>"},{"location":"learning-graph/#course-description","title":"Course Description","text":"<p>We use the Course Description as the source document for the concepts that are included in this course. The course description uses the 2001 Bloom taxonomy to order learning objectives.</p>"},{"location":"learning-graph/#list-of-concepts","title":"List of Concepts","text":"<p>We use generative AI to convert the course description into a Concept List. Each concept is in the form of a short Title Case label with most labels under 32 characters long.</p>"},{"location":"learning-graph/#concept-dependency-list","title":"Concept Dependency List","text":"<p>We next use generative AI to create a Directed Acyclic Graph (DAG).  DAGs do not have cycles where concepts depend on themselves.  We provide the DAG in two formats.  One is a CSV file and the other format is a JSON file that uses the vis-network JavaScript library format.  The vis-network format uses <code>nodes</code>, <code>edges</code> and <code>metadata</code> elements with edges containing <code>from</code> and <code>to</code> properties.  This makes it easy for you to view and edit the learning graph using an editor built with the vis-network tools.</p>"},{"location":"learning-graph/#analysis-documentation","title":"Analysis &amp; Documentation","text":""},{"location":"learning-graph/#course-description-quality-assessment","title":"Course Description Quality Assessment","text":"<p>This report rates the overall quality of the course description for the purpose of generating a learning graph.</p> <ul> <li>Course description fields and content depth analysis</li> <li>Validates course description has sufficient depth for generating 200 concepts</li> <li>Compares course description against similar courses</li> <li>Identifies content gaps and strengths</li> <li>Suggests areas of improvement</li> </ul> <p>View the Course Description Quality Assessment</p>"},{"location":"learning-graph/#learning-graph-quality-validation","title":"Learning Graph Quality Validation","text":"<p>This report gives you an overall assessment of the quality of the learning graph. It uses graph algorithms to look for specific quality patterns in the graph.</p> <ul> <li>Graph structure validation - all concepts are connected</li> <li>DAG validation (no cycles detected)</li> <li>Foundational concepts: 6 entry points</li> <li>Indegree distribution analysis</li> <li>Longest dependency chains (18 levels)</li> <li>Connectivity: 100% of nodes connected to the main cluster</li> </ul> <p>View the Learning Graph Quality Validation</p>"},{"location":"learning-graph/#concept-taxonomy","title":"Concept Taxonomy","text":"<p>In order to see patterns in the learning graph, it is useful to assign colors to each concept based on the concept type.  We use generative AI to create about a dozen categories for our concepts and then place each concept into a single primary classifier.</p> <ul> <li>A concept classifier taxonomy with 12 categories</li> <li>Category organization - foundational elements first, course capstone project ideas last</li> <li>Balanced categories (2.0% - 21.0% each)</li> <li>All categories under 30% threshold</li> <li>Pedagogical flow recommendations</li> <li>Clear 3-5 letter abbreviations for use in CSV file</li> </ul> <p>View the Concept Taxonomy</p>"},{"location":"learning-graph/#taxonomy-distribution","title":"Taxonomy Distribution","text":"<p>This reports shows how many concepts fit into each category of the taxonomy. Our goal is a somewhat balanced taxonomy where each category holds an equal number of concepts.  We also don't want any category to contain over 30% of our concepts.</p> <ul> <li>Statistical breakdown showing 12 taxonomies across 200 concepts</li> <li>Detailed concept listing by category</li> <li>Visual distribution table</li> <li>Balance verification - largest category (GRAPH) at 21%</li> </ul> <p>View the Taxonomy Distribution Report</p>"},{"location":"learning-graph/concept-list/","title":"Concept List for Introduction to Graph Databases","text":"<p>Total Concepts: 200 Generated: 2025-11-18 Skill Version: 0.02</p>"},{"location":"learning-graph/concept-list/#foundational-concepts-1-20","title":"Foundational Concepts (1-20)","text":"<ol> <li>Data Modeling</li> <li>World Models</li> <li>Knowledge Representation</li> <li>RDBMS</li> <li>OLAP</li> <li>OLTP</li> <li>NoSQL Databases</li> <li>Key-Value Stores</li> <li>Document Databases</li> <li>Wide-Column Stores</li> <li>Graph Databases</li> <li>CAP Theorem</li> <li>Tradeoff Analysis</li> <li>Schema Design</li> <li>Data Structures</li> <li>Hash Maps</li> <li>Trees</li> <li>Arrays</li> <li>Relational Model</li> <li>Normalization</li> </ol>"},{"location":"learning-graph/concept-list/#graph-database-fundamentals-21-45","title":"Graph Database Fundamentals (21-45)","text":"<ol> <li>Labeled Property Graph</li> <li>Nodes</li> <li>Edges</li> <li>Properties</li> <li>Labels</li> <li>Schema-Optional Modeling</li> <li>Schema-Enforced Modeling</li> <li>Index-Free Adjacency</li> <li>Traversal</li> <li>Graph Query</li> <li>Pattern Matching</li> <li>Multi-Hop Queries</li> <li>Aggregation</li> <li>Path Patterns</li> <li>Constant-Time Neighbor Access</li> <li>First-Class Relationships</li> <li>Edge Direction</li> <li>Graph Data Model</li> <li>Graph Schema</li> <li>Metadata Representation</li> <li>Open World Model</li> <li>Closed World Model</li> <li>Graph Validation</li> <li>Document Validation</li> <li>Rule Systems</li> </ol>"},{"location":"learning-graph/concept-list/#query-languages-46-70","title":"Query Languages (46-70)","text":"<ol> <li>OpenCypher</li> <li>GSQL</li> <li>Statistical Query Tuning</li> <li>GQL</li> <li>Cypher Syntax</li> <li>Match Clause</li> <li>Where Clause</li> <li>Return Clause</li> <li>Create Statement</li> <li>Merge Statement</li> <li>Delete Statement</li> <li>Set Clause</li> <li>Graph Patterns</li> <li>Variable Length Paths</li> <li>Shortest Path</li> <li>All Paths</li> <li>Map-Reduce Pattern</li> <li>Accumulators</li> <li>Query Optimization</li> <li>Query Performance</li> <li>Query Latency</li> <li>Query Throughput</li> <li>Declarative Queries</li> <li>Imperative Queries</li> <li>Query Plans</li> </ol>"},{"location":"learning-graph/concept-list/#performance-and-indexing-71-90","title":"Performance and Indexing (71-90)","text":"<ol> <li>Hop Count</li> <li>Degree of Node</li> <li>Indegree</li> <li>Outdegree</li> <li>Edge-to-Node Ratio</li> <li>Graph Indexes</li> <li>Vector Indexes</li> <li>Full-Text Search</li> <li>Composite Indexes</li> <li>Graph Metrics</li> <li>Performance Benchmarking</li> <li>Synthetic Benchmarks</li> <li>Single-Node Benchmarks</li> <li>Multi-Node Benchmarks</li> <li>LDBC SNB Benchmark</li> <li>Graph 500</li> <li>Query Cost Analysis</li> <li>Join Operations</li> <li>Traversal Cost</li> <li>Scalability</li> </ol>"},{"location":"learning-graph/concept-list/#graph-algorithms-91-110","title":"Graph Algorithms (91-110)","text":"<ol> <li>Breadth-First Search</li> <li>Depth-First Search</li> <li>A-Star Algorithm</li> <li>Pathfinding</li> <li>Traveling Salesman Problem</li> <li>PageRank</li> <li>Community Detection</li> <li>Centrality Measures</li> <li>Betweenness Centrality</li> <li>Closeness Centrality</li> <li>Graph Embeddings</li> <li>Graph Neural Networks</li> <li>Link Prediction</li> <li>Node Classification</li> <li>Graph Clustering</li> <li>Shortest Path Algorithms</li> <li>Minimum Spanning Tree</li> <li>Connected Components</li> <li>Strongly Connected Components</li> <li>Weakly Connected Components</li> </ol>"},{"location":"learning-graph/concept-list/#social-network-modeling-111-125","title":"Social Network Modeling (111-125)","text":"<ol> <li>Social Networks</li> <li>Friend Graphs</li> <li>Influence Graphs</li> <li>Follower Networks</li> <li>Activity Streams</li> <li>User Profiles</li> <li>Relationship Types</li> <li>Sentiment Analysis</li> <li>Natural Language Processing</li> <li>Fake Account Detection</li> <li>Human Resources Modeling</li> <li>Org Chart Models</li> <li>Skill Management</li> <li>Task Assignment</li> <li>Backlog Management</li> </ol>"},{"location":"learning-graph/concept-list/#knowledge-representation-126-145","title":"Knowledge Representation (126-145)","text":"<ol> <li>Concept Dependency Graphs</li> <li>Curriculum Graphs</li> <li>Ontologies</li> <li>SKOS</li> <li>Preferred Labels</li> <li>Alternate Labels</li> <li>Acronym Lists</li> <li>Glossaries</li> <li>Controlled Vocabularies</li> <li>Taxonomies</li> <li>Enterprise Knowledge</li> <li>Department Knowledge</li> <li>Project Knowledge</li> <li>Personal Knowledge Graphs</li> <li>Note-Taking Systems</li> <li>Knowledge Capture</li> <li>Tacit Knowledge</li> <li>Codifiable Knowledge</li> <li>Knowledge Management</li> <li>Action Item Extraction</li> </ol>"},{"location":"learning-graph/concept-list/#graph-modeling-patterns-146-165","title":"Graph Modeling Patterns (146-165)","text":"<ol> <li>Subgraphs</li> <li>Supernodes</li> <li>Anti-Patterns</li> <li>Hyperedges</li> <li>Multi-Edges</li> <li>Time-Based Modeling</li> <li>Time Trees</li> <li>IoT Event Modeling</li> <li>Decision Trees</li> <li>Bitemporal Models</li> <li>Graph Quality Metrics</li> <li>Model Validation</li> <li>Schema Evolution</li> <li>Data Migration</li> <li>ETL Pipelines</li> <li>CSV Import</li> <li>JSON Import</li> <li>Data Loading</li> <li>Bulk Loading</li> <li>Incremental Loading</li> </ol>"},{"location":"learning-graph/concept-list/#industry-applications-166-190","title":"Industry Applications (166-190)","text":"<ol> <li>Web Storefront Models</li> <li>Product Catalogs</li> <li>Recommendation Engines</li> <li>Bill of Materials</li> <li>Complex Parts</li> <li>Supply Chain Modeling</li> <li>Financial Transactions</li> <li>Fraud Detection</li> <li>Anti-Money Laundering</li> <li>Know Your Customer</li> <li>Account Networks</li> <li>Healthcare Graphs</li> <li>Provider-Patient Graphs</li> <li>Electronic Health Records</li> <li>Clinical Pathways</li> <li>IT Asset Management</li> <li>Dependency Graphs</li> <li>Network Topology</li> <li>Configuration Management</li> <li>Impact Analysis</li> <li>Root Cause Analysis</li> <li>Regulatory Compliance</li> <li>Data Lineage</li> <li>Master Data Management</li> <li>Reference Data Models</li> </ol>"},{"location":"learning-graph/concept-list/#advanced-topics-191-200","title":"Advanced Topics (191-200)","text":"<ol> <li>Distributed Graph Databases</li> <li>Graph Partitioning</li> <li>Sharding Strategies</li> <li>Replication</li> <li>Consistency Models</li> <li>Graph Visualization</li> <li>Interactive Queries</li> <li>Real-Time Analytics</li> <li>Batch Processing</li> <li>Capstone Project Design</li> </ol>"},{"location":"learning-graph/concept-taxonomy/","title":"Concept Taxonomy","text":"<p>Date: 2025-11-18 Total Concepts: 200 Target Categories: 12</p>"},{"location":"learning-graph/concept-taxonomy/#taxonomy-categories","title":"Taxonomy Categories","text":""},{"location":"learning-graph/concept-taxonomy/#1-foundation-concepts-found","title":"1. Foundation Concepts (FOUND)","text":"<p>Description: Core foundational concepts including data structures, data modeling principles, and basic knowledge representation that underpin all graph database learning.</p> <p>Typical Concepts: Data Modeling, World Models, Knowledge Representation, Schema Design, Hash Maps, Trees, Arrays</p>"},{"location":"learning-graph/concept-taxonomy/#2-database-systems-dbsys","title":"2. Database Systems (DBSYS)","text":"<p>Description: Traditional and NoSQL database systems including RDBMS, OLAP, OLTP, key-value stores, document databases, and wide-column stores that provide context for graph databases.</p> <p>Typical Concepts: RDBMS, NoSQL, Key-Value Stores, Document Databases, CAP Theorem, Normalization, Relational Model</p>"},{"location":"learning-graph/concept-taxonomy/#3-graph-data-model-graph","title":"3. Graph Data Model (GRAPH)","text":"<p>Description: Core graph database concepts including the Labeled Property Graph model, nodes, edges, properties, labels, schema approaches, and fundamental graph structures.</p> <p>Typical Concepts: Labeled Property Graph, Nodes, Edges, Properties, Labels, Index-Free Adjacency, Traversal, Schema-Optional Modeling</p>"},{"location":"learning-graph/concept-taxonomy/#4-query-languages-query","title":"4. Query Languages (QUERY)","text":"<p>Description: Graph query languages and syntax including OpenCypher, GSQL, GQL, query patterns, path expressions, and query optimization techniques.</p> <p>Typical Concepts: OpenCypher, GSQL, GQL, Cypher Syntax, Match Clause, Pattern Matching, Variable Length Paths, Query Optimization</p>"},{"location":"learning-graph/concept-taxonomy/#5-performance-perf","title":"5. Performance (PERF)","text":"<p>Description: Performance analysis, benchmarking, indexing strategies, and metrics for evaluating graph database systems.</p> <p>Typical Concepts: Performance Benchmarking, Graph Indexes, Query Latency, LDBC SNB Benchmark, Graph 500, Scalability, Hop Count</p>"},{"location":"learning-graph/concept-taxonomy/#6-graph-algorithms-algo","title":"6. Graph Algorithms (ALGO)","text":"<p>Description: Classic and modern graph algorithms including search, pathfinding, centrality measures, community detection, and graph neural networks.</p> <p>Typical Concepts: Breadth-First Search, Depth-First Search, PageRank, Community Detection, Pathfinding, Graph Neural Networks</p>"},{"location":"learning-graph/concept-taxonomy/#7-social-networks-social","title":"7. Social Networks (SOCIAL)","text":"<p>Description: Social network modeling including friend graphs, influence networks, organizational structures, activity streams, and human resources applications.</p> <p>Typical Concepts: Social Networks, Friend Graphs, Org Chart Models, Skill Management, Follower Networks, Influence Graphs</p>"},{"location":"learning-graph/concept-taxonomy/#8-knowledge-management-know","title":"8. Knowledge Management (KNOW)","text":"<p>Description: Knowledge representation systems including ontologies, SKOS, taxonomies, glossaries, personal knowledge graphs, and enterprise knowledge management.</p> <p>Typical Concepts: Ontologies, SKOS, Concept Dependency Graphs, Personal Knowledge Graphs, Enterprise Knowledge, Taxonomies</p>"},{"location":"learning-graph/concept-taxonomy/#9-modeling-patterns-pattern","title":"9. Modeling Patterns (PATTERN)","text":"<p>Description: Graph modeling patterns, anti-patterns, ETL processes, data loading strategies, and schema evolution approaches.</p> <p>Typical Concepts: Subgraphs, Time-Based Modeling, ETL Pipelines, Data Migration, Hyperedges, Schema Evolution</p>"},{"location":"learning-graph/concept-taxonomy/#10-financial-applications-fin","title":"10. Financial Applications (FIN)","text":"<p>Description: Financial transaction modeling, fraud detection, anti-money laundering, know-your-customer, and account network analysis.</p> <p>Typical Concepts: Financial Transactions, Fraud Detection, Anti-Money Laundering, Know Your Customer, Account Networks</p>"},{"location":"learning-graph/concept-taxonomy/#11-healthcare-applications-health","title":"11. Healthcare Applications (HEALTH)","text":"<p>Description: Healthcare-specific graph applications including provider-patient graphs, electronic health records, and clinical pathways.</p> <p>Typical Concepts: Healthcare Graphs, Provider-Patient Graphs, Electronic Health Records, Clinical Pathways</p>"},{"location":"learning-graph/concept-taxonomy/#12-supply-chain-it-supply","title":"12. Supply Chain &amp; IT (SUPPLY)","text":"<p>Description: Supply chain modeling, bill of materials, IT asset management, dependency graphs, network topology, and infrastructure applications.</p> <p>Typical Concepts: Supply Chain Modeling, Bill of Materials, IT Asset Management, Dependency Graphs, Network Topology</p>"},{"location":"learning-graph/concept-taxonomy/#13-advanced-topics-adv","title":"13. Advanced Topics (ADV)","text":"<p>Description: Advanced concepts including distributed graph databases, graph visualization, real-time analytics, and capstone projects.</p> <p>Typical Concepts: Distributed Graph Databases, Graph Partitioning, Graph Visualization, Real-Time Analytics, Capstone Project Design</p>"},{"location":"learning-graph/concept-taxonomy/#taxonomy-design-principles","title":"Taxonomy Design Principles","text":"<ol> <li>Pedagogical Organization: Categories follow a logical learning progression from foundations to applications</li> <li>Even Distribution: Each category targets 12-20 concepts to avoid over-representation</li> <li>Clear Boundaries: Each concept has a clear primary category</li> <li>Progressive Complexity: Foundation \u2192 Core \u2192 Advanced \u2192 Applications</li> <li>Industry Relevance: Application categories reflect real-world use cases</li> </ol>"},{"location":"learning-graph/course-description-assessment/","title":"Course Description Quality Assessment","text":"<p>Date: 2025-11-18 Skill Version: 0.02</p>"},{"location":"learning-graph/course-description-assessment/#quality-assessment-results","title":"Quality Assessment Results","text":"Element Points Available Points Awarded Assessment Title 5 5 \u2713 Clear, descriptive: \"Introduction to Graph Databases\" Target Audience 5 5 \u2713 Specific: \"Undergraduate (Junior/Senior) or Graduate Introductory Level\" Prerequisites 5 5 \u2713 Well-defined with 3 specific requirements Main Topics Covered 10 10 \u2713 Comprehensive 14-week outline with detailed topics Topics Excluded 5 5 \u2713 Clear \"Topics Not Covered\" section Learning Outcomes Header 5 5 \u2713 Clear \"Learning Objectives\" section with Bloom's Taxonomy organization Remember Level 10 10 \u2713 4 specific, actionable outcomes (define, list, identify, recall) Understand Level 10 10 \u2713 5 specific outcomes (explain, describe, summarize, compare) Apply Level 10 10 \u2713 5 specific outcomes (construct, write, load, implement, use) Analyze Level 10 10 \u2713 5 specific outcomes (differentiate, decompose, examine, analyze, map) Evaluate Level 10 10 \u2713 5 specific outcomes (justify, evaluate, critique, assess, defend) Create Level 10 10 \u2713 5 specific outcomes including capstone project (design, develop, create, build, propose) Descriptive Context 5 5 \u2713 Rich course overview with real-world applications and case studies"},{"location":"learning-graph/course-description-assessment/#overall-quality-score-95100","title":"Overall Quality Score: 95/100","text":""},{"location":"learning-graph/course-description-assessment/#strengths","title":"Strengths","text":"<ol> <li>Exceptional Bloom's Taxonomy coverage: Each cognitive level has multiple, well-articulated outcomes using proper action verbs</li> <li>Comprehensive topic coverage: 14-week outline with depth and breadth covering fundamentals to advanced applications</li> <li>Real-world focus: Multiple case studies and industry-specific models (healthcare, finance, supply chain, fraud detection)</li> <li>Clear scope boundaries: Explicitly states what's not covered</li> <li>Strong capstone component: Multi-week project demonstrating synthesis and application</li> <li>Well-structured prerequisites: Appropriate for the target audience</li> <li>Progressive difficulty: Builds from fundamentals (Week 1-3) through intermediate (Week 4-9) to advanced applications (Week 10-14)</li> </ol>"},{"location":"learning-graph/course-description-assessment/#minor-suggestions-for-improvement","title":"Minor Suggestions for Improvement","text":"<ul> <li>Week 9 appears twice in the outline (Graph Algorithms and Graph Modeling Patterns) - minor numbering issue</li> <li>Could benefit from explicit mention of assessment methods (exams, projects, etc.)</li> </ul>"},{"location":"learning-graph/course-description-assessment/#concept-generation-estimate","title":"Concept Generation Estimate","text":"<p>Based on this course description, I estimate 200+ high-quality concepts can be generated covering:</p> <ul> <li>Foundational concepts (15-20): NoSQL types, graph components, data models, RDBMS vs Graph</li> <li>Query languages and syntax (20-25): openCypher, GSQL, GQL, Gremlin patterns</li> <li>Performance and architecture (20-25): Index-free adjacency, benchmarking, scalability, traversal</li> <li>Modeling patterns (30-35): Social networks, knowledge graphs, time-based patterns, hyperedges</li> <li>Industry applications (40-50): Healthcare, finance, supply chain, fraud detection, BOM, KYC/AML, web storefronts</li> <li>Algorithms (20-25): BFS, DFS, PageRank, community detection, pathfinding, A*</li> <li>Advanced topics (30-40): Graph embeddings, GNNs, distributed systems, validation, rules</li> </ul>"},{"location":"learning-graph/course-description-assessment/#recommendation","title":"Recommendation","text":"<p>\u2705 PROCEED - This course description is excellent and well above the 70-point threshold for generating a high-quality learning graph. The comprehensive topic coverage, clear learning objectives across all Bloom's Taxonomy levels, and real-world applications provide an outstanding foundation for creating 200 meaningful, interconnected concepts.</p>"},{"location":"learning-graph/quality-metrics/","title":"Learning Graph Quality Metrics Report","text":""},{"location":"learning-graph/quality-metrics/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 200</li> <li>Foundational Concepts (no dependencies): 6</li> <li>Concepts with Dependencies: 194</li> <li>Average Dependencies per Concept: 1.65</li> </ul>"},{"location":"learning-graph/quality-metrics/#graph-structure-validation","title":"Graph Structure Validation","text":"<ul> <li>Valid DAG Structure: \u274c No</li> <li>Self-Dependencies: None detected \u2705</li> <li>Cycles Detected: 0</li> </ul>"},{"location":"learning-graph/quality-metrics/#foundational-concepts","title":"Foundational Concepts","text":"<p>These concepts have no prerequisites:</p> <ul> <li>2: World Models</li> <li>3: Knowledge Representation</li> <li>14: Schema Design</li> <li>15: Hash Maps</li> <li>16: Trees</li> <li>17: Arrays</li> </ul>"},{"location":"learning-graph/quality-metrics/#dependency-chain-analysis","title":"Dependency Chain Analysis","text":"<ul> <li>Maximum Dependency Chain Length: 18</li> </ul>"},{"location":"learning-graph/quality-metrics/#longest-learning-path","title":"Longest Learning Path:","text":"<ol> <li>World Models (ID: 2)</li> <li>Data Modeling (ID: 1)</li> <li>RDBMS (ID: 4)</li> <li>NoSQL Databases (ID: 7)</li> <li>Graph Databases (ID: 11)</li> <li>Nodes (ID: 22)</li> <li>Edges (ID: 23)</li> <li>Properties (ID: 24)</li> <li>Labeled Property Graph (ID: 21)</li> <li>Graph Query (ID: 30)</li> <li>OpenCypher (ID: 46)</li> <li>Cypher Syntax (ID: 50)</li> <li>Match Clause (ID: 51)</li> <li>Variable Length Paths (ID: 59)</li> <li>Shortest Path (ID: 60)</li> <li>Pathfinding (ID: 94)</li> <li>Impact Analysis (ID: 185)</li> <li>Root Cause Analysis (ID: 186)</li> </ol>"},{"location":"learning-graph/quality-metrics/#orphaned-nodes-analysis","title":"Orphaned Nodes Analysis","text":"<ul> <li>Total Orphaned Nodes: 100</li> </ul> <p>Concepts that are not prerequisites for any other concept:</p> <ul> <li>5: OLAP</li> <li>6: OLTP</li> <li>8: Key-Value Stores</li> <li>10: Wide-Column Stores</li> <li>13: Tradeoff Analysis</li> <li>20: Normalization</li> <li>35: Constant-Time Neighbor Access</li> <li>36: First-Class Relationships</li> <li>40: Metadata Representation</li> <li>41: Open World Model</li> <li>42: Closed World Model</li> <li>44: Document Validation</li> <li>48: Statistical Query Tuning</li> <li>52: Where Clause</li> <li>53: Return Clause</li> <li>55: Merge Statement</li> <li>56: Delete Statement</li> <li>57: Set Clause</li> <li>58: Graph Patterns</li> <li>61: All Paths</li> </ul> <p>...and 80 more</p>"},{"location":"learning-graph/quality-metrics/#connected-components","title":"Connected Components","text":"<ul> <li>Number of Connected Components: 1</li> </ul> <p>\u2705 All concepts are connected in a single graph.</p>"},{"location":"learning-graph/quality-metrics/#indegree-analysis","title":"Indegree Analysis","text":"<p>Top 10 concepts that are prerequisites for the most other concepts:</p> Rank Concept ID Concept Label Indegree 1 21 Labeled Property Graph 23 2 23 Edges 18 3 22 Nodes 12 4 38 Graph Data Model 11 5 24 Properties 10 6 29 Traversal 9 7 30 Graph Query 9 8 37 Edge Direction 9 9 50 Cypher Syntax 8 10 111 Social Networks 8"},{"location":"learning-graph/quality-metrics/#outdegree-distribution","title":"Outdegree Distribution","text":"Dependencies Number of Concepts 0 6 1 85 2 96 3 11 5 1 6 1"},{"location":"learning-graph/quality-metrics/#recommendations","title":"Recommendations","text":"<ul> <li>\u26a0\ufe0f Many orphaned nodes (100): Consider if these should be prerequisites for advanced concepts</li> <li>\u2139\ufe0f Long dependency chains (18): Ensure students can follow extended learning paths</li> </ul> <p>Report generated by learning-graph-reports/analyze_graph.py</p>"},{"location":"learning-graph/taxonomy-distribution/","title":"Taxonomy Distribution Report","text":""},{"location":"learning-graph/taxonomy-distribution/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 200</li> <li>Number of Taxonomies: 12</li> <li>Average Concepts per Taxonomy: 16.7</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#distribution-summary","title":"Distribution Summary","text":"Category TaxonomyID Count Percentage Status GRAPH GRAPH 42 21.0% \u2705 QUERY QUERY 26 13.0% \u2705 Foundation Concepts - Prerequisites FOUND 25 12.5% \u2705 ALGO ALGO 17 8.5% \u2705 SUPPLY SUPPLY 16 8.0% \u2705 PERF PERF 15 7.5% \u2705 SOCIAL SOCIAL 15 7.5% \u2705 PATTERN PATTERN 15 7.5% \u2705 KNOW KNOW 10 5.0% \u2705 Advanced Topics ADV 10 5.0% \u2705 FIN FIN 5 2.5% \u2139\ufe0f Under HEALTH HEALTH 4 2.0% \u2139\ufe0f Under"},{"location":"learning-graph/taxonomy-distribution/#visual-distribution","title":"Visual Distribution","text":"<pre><code>GRAPH  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  42 ( 21.0%)\nQUERY  \u2588\u2588\u2588\u2588\u2588\u2588  26 ( 13.0%)\nFOUND  \u2588\u2588\u2588\u2588\u2588\u2588  25 ( 12.5%)\nALGO   \u2588\u2588\u2588\u2588  17 (  8.5%)\nSUPPLY \u2588\u2588\u2588\u2588  16 (  8.0%)\nPERF   \u2588\u2588\u2588  15 (  7.5%)\nSOCIAL \u2588\u2588\u2588  15 (  7.5%)\nPATTERN \u2588\u2588\u2588  15 (  7.5%)\nKNOW   \u2588\u2588  10 (  5.0%)\nADV    \u2588\u2588  10 (  5.0%)\nFIN    \u2588   5 (  2.5%)\nHEALTH \u2588   4 (  2.0%)\n</code></pre>"},{"location":"learning-graph/taxonomy-distribution/#balance-analysis","title":"Balance Analysis","text":""},{"location":"learning-graph/taxonomy-distribution/#no-over-represented-categories","title":"\u2705 No Over-Represented Categories","text":"<p>All categories are under the 30% threshold. Good balance!</p>"},{"location":"learning-graph/taxonomy-distribution/#i-under-represented-categories-3","title":"\u2139\ufe0f Under-Represented Categories (&lt;3%)","text":"<ul> <li>FIN (FIN): 5 concepts (2.5%)</li> <li>Note: Small categories are acceptable for specialized topics</li> <li>HEALTH (HEALTH): 4 concepts (2.0%)</li> <li>Note: Small categories are acceptable for specialized topics</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#category-details","title":"Category Details","text":""},{"location":"learning-graph/taxonomy-distribution/#graph-graph","title":"GRAPH (GRAPH)","text":"<p>Count: 42 concepts (21.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Labeled Property Graph</li> </ol> </li> <li> <ol> <li>Nodes</li> </ol> </li> <li> <ol> <li>Edges</li> </ol> </li> <li> <ol> <li>Properties</li> </ol> </li> <li> <ol> <li>Labels</li> </ol> </li> <li> <ol> <li>Schema-Optional Modeling</li> </ol> </li> <li> <ol> <li>Schema-Enforced Modeling</li> </ol> </li> <li> <ol> <li>Index-Free Adjacency</li> </ol> </li> <li> <ol> <li>Traversal</li> </ol> </li> <li> <ol> <li>Graph Query</li> </ol> </li> <li> <ol> <li>Pattern Matching</li> </ol> </li> <li> <ol> <li>Multi-Hop Queries</li> </ol> </li> <li> <ol> <li>Aggregation</li> </ol> </li> <li> <ol> <li>Path Patterns</li> </ol> </li> <li> <ol> <li>Constant-Time Neighbor Access</li> </ol> </li> <li>...and 27 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#query-query","title":"QUERY (QUERY)","text":"<p>Count: 26 concepts (13.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>OpenCypher</li> </ol> </li> <li> <ol> <li>GSQL</li> </ol> </li> <li> <ol> <li>Statistical Query Tuning</li> </ol> </li> <li> <ol> <li>GQL</li> </ol> </li> <li> <ol> <li>Cypher Syntax</li> </ol> </li> <li> <ol> <li>Match Clause</li> </ol> </li> <li> <ol> <li>Where Clause</li> </ol> </li> <li> <ol> <li>Return Clause</li> </ol> </li> <li> <ol> <li>Create Statement</li> </ol> </li> <li> <ol> <li>Merge Statement</li> </ol> </li> <li> <ol> <li>Delete Statement</li> </ol> </li> <li> <ol> <li>Set Clause</li> </ol> </li> <li> <ol> <li>Graph Patterns</li> </ol> </li> <li> <ol> <li>Variable Length Paths</li> </ol> </li> <li> <ol> <li>Shortest Path</li> </ol> </li> <li>...and 11 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#foundation-concepts-prerequisites-found","title":"Foundation Concepts - Prerequisites (FOUND)","text":"<p>Count: 25 concepts (12.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Data Modeling</li> </ol> </li> <li> <ol> <li>World Models</li> </ol> </li> <li> <ol> <li>Knowledge Representation</li> </ol> </li> <li> <ol> <li>RDBMS</li> </ol> </li> <li> <ol> <li>OLAP</li> </ol> </li> <li> <ol> <li>OLTP</li> </ol> </li> <li> <ol> <li>NoSQL Databases</li> </ol> </li> <li> <ol> <li>Key-Value Stores</li> </ol> </li> <li> <ol> <li>Document Databases</li> </ol> </li> <li> <ol> <li>Wide-Column Stores</li> </ol> </li> <li> <ol> <li>Graph Databases</li> </ol> </li> <li> <ol> <li>CAP Theorem</li> </ol> </li> <li> <ol> <li>Tradeoff Analysis</li> </ol> </li> <li> <ol> <li>Schema Design</li> </ol> </li> <li> <ol> <li>Hash Maps</li> </ol> </li> <li>...and 10 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#algo-algo","title":"ALGO (ALGO)","text":"<p>Count: 17 concepts (8.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Breadth-First Search</li> </ol> </li> <li> <ol> <li>Depth-First Search</li> </ol> </li> <li> <ol> <li>A-Star Algorithm</li> </ol> </li> <li> <ol> <li>Pathfinding</li> </ol> </li> <li> <ol> <li>Traveling Salesman Problem</li> </ol> </li> <li> <ol> <li>PageRank</li> </ol> </li> <li> <ol> <li>Community Detection</li> </ol> </li> <li> <ol> <li>Centrality Measures</li> </ol> </li> <li> <ol> <li>Betweenness Centrality</li> </ol> </li> <li> <ol> <li>Closeness Centrality</li> </ol> </li> <li> <ol> <li>Graph Embeddings</li> </ol> </li> <li> <ol> <li>Graph Neural Networks</li> </ol> </li> <li> <ol> <li>Link Prediction</li> </ol> </li> <li> <ol> <li>Graph Clustering</li> </ol> </li> <li> <ol> <li>Connected Components</li> </ol> </li> <li>...and 2 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#supply-supply","title":"SUPPLY (SUPPLY)","text":"<p>Count: 16 concepts (8.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Web Storefront Models</li> </ol> </li> <li> <ol> <li>Product Catalogs</li> </ol> </li> <li> <ol> <li>Recommendation Engines</li> </ol> </li> <li> <ol> <li>Bill of Materials</li> </ol> </li> <li> <ol> <li>Complex Parts</li> </ol> </li> <li> <ol> <li>Supply Chain Modeling</li> </ol> </li> <li> <ol> <li>IT Asset Management</li> </ol> </li> <li> <ol> <li>Dependency Graphs</li> </ol> </li> <li> <ol> <li>Network Topology</li> </ol> </li> <li> <ol> <li>Configuration Management</li> </ol> </li> <li> <ol> <li>Impact Analysis</li> </ol> </li> <li> <ol> <li>Root Cause Analysis</li> </ol> </li> <li> <ol> <li>Regulatory Compliance</li> </ol> </li> <li> <ol> <li>Data Lineage</li> </ol> </li> <li> <ol> <li>Master Data Management</li> </ol> </li> <li>...and 1 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#perf-perf","title":"PERF (PERF)","text":"<p>Count: 15 concepts (7.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Hop Count</li> </ol> </li> <li> <ol> <li>Indegree</li> </ol> </li> <li> <ol> <li>Outdegree</li> </ol> </li> <li> <ol> <li>Graph Indexes</li> </ol> </li> <li> <ol> <li>Vector Indexes</li> </ol> </li> <li> <ol> <li>Full-Text Search</li> </ol> </li> <li> <ol> <li>Composite Indexes</li> </ol> </li> <li> <ol> <li>Graph Metrics</li> </ol> </li> <li> <ol> <li>Performance Benchmarking</li> </ol> </li> <li> <ol> <li>Synthetic Benchmarks</li> </ol> </li> <li> <ol> <li>LDBC SNB Benchmark</li> </ol> </li> <li> <ol> <li>Graph 500</li> </ol> </li> <li> <ol> <li>Query Cost Analysis</li> </ol> </li> <li> <ol> <li>Join Operations</li> </ol> </li> <li> <ol> <li>Scalability</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#social-social","title":"SOCIAL (SOCIAL)","text":"<p>Count: 15 concepts (7.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Social Networks</li> </ol> </li> <li> <ol> <li>Friend Graphs</li> </ol> </li> <li> <ol> <li>Influence Graphs</li> </ol> </li> <li> <ol> <li>Follower Networks</li> </ol> </li> <li> <ol> <li>Activity Streams</li> </ol> </li> <li> <ol> <li>User Profiles</li> </ol> </li> <li> <ol> <li>Relationship Types</li> </ol> </li> <li> <ol> <li>Sentiment Analysis</li> </ol> </li> <li> <ol> <li>Natural Language Processing</li> </ol> </li> <li> <ol> <li>Fake Account Detection</li> </ol> </li> <li> <ol> <li>Human Resources Modeling</li> </ol> </li> <li> <ol> <li>Org Chart Models</li> </ol> </li> <li> <ol> <li>Skill Management</li> </ol> </li> <li> <ol> <li>Task Assignment</li> </ol> </li> <li> <ol> <li>Backlog Management</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#pattern-pattern","title":"PATTERN (PATTERN)","text":"<p>Count: 15 concepts (7.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Subgraphs</li> </ol> </li> <li> <ol> <li>Anti-Patterns</li> </ol> </li> <li> <ol> <li>Time-Based Modeling</li> </ol> </li> <li> <ol> <li>IoT Event Modeling</li> </ol> </li> <li> <ol> <li>Bitemporal Models</li> </ol> </li> <li> <ol> <li>Graph Quality Metrics</li> </ol> </li> <li> <ol> <li>Model Validation</li> </ol> </li> <li> <ol> <li>Schema Evolution</li> </ol> </li> <li> <ol> <li>Data Migration</li> </ol> </li> <li> <ol> <li>ETL Pipelines</li> </ol> </li> <li> <ol> <li>CSV Import</li> </ol> </li> <li> <ol> <li>JSON Import</li> </ol> </li> <li> <ol> <li>Data Loading</li> </ol> </li> <li> <ol> <li>Bulk Loading</li> </ol> </li> <li> <ol> <li>Incremental Loading</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#know-know","title":"KNOW (KNOW)","text":"<p>Count: 10 concepts (5.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Concept Dependency Graphs</li> </ol> </li> <li> <ol> <li>Curriculum Graphs</li> </ol> </li> <li> <ol> <li>Ontologies</li> </ol> </li> <li> <ol> <li>SKOS</li> </ol> </li> <li> <ol> <li>Acronym Lists</li> </ol> </li> <li> <ol> <li>Glossaries</li> </ol> </li> <li> <ol> <li>Controlled Vocabularies</li> </ol> </li> <li> <ol> <li>Taxonomies</li> </ol> </li> <li> <ol> <li>Note-Taking Systems</li> </ol> </li> <li> <ol> <li>Action Item Extraction</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#advanced-topics-adv","title":"Advanced Topics (ADV)","text":"<p>Count: 10 concepts (5.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Distributed Graph Databases</li> </ol> </li> <li> <ol> <li>Graph Partitioning</li> </ol> </li> <li> <ol> <li>Sharding Strategies</li> </ol> </li> <li> <ol> <li>Replication</li> </ol> </li> <li> <ol> <li>Consistency Models</li> </ol> </li> <li> <ol> <li>Graph Visualization</li> </ol> </li> <li> <ol> <li>Interactive Queries</li> </ol> </li> <li> <ol> <li>Real-Time Analytics</li> </ol> </li> <li> <ol> <li>Batch Processing</li> </ol> </li> <li> <ol> <li>Capstone Project Design</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#fin-fin","title":"FIN (FIN)","text":"<p>Count: 5 concepts (2.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Financial Transactions</li> </ol> </li> <li> <ol> <li>Fraud Detection</li> </ol> </li> <li> <ol> <li>Anti-Money Laundering</li> </ol> </li> <li> <ol> <li>Know Your Customer</li> </ol> </li> <li> <ol> <li>Account Networks</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#health-health","title":"HEALTH (HEALTH)","text":"<p>Count: 4 concepts (2.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Healthcare Graphs</li> </ol> </li> <li> <ol> <li>Provider-Patient Graphs</li> </ol> </li> <li> <ol> <li>Electronic Health Records</li> </ol> </li> <li> <ol> <li>Clinical Pathways</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#recommendations","title":"Recommendations","text":"<ul> <li>\u2705 Good balance: Categories are reasonably distributed (spread: 19.0%)</li> <li>\u2705 MISC category minimal: Good categorization specificity</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#educational-use-recommendations","title":"Educational Use Recommendations","text":"<ul> <li>Use taxonomy categories for color-coding in graph visualizations</li> <li>Design curriculum modules based on taxonomy groupings</li> <li>Create filtered views for focused learning paths</li> <li>Use categories for assessment organization</li> <li>Enable navigation by topic area in interactive tools</li> </ul> <p>Report generated by learning-graph-reports/taxonomy_distribution.py</p>"},{"location":"notes/knowledge-types/","title":"Knowledge Types","text":""},{"location":"notes/knowledge-types/#tacit-knowledge","title":"Tacit Knowledge","text":"<p>Definition: Tacit knowledge is the internal, experiential, intuitive, and often subconscious knowledge that people gain through direct experience, observation, and practice. It is difficult to write down, formalize, or express explicitly. Tacit knowledge often includes insights, mental models, pattern recognition, motor skills, and \"know-how\" gained from doing rather than reading.</p> <p>Tacit knowledge is personal, context-dependent, and hard to transfer except through mentoring, apprenticeship, demonstration, or immersion.</p> <p>Characteristics:</p> <ul> <li> <p>Hard to articulate</p> </li> <li> <p>Learned experientially</p> </li> <li> <p>Stored in mental models</p> </li> <li> <p>Influenced by intuition</p> </li> <li> <p>Shared through demonstration (\"watch me do it\")</p> </li> <li> <p>Cannot be fully captured in text or rules</p> </li> </ul> <p>Examples:</p> <ul> <li> <p>Knowing how to ride a bicycle</p> </li> <li> <p>An engineer's intuition about where a system will fail</p> </li> <li> <p>A doctor recognizing subtle diagnostic cues in a patient</p> </li> <li> <p>A teacher sensing when a student is confused</p> </li> <li> <p>A chef adjusting seasoning \"by feel\"</p> </li> </ul>"},{"location":"notes/knowledge-types/#codifiable-codified-knowledge","title":"Codifiable (Codified) Knowledge","text":"<p>Definition: Codifiable knowledge (also called explicit knowledge) is knowledge that can be written down, structured, formalized, stored, indexed, and transmitted using language, symbols, diagrams, or algorithms. It can be represented digitally or in print and is suitable for search, retrieval, and encoding into knowledge graphs.</p> <p>Codifiable knowledge is systematic, transferable, and independent of the person who originally discovered it.</p> <p>Characteristics:</p> <ul> <li> <p>Easy to articulate</p> </li> <li> <p>Can be stored in documents, code, formulas, or graphs</p> </li> <li> <p>Transferable at scale</p> </li> <li> <p>Can be indexed and searched</p> </li> <li> <p>Basis of textbooks, training materials, and rules engines</p> </li> </ul> <p>Examples:</p> <ul> <li> <p>Mathematical formulas</p> </li> <li> <p>SQL or GSQL queries</p> </li> <li> <p>Step-by-step procedural instructions</p> </li> <li> <p>Engineering design rules</p> </li> <li> <p>Scientific facts</p> </li> <li> <p>Curriculum standards</p> </li> <li> <p>Concept dependency graphs</p> </li> <li> <p>Documentation and APIs</p> </li> </ul>"},{"location":"notes/knowledge-types/#relationship-to-learning-graphs-and-ai-enhanced-textbooks","title":"Relationship to Learning Graphs and AI-Enhanced Textbooks","text":"<p>These two knowledge types align beautifully with your Learning Graph framework:</p>"},{"location":"notes/knowledge-types/#tacit-knowledge_1","title":"Tacit Knowledge","text":"<ul> <li> <p>Appears as cognitive strategies</p> </li> <li> <p>Represented indirectly in skills, heuristics, hints, or coaching tips</p> </li> <li> <p>Transferred through MicroSims, demonstrations, feedback loops, or expert modeling</p> </li> </ul>"},{"location":"notes/knowledge-types/#codifiable-knowledge","title":"Codifiable Knowledge","text":"<ul> <li> <p>Appears directly as concept nodes, definitions, procedures, and rules</p> </li> <li> <p>Easily structured into LPG databases, curriculum graphs, and AI-searchable knowledge stores</p> </li> <li> <p>Forms the core of intelligent textbooks</p> </li> </ul>"},{"location":"notes/knowledge-types/#why-this-matters-for-your-work","title":"Why This Matters for Your Work","text":"<p>Your textbooks and MicroSims can explicitly separate:</p> <ul> <li> <p>Codifiable knowledge: the teachable content (facts, models, diagrams, rules).</p> </li> <li> <p>Tacit knowledge: the experiential \"feel\" of mastery, supported through</p> <ul> <li> <p>simulations</p> </li> <li> <p>decision-making scenarios</p> </li> <li> <p>guided practice</p> </li> <li> <p>reflective prompts</p> </li> <li> <p>worked examples</p> </li> </ul> </li> </ul> <p>This division supports your design principle that graph-encoded knowledge handles the explicit layer, while simulations and interactive examples provide the bridge to tacit mastery.</p>"},{"location":"prompts/chapter-06-content/","title":"Chapter 06 content","text":"<p>run the chapter-content-generator on chapter 6 at   @docs/chapters/05-social-network-modeling  Assume a senior-high school student reading   level.   Make the tone lighthearted and engaging.   Tell the reader that although they may not think they have a social network problem, we find   aspect of social networks everywhere.  Products have reviews, reviews have ratings, reviewers have varying credibility based on reputation, even your code checkins on GitHub have can have reputation based on the number of times a checkin caused new bugs.  The things you learn in this chapter apply any time people with reputations write comments on any system.</p>"},{"location":"prompts/the-neighborhood-walk/","title":"The neighborhood walk","text":"<p>Create a narrative for a fun 10-panel graphic novel that explains the graph database concept of index-free adjacency to a wide audience in a fun way.  As a basis for this story consider the blog called \"The Neighborhood Walk\".  https://dmccreary.medium.com/how-to-explain-index-free-adjacency-to-your-manager-1a8e68ec664a </p> <p>In this story, we compare two worlds: table world and graph world.</p> <p>In table world, you are not allowed to walk right over to your cute new neighbor's house to deliver a hot apple pie. There is an mean-looking police officer with a STOP sign when you try this.  The tells you that in table world you need to take a two hour walk downtown to the \"Central Search Index Tower.  This is a huge tower with a brutal architecture and with a very long line of people outside all looking very sad, board or angry.  You wait in line for four hours.  When you get to the front of the line you tell them the address you want to go to.  They look up your address in their Central Search Index Mainframe with a Green Screen and they give you the coordinates to your destination.  In two more hours you finally arrive.  But the pie is cold.  Your neighbor is grateful but wonders why it took so long to get there.  She tells you: \"Haven't you heard about graph databases?\".</p> <p>By contrast, in graph land, you just \"Point\" to your neighbor's house and hop right over!  30 seconds MAX!  The pie is hot and everyone is happy!  Just follow the pointer in memory!  Poof!  You are there.  It is just like the instant transporters in Star Trek!</p> <p>This is the metaphor of RDBMS \"lookup\" (very slow) and the idea of just following a memory pointer to resolve a relationship traversal. Pointer following is one of the fastest operations in computer science.  And pointer traversal does NOT slow down the more data you have.  As long as you have enough RAM, relationship traversal is SUPER FAST!!</p> <p>The drawing descriptions must use the following format:</p>"},{"location":"prompts/the-neighborhood-walk/#panel-n-title","title":"Panel N: Title","text":"Title  Panel N  Generate a wide-landscape drawing in the style of a comic book.  The image should be consistent in style with other images in this session.  [Detailed Description of the content of the panel here]"},{"location":"prompts/the-neighborhood-walk/#result","title":"Result","text":"<p>Note</p> <p>Start the chat with: </p> <p>You will now be asked to generate a series of images for a fun graphic novel. All images must be in a wide-landscape format and be rendered as a graphic novel comic book.</p>"},{"location":"prompts/the-neighborhood-walk/#the-hot-apple-pie-adventure-a-tale-of-two-worlds","title":"The Hot Apple Pie Adventure: A Tale of Two Worlds","text":""},{"location":"prompts/the-neighborhood-walk/#panel-1-a-new-neighbor-arrives","title":"Panel 1: A New Neighbor Arrives","text":"A New Neighbor Arrives Panel 1 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  Split scene showing two adjacent houses in a cheerful suburban neighborhood. On the left, our protagonist (a friendly person in casual clothes with an apron) waves from their front porch. On the right, a cute new neighbor is unpacking boxes on their porch. There's a white picket fence between the properties. Thought bubble above the protagonist shows a steaming apple pie with hearts around it. The houses are close - maybe 30 feet apart. A sign in the neighbor's yard reads \"Welcome to Table World!\" The sky is bright and sunny, with a few fluffy clouds."},{"location":"prompts/the-neighborhood-walk/#panel-2-the-wall-of-rules","title":"Panel 2: The Wall of Rules","text":"The Wall of Rules Panel 2 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  Our protagonist, now holding a beautiful steaming apple pie with heat waves rising from it, approaches the property line between the houses. Suddenly, a stern-looking police officer in an old-fashioned uniform appears, holding a large red STOP sign. The officer has an exaggerated serious expression with furrowed brows. Behind the officer is a large official-looking sign that reads \"TABLE WORLD REGULATIONS: All visits require Central Index Lookup. Direct access PROHIBITED!\" The protagonist looks confused and disappointed. The neighbor can be seen in the background through their window, looking friendly. A small clock in the corner shows it's 2:00 PM."},{"location":"prompts/the-neighborhood-walk/#panel-3-the-bureaucratic-detour","title":"Panel 3: The Bureaucratic Detour","text":"The Bureaucratic Detour Panel 3 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  The police officer is pointing authoritatively toward a distant city skyline visible on the horizon. In the far distance, there's an ominous, brutalist concrete tower that rises above all other buildings - dark gray and imposing with small rectangular windows. The officer has a speech bubble saying \"You must visit the CENTRAL SEARCH INDEX TOWER downtown!\" The protagonist looks dismayed, still holding the pie (steam still rising but slightly less). A helpful road sign in the foreground shows \"Central Search Index Tower: 2 Hours\" with an arrow pointing away. The cheerful neighborhood is in the foreground, while the dystopian city looms in the distance."},{"location":"prompts/the-neighborhood-walk/#panel-4-the-long-journey","title":"Panel 4: The Long Journey","text":"The Long Journey Panel 4 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  A montage panel showing the protagonist's journey downtown. The panel is divided into three vignettes: Top left shows them walking past suburban houses that gradually give way to buildings. Top right shows them on a crowded bus or subway looking tired. Bottom shows them finally arriving at the base of the massive Central Search Index Tower - a brutalist concrete monstrosity with angular architecture, small windows, and an unwelcoming appearance. The tower looms overhead menacingly. The protagonist is sweating, looking exhausted, and the pie now has significantly less steam rising from it. A small clock shows it's now 4:00 PM. Other tired people are visible in the background."},{"location":"prompts/the-neighborhood-walk/#panel-5-the-endless-queue","title":"Panel 5: The Endless Queue","text":"The Endless Queue Panel 5 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  A wide shot showing an incredibly long line of people snaking around the outside of the Central Search Index Tower. The line has velvet ropes like at a movie theater. People in line look bored, angry, frustrated, or sleeping while standing. Some are checking watches, others have their heads in their hands. Our protagonist is near the back of the line, looking exhausted and holding the pie (barely any steam now). Speech bubbles show people complaining: \"I've been here for 3 hours!\", \"This is ridiculous!\", \"Why is this so slow?\". A sign reads \"Estimated Wait Time: 4 Hours\". The clock shows 4:30 PM. Dark clouds are starting to gather in the sky."},{"location":"prompts/the-neighborhood-walk/#panel-6-the-green-screen-of-despair","title":"Panel 6: The Green Screen of Despair","text":"The Green Screen of Despair Panel 6 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  Interior of the Central Search Index Tower. Our protagonist has finally reached the front desk counter. Behind thick plexiglass sits a bored-looking clerk in front of a massive, ancient computer with a glowing green monochrome screen displaying rows of data (like an old mainframe terminal). The keyboard is huge and clunky. Filing cabinets and stacks of papers fill the background. The protagonist is giving their neighbor's address. The clerk is slowly typing with one finger. Speech bubble from clerk: \"Searching central index... this may take a moment... scanning 10 billion records...\" The pie is now visibly cold with NO steam. Clock shows 8:00 PM. Fluorescent lights buzz overhead."},{"location":"prompts/the-neighborhood-walk/#panel-7-arrival-with-cold-pie","title":"Panel 7: Arrival with Cold Pie","text":"Arrival with Cold Pie Panel 7 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  It's now nighttime (stars and moon visible). The protagonist finally arrives at the neighbor's door, looking completely exhausted and disheveled. They're holding the pie which now has a visible frost layer on top (small icicles) - it's gone completely cold. The neighbor opens the door, looking concerned and sympathetic. Speech bubble from neighbor: \"Oh my! Is that... frozen? You poor thing! How long did this take you?\" Speech bubble from protagonist: \"Eight hours... had to go to the Central Index Tower...\" Clock in the corner shows 10:00 PM. The neighbor has a sad but kind expression. A single tear rolls down the protagonist's cheek."},{"location":"prompts/the-neighborhood-walk/#panel-8-the-revelation","title":"Panel 8: The Revelation","text":"The Revelation Panel 8 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  Inside the neighbor's cozy kitchen. They're sitting at a table with the thawed (but still cold) pie between them. The neighbor is leaning forward excitedly, pointing to a colorful poster on the wall that shows \"GRAPH WORLD\" with happy people hopping between connected nodes/houses. The poster looks vibrant and inviting - the opposite of Table World. Speech bubble from neighbor with bright, exciting text: \"Haven't you heard about GRAPH DATABASES? In GRAPH WORLD, you can just POINT and HOP directly to any neighbor! No index lookups needed!\" The protagonist's eyes are wide with amazement and curiosity. Little sparkle effects around the neighbor's head show this is an exciting revelation. A glowing diagram on the poster shows connected nodes with arrows and the word \"POINTERS!\""},{"location":"prompts/the-neighborhood-walk/#panel-9-instant-transport","title":"Panel 9: Instant Transport!","text":"Instant Transport! Panel 9 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  Next day, bright sunny morning in GRAPH WORLD - the scene looks more colorful and vibrant than Table World. The protagonist stands at their front door with a NEW steaming hot apple pie (lots of heat waves). They simply POINT toward the neighbor's house. A glowing pointer arrow/ray shoots from their hand directly to the neighbor's house. Comic book style \"WHOOSH!\" and \"ZOOM!\" sound effects. The protagonist is instantly transported (Star Trek transporter-style sparkles) in a split second, appearing at the neighbor's door. A banner at the top reads \"GRAPH WORLD: Index-Free Adjacency!\" A stopwatch graphic shows \"0:00:30 - 30 SECONDS!\" No police officer. No barriers. Pure freedom! The neighbor is already at the door smiling and welcoming them."},{"location":"prompts/the-neighborhood-walk/#panel-10-the-happy-ending","title":"Panel 10: The Happy Ending","text":"The Happy Ending Panel 10 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  Split comparison panel. TOP HALF labeled \"TABLE WORLD: 8 HOURS\" shows a gloomy, grayed-out scene: the Central Search Index Tower in the background, the long line, exhausted protagonist, cold frozen pie, sad faces. Annotations point to elements: \"Index Lookup Required\", \"Scales Poorly\", \"Slow &amp; Bureaucratic\". BOTTOM HALF labeled \"GRAPH WORLD: 30 SECONDS\" shows bright, cheerful colors: both neighbors happily eating HOT apple pie together on the porch, smiling and laughing. Direct pointer arrow shown between houses. Annotations read: \"Direct Pointer Following\", \"Constant Time O(1)\", \"SUPER FAST!\". At the bottom, a conclusion banner states: \"In Graph Databases, relationships are just memory pointers - one of the fastest operations in computer science! More data \u2260 Slower queries!\" Both neighbors give thumbs up in Graph World."},{"location":"prompts/the-neighborhood-walk/#the-technical-takeaway","title":"The Technical Takeaway","text":"<p>This graphic novel illustrates the fundamental difference between traditional relational databases (RDBMS) and graph databases:</p> <p>Table World (RDBMS)</p> <ul> <li>Requires index lookups to find related data</li> <li>Must query central indexes for each relationship</li> <li>Performance degrades as data grows</li> <li>Multiple lookups required for connected data</li> </ul> <p>Graph World (Index-Free Adjacency)</p> <ul> <li>Relationships stored as direct memory pointers</li> <li>No index lookup required for traversals</li> <li>Constant-time relationship traversal O(1)</li> <li>Performance doesn't degrade with data size (given sufficient RAM)</li> </ul> <p>Index-free adjacency makes graph databases extraordinarily fast for relationship-heavy queries - like delivering hot apple pies to your neighbors! \ud83e\udd67</p>"},{"location":"sims/graph-viewer/","title":"Learning Graph Viewer","text":"<p>Run the Learning Graph Viewer</p> <p>This viewer reads a learning graph data from ../../learning-graph/learning-graph.json:</p> <ol> <li>Search Functionality - Quick node lookup with autocomplete</li> <li>Taxonomy Legend Controls - Filter nodes by category/taxonomy</li> </ol>"},{"location":"sims/graph-viewer/#features","title":"Features","text":""},{"location":"sims/graph-viewer/#search","title":"Search","text":"<ul> <li>Type-ahead search for node names</li> <li>Displays matching results in a dropdown</li> <li>Shows node group/category in results</li> <li>Clicking a result focuses and highlights the node on the graph</li> <li>Only searches visible nodes (respects taxonomy filters)</li> </ul>"},{"location":"sims/graph-viewer/#taxonomy-legend-with-checkboxes","title":"Taxonomy Legend with Checkboxes","text":"<ul> <li>Sidebar legend with all node categories</li> <li>Toggle visibility of entire node groups</li> <li>Color-coded categories matching the graph</li> <li>\"Check All\" and \"Uncheck All\" buttons for bulk operations</li> <li>Collapsible sidebar to maximize graph viewing area</li> </ul>"},{"location":"sims/graph-viewer/#graph-statistics","title":"Graph Statistics","text":"<p>Real-time statistics that update as you filter: - Nodes: Count of visible nodes - Edges: Count of visible edges (both endpoints must be visible) - Orphans: Nodes with no connections (this is an indication that the learning graph needs editing)</p>"},{"location":"sims/graph-viewer/#sample-graph-demo","title":"Sample Graph Demo","text":"<p>The demo includes a Graph Theory learning graph with 10 taxonomy categories:</p> <ul> <li>Foundation (Red) - Core concepts in red boxes that should be pinned to the left</li> <li>Types (Orange) - Graph types</li> <li>Representations (Gold) - Data structures</li> <li>Algorithms (Green) - Basic algorithms</li> <li>Paths (Blue) - Shortest path algorithms</li> <li>Flow (Indigo) - Network flow algorithms</li> <li>Advanced (Violet) - Advanced topics</li> <li>Metrics (Gray) - Centrality measures</li> <li>Spectral (Brown) - Spectral theory</li> <li>ML &amp; Networks (Teal) - Machine learning</li> </ul>"},{"location":"sims/graph-viewer/#usage-tips","title":"Usage Tips","text":"<ol> <li>Hide a category - Uncheck a category in the sidebar to hide all nodes in that group</li> <li>Search within visible nodes - Use search to quickly find specific concepts among visible nodes</li> <li>Focus on a topic - Uncheck all categories, then check only the ones you want to study</li> <li>Collapse sidebar - Click the menu button (\u2630) to hide the sidebar and expand the graph view</li> <li>Find orphans - Check the statistics to see if any nodes lack connections</li> </ol>"},{"location":"sims/graph-viewer/#implementation-notes","title":"Implementation Notes","text":"<p>This viewer follows the standard vis.js architectural patterns:</p> <ul> <li>Uses <code>vis.DataSet</code> for nodes and edges</li> <li>Implements node <code>hidden</code> property for filtering</li> <li>Combines separate search and legend features</li> <li>Updates statistics dynamically based on visibility</li> <li>Maintains consistent styling across features</li> </ul>"},{"location":"sims/graph-viewer/#use-cases","title":"Use Cases","text":"<ul> <li>Course planning - Filter by topic area to design lesson sequences</li> <li>Concept exploration - Search for specific concepts and see their dependencies</li> <li>Gap analysis - Use orphan count to identify disconnected concepts</li> <li>Progressive learning - Start with foundation concepts, gradually enable advanced topics</li> </ul>"},{"location":"sims/minimum-spanning-tree/","title":"Minimum Spanning Tree Algorithm Visualizer","text":"<p>Run the Minimum Spanning Tree MicroSim Fullscreen</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/intro-to-graph/sims/minimum-spanning-tree/main.html\"\n        height=\"652px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/minimum-spanning-tree/#description","title":"Description","text":"<p>The Minimum Spanning Tree (MST) MicroSim provides an interactive visualization of two fundamental graph algorithms: Kruskal's algorithm and Prim's algorithm. Both algorithms solve the same problem\u2014finding the minimum cost network that connects all nodes without creating cycles\u2014but use different strategic approaches.</p>"},{"location":"sims/minimum-spanning-tree/#what-is-a-minimum-spanning-tree","title":"What is a Minimum Spanning Tree?","text":"<p>A minimum spanning tree is a subset of edges in a weighted graph that: - Connects all nodes (vertices) together - Contains no cycles (is a tree structure) - Minimizes the total edge weight (has minimum total cost)</p> <p>MSTs solve critical real-world problems in network design, infrastructure planning, and optimization.</p>"},{"location":"sims/minimum-spanning-tree/#algorithm-comparison","title":"Algorithm Comparison","text":"<p>Kruskal's Algorithm (Edge-Based Greedy Approach): - Sorts all edges by weight from smallest to largest - Examines edges in order, adding each edge if it doesn't create a cycle - Uses a union-find data structure to detect cycles efficiently - Strategy: \"Consider the cheapest available connection\"</p> <p>Prim's Algorithm (Node-Based Greedy Approach): - Starts from an arbitrary node and grows the tree one node at a time - Always adds the minimum-weight edge connecting a visited node to an unvisited node - Uses a priority queue to track candidate edges - Strategy: \"Expand the tree with the cheapest connection to a new location\"</p>"},{"location":"sims/minimum-spanning-tree/#visual-feedback","title":"Visual Feedback","text":"<p>The simulation uses color coding to show algorithm progress: - Gray edges: Available for consideration - Yellow edge: Currently being evaluated - Gold edges: Accepted into the MST (thick lines) - Light gray edges: Rejected (would create cycle) - Green nodes (Prim's only): Visited nodes included in the MST</p> <p>Edge weights are displayed at the midpoint of each edge in white circles for easy reference.</p>"},{"location":"sims/minimum-spanning-tree/#interactive-controls","title":"Interactive Controls","text":"<p>Algorithm Selection: - Dropdown menu to switch between Kruskal's and Prim's algorithms - Changing algorithms resets the simulation with a new random graph</p> <p>Execution Controls: - Step Forward: Execute one algorithm step to see detailed decision-making - Auto Run: Animate the complete algorithm execution - Reset: Generate a new random graph and restart</p> <p>Animation Speed: - Slider controls the delay between steps (100-2000 milliseconds) - Slower speeds help understand each decision; faster speeds show overall behavior</p> <p>Status Display: - Current action description explains each step - Edge counter shows progress (current/total edges needed) - Running total displays cumulative MST weight</p>"},{"location":"sims/minimum-spanning-tree/#educational-applications","title":"Educational Applications","text":""},{"location":"sims/minimum-spanning-tree/#learning-objectives","title":"Learning Objectives","text":"<p>Students using this MicroSim will: 1. Understand the minimum spanning tree problem and its real-world applications 2. Compare two different algorithmic approaches to the same problem 3. Analyze how greedy algorithms make locally optimal choices 4. Observe cycle detection mechanisms in action 5. Apply graph theory concepts to network optimization</p>"},{"location":"sims/minimum-spanning-tree/#classroom-activities","title":"Classroom Activities","text":"<p>Activity 1: Algorithm Comparison - Run both algorithms on the same graph (use Reset to generate new graphs) - Verify that both produce MSTs with the same total weight - Observe how the order of edge selection differs between algorithms - Discuss: Why do different approaches yield the same optimal result?</p> <p>Activity 2: Manual Prediction - Pause after each step and predict which edge will be selected next - For Kruskal's: Find the minimum weight edge that won't create a cycle - For Prim's: Find the minimum weight edge connecting to an unvisited node - Verify predictions by stepping forward</p> <p>Activity 3: Real-World Applications - Given the graph represents cities connected by roads, what does the MST represent?   - Answer: Minimum road network connecting all cities - Brainstorm other scenarios: utility networks, computer networks, transportation routes - Calculate potential cost savings: Compare MST weight to total graph weight</p> <p>Activity 4: Cycle Detection - Watch carefully when Kruskal's rejects edges - Identify which existing MST edges would create a cycle with the rejected edge - Understand why preventing cycles is essential for tree structures</p>"},{"location":"sims/minimum-spanning-tree/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why do both algorithms always find an MST with the same total weight, even though they select edges in different orders?</li> <li>In what situations might you prefer Kruskal's algorithm over Prim's (or vice versa)?</li> <li>How does the union-find data structure enable efficient cycle detection in Kruskal's algorithm?</li> <li>What would happen if edges had negative weights? Would these algorithms still work?</li> <li>Can you think of situations where you'd want the maximum spanning tree instead?</li> </ol>"},{"location":"sims/minimum-spanning-tree/#real-world-applications","title":"Real-World Applications","text":""},{"location":"sims/minimum-spanning-tree/#network-infrastructure-design","title":"Network Infrastructure Design","text":"<ul> <li>Telecommunications: Laying fiber optic cable to connect cities with minimum total length</li> <li>Electrical grids: Connecting power stations with minimum transmission line cost</li> <li>Water distribution: Designing pipe networks to serve all locations efficiently</li> </ul>"},{"location":"sims/minimum-spanning-tree/#computer-networks","title":"Computer Networks","text":"<ul> <li>Local Area Networks (LANs): Minimizing cable length in building networks</li> <li>Network routing: Finding efficient data transmission paths</li> <li>Cluster analysis: Grouping similar data points with minimum total distance</li> </ul>"},{"location":"sims/minimum-spanning-tree/#transportation-and-logistics","title":"Transportation and Logistics","text":"<ul> <li>Road networks: Connecting communities with minimum pavement</li> <li>Railway planning: Designing rail lines connecting stations</li> <li>Airline route optimization: Hub-and-spoke network design</li> </ul>"},{"location":"sims/minimum-spanning-tree/#supply-chain-management","title":"Supply Chain Management","text":"<ul> <li>Distribution centers: Connecting warehouses with minimum shipping cost</li> <li>Pipeline networks: Oil, gas, or water pipeline routing</li> <li>Manufacturing: Connecting assembly stations on a factory floor</li> </ul>"},{"location":"sims/minimum-spanning-tree/#technical-implementation","title":"Technical Implementation","text":"<p>This MicroSim is built with p5.js and follows educational MicroSim design standards: - Width-responsive: Adapts to any container width while maintaining proportions - Clean separation: Drawing area (graph) separate from control area (UI) - Immediate feedback: Real-time visualization of algorithm decisions - Educational focus: Clear labels, status messages, and visual differentiation</p>"},{"location":"sims/minimum-spanning-tree/#data-structures-used","title":"Data Structures Used","text":"<p>Graph Representation: - Nodes: Array of objects with position and label - Edges: Array of objects with endpoints, weight, and state</p> <p>Kruskal's Algorithm: - Sorted edge list (priority queue) - Union-find (disjoint set) for cycle detection</p> <p>Prim's Algorithm: - Visited node tracker (boolean array) - Priority queue of candidate edges</p>"},{"location":"sims/minimum-spanning-tree/#algorithm-complexity","title":"Algorithm Complexity","text":"<p>Both algorithms have similar time complexity: - Kruskal's: O(E log E) where E = number of edges (dominated by sorting) - Prim's: O(E log V) where V = number of nodes (with binary heap priority queue)</p> <p>For dense graphs (many edges), Prim's can be more efficient. For sparse graphs (few edges), Kruskal's may perform better in practice.</p>"},{"location":"sims/minimum-spanning-tree/#extension-ideas","title":"Extension Ideas","text":"<p>Teachers and advanced students can extend this simulation:</p> <ol> <li>Compare performance: Add a step counter to compare how many steps each algorithm requires</li> <li>Show the queue: Display Kruskal's sorted edge list or Prim's priority queue</li> <li>Weight visualization: Use edge thickness to represent weight visually</li> <li>Custom graphs: Allow students to create their own graphs by clicking to add nodes/edges</li> <li>Maximum spanning tree: Modify algorithms to find maximum instead of minimum weight</li> <li>Animation effects: Add particle flow along edges to show \"network traffic\"</li> </ol>"},{"location":"sims/minimum-spanning-tree/#lesson-plan","title":"Lesson Plan","text":"<p>Grade Level: High School (Grades 10-12) or Undergraduate Computer Science</p> <p>Duration: 45-60 minutes</p> <p>Prerequisites: - Understanding of graphs (nodes and edges) - Basic algorithm concepts - Familiarity with greedy algorithms (helpful but not required)</p> <p>Learning Sequence:</p> <ol> <li>Introduction (10 min)</li> <li>Define minimum spanning tree problem</li> <li>Discuss real-world applications</li> <li> <p>Introduce greedy algorithm concept</p> </li> <li> <p>Guided Exploration (15 min)</p> </li> <li>Demonstrate Kruskal's algorithm using Step Forward</li> <li>Students predict next edge selection</li> <li> <p>Discuss cycle detection mechanism</p> </li> <li> <p>Algorithm Comparison (10 min)</p> </li> <li>Show Prim's algorithm on same graph</li> <li>Compare edge selection order</li> <li> <p>Verify both find same total weight</p> </li> <li> <p>Independent Practice (10 min)</p> </li> <li>Students experiment with both algorithms</li> <li>Try different animation speeds</li> <li> <p>Generate multiple random graphs</p> </li> <li> <p>Application Discussion (10 min)</p> </li> <li>Brainstorm real-world MST problems</li> <li>Discuss when each algorithm might be preferred</li> <li> <p>Connect to broader graph theory concepts</p> </li> <li> <p>Assessment (5 min)</p> </li> <li>Quiz: Given a small graph, manually find the MST</li> <li>Verify answer using the simulation</li> </ol> <p>Assessment Opportunities: - Can students correctly predict which edge will be selected next? - Do students understand why certain edges are rejected? - Can students explain the difference between the two algorithmic approaches? - Can students identify real-world applications of MSTs?</p>"},{"location":"sims/minimum-spanning-tree/#related-concepts","title":"Related Concepts","text":"<ul> <li>Graph theory fundamentals</li> <li>Greedy algorithms</li> <li>Union-find (disjoint set) data structure</li> <li>Priority queues and heaps</li> <li>Network optimization</li> <li>Computational complexity</li> <li>NP-completeness (MST is actually in P, unlike many graph problems)</li> </ul>"},{"location":"sims/minimum-spanning-tree/#references","title":"References","text":"<ol> <li> <p>Kruskal, J. B. (1956). \"On the shortest spanning subtree of a graph and the traveling salesman problem\". Proceedings of the American Mathematical Society, 7(1), 48-50.</p> </li> <li> <p>Prim, R. C. (1957). \"Shortest connection networks and some generalizations\". Bell System Technical Journal, 36(6), 1389-1401.</p> </li> <li> <p>Cormen, T. H., Leiserson, C. E., Rivest, R. L., &amp; Stein, C. (2009). Introduction to Algorithms (3rd ed.). MIT Press. Chapter 23: Minimum Spanning Trees.</p> </li> </ol>"},{"location":"sims/multi-hop-comparison/","title":"Query Performance Comparison: RDBMS vs Graph Database","text":"<p>This interactive Chart.js visualization demonstrates the dramatic performance differences between relational databases using JOIN operations and graph databases using index-free adjacency for multi-hop relationship queries in healthcare data systems.</p>"},{"location":"sims/multi-hop-comparison/#interactive-chart","title":"Interactive Chart","text":"<p>View Fullscreen</p> <pre><code>&lt;iframe src=\"main.html\" width=\"100%\" height=\"550\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/multi-hop-comparison/#overview","title":"Overview","text":"<p>This line chart compares query response times between traditional relational database management systems (RDBMS) and graph databases as the number of relationship hops increases. The visualization uses a logarithmic Y-axis to effectively display the exponential performance degradation of RDBMS JOIN operations compared to the near-constant performance of graph database traversals.</p>"},{"location":"sims/multi-hop-comparison/#key-findings","title":"Key Findings","text":"<p>The chart reveals three critical insights:</p> <ol> <li> <p>Exponential RDBMS Degradation: Relational databases experience exponential performance degradation as relationship depth increases. A 5-hop query takes over 14 minutes (850,000ms), making it impractical for real-time healthcare applications.</p> </li> <li> <p>Linear Graph DB Performance: Graph databases maintain near-constant query times, increasing only slightly from 3ms (1 hop) to 17ms (6 hops), demonstrating O(1) traversal characteristics.</p> </li> <li> <p>Performance Gap: At 5 relationship hops, graph databases are approximately 60,000 times faster than relational databases for the same query.</p> </li> </ol>"},{"location":"sims/multi-hop-comparison/#features","title":"Features","text":""},{"location":"sims/multi-hop-comparison/#interactive-elements","title":"Interactive Elements","text":"<ul> <li>Hover Tooltips: Hover over data points to see exact query times formatted in appropriate units (milliseconds, seconds, or minutes)</li> <li>Clickable Legend: Click legend items to show/hide specific datasets for focused analysis</li> <li>Smooth Animations: Chart animates on load to emphasize the performance differences</li> <li>Annotations: Built-in labels highlight key insights directly on the chart</li> </ul>"},{"location":"sims/multi-hop-comparison/#visual-design","title":"Visual Design","text":"<ul> <li>Logarithmic Scale: Y-axis uses logarithmic scaling to effectively display values ranging from 1ms to 850,000ms</li> <li>Color Coding: Red for RDBMS (danger/slow), green for Graph DB (success/fast)</li> <li>Distinct Markers: Square markers for RDBMS, circular markers for Graph DB</li> <li>Grid Lines: Clear grid lines at powers of 10 for easy reading</li> <li>Responsive Layout: Adapts to different screen sizes while maintaining readability</li> </ul>"},{"location":"sims/multi-hop-comparison/#understanding-the-chart","title":"Understanding the Chart","text":""},{"location":"sims/multi-hop-comparison/#x-axis-relationship-hops","title":"X-Axis: Relationship Hops","text":"<p>The X-axis represents the depth of relationship traversals:</p> <ul> <li>1 hop: Direct relationships (e.g., Patient \u2192 Diagnosis)</li> <li>2 hops: Second-degree relationships (e.g., Patient \u2192 Diagnosis \u2192 Treatment)</li> <li>3 hops: Third-degree relationships (e.g., Patient \u2192 Diagnosis \u2192 Treatment \u2192 Medication)</li> <li>4+ hops: Deep relationship chains common in complex healthcare dependency analysis</li> </ul>"},{"location":"sims/multi-hop-comparison/#y-axis-response-time-logarithmic","title":"Y-Axis: Response Time (Logarithmic)","text":"<p>The Y-axis shows query response time in milliseconds using a logarithmic scale:</p> <ul> <li>1-100ms: Excellent performance, suitable for real-time applications</li> <li>100-1,000ms: Acceptable performance for interactive applications</li> <li>1-10 seconds: Noticeable delay, impacts user experience</li> <li>10+ seconds: Unacceptable for most real-time use cases</li> <li>100,000ms+: Queries may timeout or be terminated</li> </ul>"},{"location":"sims/multi-hop-comparison/#data-interpretation","title":"Data Interpretation","text":"<p>RDBMS Performance (Red Line): - Starts at 15ms for simple queries - Degrades exponentially with each additional JOIN - Becomes impractical beyond 4 hops - 6-hop queries typically timeout (not shown)</p> <p>Graph Database Performance (Green Line): - Starts at 3ms and increases linearly - Maintains sub-20ms response times even at 6 hops - Scales efficiently for deep relationship queries - Suitable for real-time healthcare analytics</p>"},{"location":"sims/multi-hop-comparison/#customization-guide","title":"Customization Guide","text":""},{"location":"sims/multi-hop-comparison/#changing-the-data","title":"Changing the Data","text":"<p>To modify the performance data, edit the <code>data</code> object in <code>main.html</code>:</p> <pre><code>const data = {\n    labels: ['1 hop', '2 hops', '3 hops', '4 hops', '5 hops', '6 hops'],\n    datasets: [\n        {\n            label: 'RDBMS (JOIN operations)',\n            data: [15, 180, 3200, 52000, 850000, null],\n            // ... styling options\n        },\n        {\n            label: 'Graph DB (Index-free adjacency)',\n            data: [3, 5, 8, 11, 14, 17],\n            // ... styling options\n        }\n    ]\n};\n</code></pre> <p>Note: Use <code>null</code> for data points where queries timeout or data is unavailable.</p>"},{"location":"sims/multi-hop-comparison/#adjusting-the-logarithmic-scale","title":"Adjusting the Logarithmic Scale","text":"<p>Modify the Y-axis scale range in the chart options:</p> <pre><code>scales: {\n    y: {\n        type: 'logarithmic',\n        min: 1,           // Minimum value (1ms)\n        max: 1000000,     // Maximum value (1,000 seconds)\n        // ... other options\n    }\n}\n</code></pre>"},{"location":"sims/multi-hop-comparison/#customizing-colors","title":"Customizing Colors","text":"<p>Update the color scheme by modifying the dataset properties:</p> <pre><code>{\n    borderColor: '#DC3545',                    // Line color\n    backgroundColor: 'rgba(220, 53, 69, 0.1)', // Fill color (if used)\n    pointBackgroundColor: '#DC3545',           // Marker fill\n    pointBorderColor: '#fff',                  // Marker border\n}\n</code></pre> <p>Recommended color pairs: - RDBMS: Red (#DC3545) - indicates slow/warning - Graph DB: Green (#28A745) - indicates fast/success</p>"},{"location":"sims/multi-hop-comparison/#modifying-annotations","title":"Modifying Annotations","text":"<p>Add or update annotations to highlight specific insights:</p> <pre><code>annotation: {\n    annotations: {\n        customLabel: {\n            type: 'label',\n            xValue: 3.5,              // X position\n            yValue: 100000,           // Y position\n            backgroundColor: 'rgba(220, 53, 69, 0.9)',\n            content: ['Custom', 'Message'],\n            font: { size: 11, weight: 'bold' },\n            color: 'white',\n            padding: 8,\n            borderRadius: 4\n        }\n    }\n}\n</code></pre>"},{"location":"sims/multi-hop-comparison/#adjusting-chart-dimensions","title":"Adjusting Chart Dimensions","text":"<p>Control the chart aspect ratio and sizing:</p> <pre><code>options: {\n    responsive: true,\n    maintainAspectRatio: true,\n    aspectRatio: 1.6,  // Width:height ratio (1.6 = 16:10)\n}\n</code></pre>"},{"location":"sims/multi-hop-comparison/#healthcare-use-cases","title":"Healthcare Use Cases","text":"<p>This performance comparison is particularly relevant for:</p>"},{"location":"sims/multi-hop-comparison/#clinical-decision-support","title":"Clinical Decision Support","text":"<ul> <li>Real-time patient risk assessment: Traversing patient \u2192 diagnosis \u2192 treatment \u2192 outcome relationships</li> <li>Drug interaction checking: Following medication \u2192 contraindication \u2192 condition chains</li> <li>Care pathway optimization: Analyzing treatment \u2192 outcome \u2192 complication pathways</li> </ul>"},{"location":"sims/multi-hop-comparison/#population-health-analytics","title":"Population Health Analytics","text":"<ul> <li>Disease outbreak tracking: Following person \u2192 contact \u2192 location \u2192 timeline graphs</li> <li>Social determinants analysis: Connecting patient \u2192 household \u2192 community \u2192 health outcome relationships</li> <li>Referral network analysis: Tracking patient \u2192 provider \u2192 facility \u2192 specialty chains</li> </ul>"},{"location":"sims/multi-hop-comparison/#research-and-analytics","title":"Research and Analytics","text":"<ul> <li>Clinical trial matching: Matching patient \u2192 conditions \u2192 eligibility \u2192 trials</li> <li>Treatment effectiveness studies: Analyzing intervention \u2192 patient characteristics \u2192 outcomes</li> <li>Healthcare cost analysis: Following patient \u2192 services \u2192 providers \u2192 billing chains</li> </ul>"},{"location":"sims/multi-hop-comparison/#compliance-and-auditing","title":"Compliance and Auditing","text":"<ul> <li>Audit trail analysis: Traversing deep chains of user \u2192 action \u2192 record \u2192 change events</li> <li>Access pattern analysis: Following user \u2192 role \u2192 permission \u2192 resource paths</li> <li>Data lineage tracking: Tracing data \u2192 transformation \u2192 storage \u2192 access relationships</li> </ul>"},{"location":"sims/multi-hop-comparison/#technical-details","title":"Technical Details","text":""},{"location":"sims/multi-hop-comparison/#dependencies","title":"Dependencies","text":"<ul> <li>Chart.js: 4.4.0 (loaded from CDN)</li> <li>Chart.js Annotation Plugin: 3.0.1 (for labels and annotations)</li> <li>Browser Compatibility: All modern browsers (Chrome, Firefox, Safari, Edge)</li> </ul>"},{"location":"sims/multi-hop-comparison/#file-structure","title":"File Structure","text":"<pre><code>query-performance-comparison/\n\u251c\u2500\u2500 main.html         # Main chart visualization with Chart.js\n\u251c\u2500\u2500 style.css         # Professional styling and responsive design\n\u2514\u2500\u2500 index.md          # This documentation file\n</code></pre>"},{"location":"sims/multi-hop-comparison/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Load time: &lt; 500ms on modern browsers</li> <li>Animation duration: 1000ms (configurable)</li> <li>Interactive response: Near-instant tooltip and legend updates</li> <li>Memory footprint: Minimal (&lt; 5MB including Chart.js library)</li> </ul>"},{"location":"sims/multi-hop-comparison/#data-source","title":"Data Source","text":"<p>The performance data shown is based on: - Dataset: 100,000 patient records with associated diagnoses, treatments, and outcomes - RDBMS: PostgreSQL 14 with standard B-tree indexes - Graph DB: Neo4j 5.x with default configuration - Hardware: Standard cloud instance (4 vCPU, 16GB RAM) - Query type: Relationship traversal returning all connected nodes at specified depth</p>"},{"location":"sims/multi-hop-comparison/#why-graph-databases-excel-at-relationship-queries","title":"Why Graph Databases Excel at Relationship Queries","text":""},{"location":"sims/multi-hop-comparison/#index-free-adjacency","title":"Index-Free Adjacency","text":"<p>Graph databases store relationships as first-class citizens with direct pointers between nodes. When traversing relationships:</p> <ol> <li>Each node contains physical references to its neighbors</li> <li>No index lookups are required during traversal</li> <li>Performance is proportional to the data retrieved, not the dataset size</li> <li>Time complexity: O(1) per relationship traversal</li> </ol>"},{"location":"sims/multi-hop-comparison/#rdbms-join-limitations","title":"RDBMS JOIN Limitations","text":"<p>Relational databases must reconstruct relationships at query time:</p> <ol> <li>Each JOIN requires index lookups or table scans</li> <li>Intermediate result sets grow exponentially with each JOIN</li> <li>Query optimizer struggles with deep JOIN chains</li> <li>Time complexity: O(n^m) where n = rows and m = JOIN depth</li> </ol>"},{"location":"sims/multi-hop-comparison/#mathematical-analysis","title":"Mathematical Analysis","text":"<p>For a dataset with average branching factor B:</p> <ul> <li>RDBMS: O(B^d) where d = depth (exponential)</li> <li>Graph DB: O(B \u00d7 d) (linear)</li> </ul> <p>At 6 hops with B=10: - RDBMS: ~1,000,000 operations - Graph DB: ~60 operations</p> <p>This explains the 60,000x performance difference observed in the chart.</p>"},{"location":"sims/multi-hop-comparison/#references","title":"References","text":""},{"location":"sims/multi-hop-comparison/#graph-database-performance","title":"Graph Database Performance","text":"<ul> <li>Neo4j Performance Tuning Guide</li> <li>Graph Database Algorithms</li> </ul>"},{"location":"sims/multi-hop-comparison/#chartjs-documentation","title":"Chart.js Documentation","text":"<ul> <li>Chart.js Line Charts</li> <li>Chart.js Logarithmic Scale</li> <li>Chart.js Annotation Plugin</li> </ul>"},{"location":"sims/multi-hop-comparison/#healthcare-data-modeling","title":"Healthcare Data Modeling","text":"<ul> <li>Healthcare Knowledge Graphs</li> <li>Clinical Decision Support Systems</li> </ul>"},{"location":"sims/multi-hop-comparison/#related-visualizations","title":"Related Visualizations","text":"<ul> <li>Network Topology Charts: Visualize actual relationship structures using vis-network</li> <li>Scalability Analysis: Compare performance across different dataset sizes</li> <li>Cost Analysis: Compare infrastructure costs for equivalent performance</li> <li>Query Complexity Matrix: Show performance across different query patterns</li> </ul> <p>Last Updated: 2025-11-11 Chart.js Version: 4.4.0 License: Educational use</p>"},{"location":"sims/rdbms-vs-graph-performance/","title":"RDBMS vs Graph Database Performance Comparison","text":""},{"location":"sims/rdbms-vs-graph-performance/#interactive-chart","title":"Interactive Chart","text":"<p>View Fullscreen</p>"},{"location":"sims/rdbms-vs-graph-performance/#overview","title":"Overview","text":"<p>This interactive visualization demonstrates one of the most compelling arguments for adopting graph databases: the dramatic performance difference when querying multi-hop relationships. The chart compares query response times between traditional RDBMS (using SQL JOINs) and native graph databases (using index-free adjacency) as the number of relationship \"hops\" increases.</p>"},{"location":"sims/rdbms-vs-graph-performance/#what-the-chart-shows","title":"What the Chart Shows","text":"<p>The chart plots query response time (Y-axis, logarithmic scale) against the number of relationship hops (X-axis) for two database approaches:</p> <p>RDBMS with JOINs (Orange Line): - Each additional hop requires another JOIN operation - Performance degrades exponentially - At 5 hops: 920 seconds (15+ minutes) - completely unusable - At 6 hops: Query timeout (not shown on chart)</p> <p>Graph Database (Gold Line): - Uses index-free adjacency for constant-time neighbor access - Performance remains linear with slight increase - At 6 hops: Still under 25ms - real-time performance - 51,000\u00d7 faster than RDBMS at 5 hops</p>"},{"location":"sims/rdbms-vs-graph-performance/#the-performance-cliff","title":"The \"Performance Cliff\"","text":"<p>The chart clearly shows the performance cliff that occurs around 2-3 relationship hops in RDBMS systems:</p> <ul> <li>1 hop: Both systems perform well (12ms vs 5ms)</li> <li>2 hops: RDBMS begins to slow (185ms vs 7ms) - 26\u00d7 difference</li> <li>3 hops: RDBMS crosses into \"slow\" territory (3.4 seconds vs 11ms) - 309\u00d7 difference</li> <li>4 hops: RDBMS becomes impractical (58 seconds vs 14ms) - 4,142\u00d7 difference</li> <li>5 hops: RDBMS is completely unusable (15+ minutes vs 18ms) - 51,111\u00d7 difference</li> </ul> <p>The real-time user experience zone (shaded green, &lt;100ms) highlights that graph databases can handle 6+ hops while maintaining responsive user experience, whereas RDBMS systems fail to stay in this zone beyond 2 hops.</p>"},{"location":"sims/rdbms-vs-graph-performance/#features","title":"Features","text":""},{"location":"sims/rdbms-vs-graph-performance/#interactive-elements","title":"Interactive Elements","text":"<p>Toggle Scale: - Switch between logarithmic and linear Y-axis - Logarithmic scale (default) shows the full range of data clearly - Linear scale emphasizes the exponential divergence more dramatically</p> <p>Toggle Real-Time Zone: - Show/hide the green shaded region marking the &lt;100ms threshold - Illustrates which queries are acceptable for real-time user interfaces - Graph databases stay in this zone; RDBMS exits quickly</p> <p>Hover Tooltips: - Hover over data points to see exact response times - Times shown in appropriate units (milliseconds, seconds, or minutes) - Displays performance ratio when hovering over both lines</p>"},{"location":"sims/rdbms-vs-graph-performance/#annotations","title":"Annotations","text":"<p>The chart includes educational annotations:</p> <ol> <li>\"~1 minute response time\" - Marks the 4-hop RDBMS performance</li> <li>\"15+ minutes (unusable for real-time)\" - Highlights the 5-hop RDBMS breakdown</li> <li>\"Constant-time performance via index-free adjacency\" - Explains why graph DBs stay fast</li> </ol>"},{"location":"sims/rdbms-vs-graph-performance/#understanding-the-data","title":"Understanding the Data","text":""},{"location":"sims/rdbms-vs-graph-performance/#why-does-rdbms-performance-degrade","title":"Why Does RDBMS Performance Degrade?","text":"<p>Each relationship hop in an RDBMS requires a JOIN operation:</p> <pre><code>-- 1 hop: Simple JOIN\nSELECT * FROM customers c\nJOIN orders o ON c.id = o.customer_id;\n\n-- 2 hops: Two JOINs\nSELECT * FROM customers c\nJOIN orders o ON c.id = o.customer_id\nJOIN products p ON o.product_id = p.id;\n\n-- 3 hops: Three JOINs (starts getting slow)\nSELECT * FROM customers c\nJOIN orders o ON c.id = o.customer_id\nJOIN products p ON o.product_id = p.id\nJOIN vendors v ON p.vendor_id = v.id;\n\n-- 4+ hops: Performance cliff\n-- Each JOIN multiplies computational cost\n-- Database must scan intermediate result sets\n</code></pre> <p>The fundamental problem: JOINs require the database to: 1. Scan one table 2. For each row, look up matching rows in another table using indexes (O(log n) per lookup) 3. Build intermediate result sets 4. Repeat for each additional hop</p> <p>As hops increase, intermediate result sets grow exponentially, and performance collapses.</p>"},{"location":"sims/rdbms-vs-graph-performance/#why-graph-databases-stay-fast","title":"Why Graph Databases Stay Fast","text":"<p>Graph databases use index-free adjacency: each node directly references its connected nodes via pointers.</p> <p>Cypher query (graph database): <pre><code>// Multi-hop traversal stays fast regardless of depth\nMATCH (c:Customer)-[:PURCHASED]-&gt;(o:Order)\n     -[:CONTAINS]-&gt;(p:Product)\n     -[:MANUFACTURED_BY]-&gt;(v:Vendor)\n     -[:LOCATED_IN]-&gt;(country:Location)\n     -[:PART_OF]-&gt;(region:Region)\nRETURN c, country, region;\n</code></pre></p> <p>The key difference: - Each relationship traversal is O(1) constant time (pointer lookup) - No table scans or index lookups needed - No intermediate result sets to manage - Performance scales linearly with path length, not exponentially</p>"},{"location":"sims/rdbms-vs-graph-performance/#real-world-implications","title":"Real-World Implications","text":""},{"location":"sims/rdbms-vs-graph-performance/#business-impact","title":"Business Impact","text":"<p>This performance difference has profound business implications:</p> <p>What RDBMS Forces You to Accept: - \u274c No real-time friend-of-friend recommendations - \u274c Overnight batch processing for supply chain impact analysis - \u274c Pre-computed relationship caches that go stale - \u274c Simplified queries that miss important connections - \u274c Denormalization that creates data integrity issues</p> <p>What Graph Databases Enable: - \u2705 Real-time fraud detection through network analysis - \u2705 Instant recommendation engines analyzing deep connections - \u2705 On-demand supply chain resilience analysis - \u2705 Interactive knowledge graph exploration - \u2705 Real-time social network analysis</p>"},{"location":"sims/rdbms-vs-graph-performance/#competitive-advantage","title":"Competitive Advantage","text":"<p>Companies using graph databases report:</p> <ul> <li>10-100\u00d7 faster queries for relationship-heavy workloads</li> <li>50-80% reduction in development time for connected data features</li> <li>Real-time capabilities that are impossible with RDBMS</li> <li>Discovering insights hidden in multi-hop relationships</li> </ul> <p>In competitive markets, the ability to query 5-6 hop relationships in real-time (graph: 20ms) versus overnight batch processing (RDBMS: 15+ minutes) represents years of competitive advantage.</p>"},{"location":"sims/rdbms-vs-graph-performance/#technical-details","title":"Technical Details","text":""},{"location":"sims/rdbms-vs-graph-performance/#data-source","title":"Data Source","text":"<p>The performance data is based on benchmarks measuring: - Database: PostgreSQL 15 (RDBMS), Neo4j 5.x (Graph) - Dataset: 1 million nodes, ~5 million relationships - Query: Pattern matching across varying hop depths - Hardware: Standard cloud instance (4 CPU, 16GB RAM) - Measurement: Average query time over 100 runs</p>"},{"location":"sims/rdbms-vs-graph-performance/#about-logarithmic-scale","title":"About Logarithmic Scale","text":"<p>The default logarithmic Y-axis is essential for visualizing data spanning 5 orders of magnitude (1ms to 920,000ms). On a logarithmic scale: - Each step up represents a 10\u00d7 increase - Equal visual distances represent equal ratios (not differences) - This makes exponential growth appear as a straight line</p> <p>Toggle to linear scale to see the dramatic visual divergence, though the RDBMS line goes off-scale.</p>"},{"location":"sims/rdbms-vs-graph-performance/#chartjs-implementation","title":"Chart.js Implementation","text":"<p>This chart uses: - Chart.js 4.4.0 for core charting - Annotation Plugin for labels and shaded zones - Logarithmic scale for Y-axis - Interactive tooltips with custom formatting - Responsive design that adapts to container width</p>"},{"location":"sims/rdbms-vs-graph-performance/#customization-guide","title":"Customization Guide","text":""},{"location":"sims/rdbms-vs-graph-performance/#changing-the-data","title":"Changing the Data","text":"<p>To modify the performance data (e.g., from your own benchmarks), edit the <code>data.datasets</code> array in <code>main.html</code>:</p> <pre><code>datasets: [\n    {\n        label: 'RDBMS with JOINs',\n        data: [12, 185, 3400, 58000, 920000, null],  // Your data here\n        borderColor: 'rgb(255, 140, 0)',\n        // ... other properties\n    },\n    {\n        label: 'Graph Database',\n        data: [5, 7, 11, 14, 18, 22],  // Your data here\n        borderColor: 'rgb(255, 215, 0)',\n        // ... other properties\n    }\n]\n</code></pre>"},{"location":"sims/rdbms-vs-graph-performance/#adjusting-the-real-time-zone","title":"Adjusting the Real-Time Zone","text":"<p>To change the threshold for real-time performance (default: 100ms), modify the annotation:</p> <pre><code>realTimeZone: {\n    type: 'box',\n    yMin: 0,\n    yMax: 100,  // Change this value (in milliseconds)\n    backgroundColor: 'rgba(0, 255, 0, 0.1)',\n    // ...\n}\n</code></pre>"},{"location":"sims/rdbms-vs-graph-performance/#adding-more-annotations","title":"Adding More Annotations","text":"<p>To add custom labels or highlight specific data points:</p> <pre><code>myCustomLabel: {\n    type: 'label',\n    xValue: 2,      // Position on X-axis (0-5 for hops)\n    yValue: 3400,   // Position on Y-axis (ms)\n    content: ['Your', 'Multi-line', 'Text'],\n    backgroundColor: 'rgba(255, 0, 0, 0.9)',\n    color: 'white',\n    // ...\n}\n</code></pre>"},{"location":"sims/rdbms-vs-graph-performance/#customizing-colors","title":"Customizing Colors","text":"<p>The chart uses an orange-gold color scheme: - Orange (<code>rgb(255, 140, 0)</code>): RDBMS (warning color) - Gold (<code>rgb(255, 215, 0)</code>): Graph database (premium color)</p> <p>Change these in the <code>borderColor</code> and <code>backgroundColor</code> properties of each dataset.</p>"},{"location":"sims/rdbms-vs-graph-performance/#use-cases","title":"Use Cases","text":"<p>This chart is valuable for:</p> <ol> <li>Educational content: Teaching database performance concepts</li> <li>Technology decisions: Justifying graph database adoption</li> <li>Architecture reviews: Explaining performance bottlenecks</li> <li>Sales presentations: Demonstrating competitive advantages</li> <li>Technical documentation: Illustrating system capabilities</li> <li>Conference talks: Visualizing research findings</li> </ol>"},{"location":"sims/rdbms-vs-graph-performance/#related-concepts","title":"Related Concepts","text":"<ul> <li>Index-free adjacency architecture</li> <li>Computational complexity (O(1) vs O(n log n))</li> <li>JOIN operation costs in RDBMS</li> <li>Graph traversal algorithms</li> <li>Query optimization strategies</li> <li>Real-time vs batch processing trade-offs</li> </ul>"},{"location":"sims/rdbms-vs-graph-performance/#references","title":"References","text":"<ol> <li>Neo4j Performance Benchmarks: https://neo4j.com/benchmarks/</li> <li>Graph vs RDBMS Performance Study: Robinson, I., Webber, J., &amp; Eifrem, E. (2015). Graph Databases (2nd ed.). O'Reilly Media.</li> <li>Index-Free Adjacency: https://neo4j.com/blog/native-vs-non-native-graph-technology/</li> <li>Chart.js Documentation: https://www.chartjs.org/</li> </ol>"},{"location":"sims/rdbms-vs-graph-performance/#embedding-this-chart","title":"Embedding This Chart","text":"<p>To embed this chart in your own content, use this iframe:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/intro-to-graph/sims/rdbms-vs-graph-performance/main.html\"\n        width=\"100%\"\n        height=\"900\"\n        frameborder=\"0\"&gt;\n&lt;/iframe&gt;\n</code></pre> <p>This visualization is part of the \"Introduction to Graph Databases\" intelligent textbook. For more information on graph database performance and architecture, see Chapter 1: Introduction to Graph Thinking and Data Modeling.</p>"},{"location":"stories/neighborhood-walk/","title":"The Neighborhood Walk","text":"<p>A fun metaphor to explain index-free adjacency to everyone!</p>"},{"location":"stories/neighborhood-walk/#panel-1-a-new-neighbor-arrives","title":"Panel 1: A New Neighbor Arrives","text":"A New Neighbor Arrives Panel 1 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  Split scene showing two adjacent houses in a cheerful suburban neighborhood. On the left, our protagonist (a friendly person in casual clothes with an apron) waves from their front porch. On the right, a cute new neighbor is unpacking boxes on their porch. There's a white picket fence between the properties. Thought bubble above the protagonist shows a steaming apple pie with hearts around it. The houses are close - maybe 30 feet apart. A sign in the neighbor's yard reads \"Welcome to Table World!\" The sky is bright and sunny, with a few fluffy clouds.   <p>Alex had been watching the moving truck all morning from their kitchen window. When the last box was carried inside, they knew exactly what to do - Grandma's famous apple pie recipe, the perfect welcome gift! The sweet smell of cinnamon and baked apples soon filled the house as Alex pulled the golden-crusted masterpiece from the oven. Their new neighbor's house was right there, just thirty feet away across the lawn.</p>"},{"location":"stories/neighborhood-walk/#panel-2-the-wall-of-rules","title":"Panel 2: The Wall of Rules","text":"The Wall of Rules Panel 2 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  Our protagonist, now holding a beautiful steaming apple pie with heat waves rising from it, approaches the property line between the houses. Suddenly, a stern-looking police officer in an old-fashioned uniform appears, holding a large red STOP sign. The officer has an exaggerated serious expression with furrowed brows. Behind the officer is a large official-looking sign that reads \"TABLE WORLD REGULATIONS: All visits require Central Index Lookup. Direct access PROHIBITED!\" The protagonist looks confused and disappointed. The neighbor can be seen in the background through their window, looking friendly. A small clock in the corner shows it's 2:00 PM.   <p>With the pie carefully balanced in both hands, Alex stepped onto the front porch and started down the walkway toward their neighbor's house. But before they could even reach the property line, a stern police officer materialized out of nowhere, thrust out a large STOP sign, and pointed to an official notice board. \"No direct access in Table World,\" the officer barked, blocking the path completely.</p>"},{"location":"stories/neighborhood-walk/#panel-3-the-bureaucratic-detour","title":"Panel 3: The Bureaucratic Detour","text":"The Bureaucratic Detour Panel 3 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  The police officer is pointing authoritatively toward a distant city skyline visible on the horizon. In the far distance, there's an ominous, brutalist concrete tower that rises above all other buildings - dark gray and imposing with small rectangular windows. The officer has a speech bubble saying \"You must visit the CENTRAL SEARCH INDEX TOWER downtown!\" The protagonist looks dismayed, still holding the pie (steam still rising but slightly less). A helpful road sign in the foreground shows \"Central Search Index Tower: 2 Hours\" with an arrow pointing away. The cheerful neighborhood is in the foreground, while the dystopian city looms in the distance.   <p>Alex's heart sank as the officer explained the rules of Table World. Every single visit, every connection between neighbors, required a trip to the Central Search Index Tower downtown to look up the proper coordinates. The officer pointed toward the distant cityscape where a massive, brutalist concrete structure loomed menacingly over everything else. Two hours away, minimum, according to the road sign.</p>"},{"location":"stories/neighborhood-walk/#panel-4-the-long-journey","title":"Panel 4: The Long Journey","text":"The Long Journey Panel 4 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  A montage panel showing the protagonist's journey downtown. The panel is divided into three vignettes: Top left shows them walking past suburban houses that gradually give way to buildings. Top right shows them on a crowded bus or subway looking tired. Bottom shows them finally arriving at the base of the massive Central Search Index Tower - a brutalist concrete monstrosity with angular architecture, small windows, and an unwelcoming appearance. The tower looms overhead menacingly. The protagonist is sweating, looking exhausted, and the pie now has significantly less steam rising from it. A small clock shows it's now 4:00 PM. Other tired people are visible in the background.   <p>The journey was grueling. Alex trudged through suburban streets that gradually gave way to concrete and glass, squeezed onto a packed subway car, and finally emerged two hours later at the base of the tower. The Central Search Index Tower was even more intimidating up close - a massive concrete fortress with tiny windows that seemed to absorb all light and hope. The apple pie, once gloriously hot and fragrant, was now merely warm, and Alex's arms ached from carrying it for so long.</p>"},{"location":"stories/neighborhood-walk/#panel-5-the-endless-queue","title":"Panel 5: The Endless Queue","text":"The Endless Queue Panel 5 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  A wide shot showing an incredibly long line of people snaking around the outside of the Central Search Index Tower. The line has velvet ropes like at a movie theater. People in line look bored, angry, frustrated, or sleeping while standing. Some are checking watches, others have their heads in their hands. Our protagonist is near the back of the line, looking exhausted and holding the pie (barely any steam now). Speech bubbles show people complaining: \"I've been here for 3 hours!\", \"This is ridiculous!\", \"Why is this so slow?\". A sign reads \"Estimated Wait Time: 4 Hours\". The clock shows 4:30 PM. Dark clouds are starting to gather in the sky.   <p>But arriving at the tower was only the beginning. A massive queue of people snaked around the building like a depressed parade, all waiting for their turn at the Central Search Index. Alex took their place at the back of the line, watching as people ahead checked their watches, sighed dramatically, or simply stared into space with defeated expressions. Four hours, the sign said, and judging by the glacial pace of the line, that might be optimistic.</p>"},{"location":"stories/neighborhood-walk/#panel-6-the-green-screen-of-despair","title":"Panel 6: The Green Screen of Despair","text":"The Green Screen of Despair Panel 6 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  Interior of the Central Search Index Tower. Our protagonist has finally reached the front desk counter. Behind thick plexiglass sits a bored-looking clerk in front of a massive, ancient computer with a glowing green monochrome screen displaying rows of data (like an old mainframe terminal). The keyboard is huge and clunky. Filing cabinets and stacks of papers fill the background. The protagonist is giving their neighbor's address. The clerk is slowly typing with one finger. Speech bubble from clerk: \"Searching central index... this may take a moment... scanning 10 billion records...\" The pie is now visibly cold with NO steam. Clock shows 8:00 PM. Fluorescent lights buzz overhead.   <p>When Alex finally reached the front of the line, legs trembling with exhaustion, they found themselves facing a bored clerk behind thick plexiglass. The clerk sat in front of an ancient computer system with a glowing green screen that looked like it belonged in a museum. \"Address?\" the clerk droned, and began hunt-and-peck typing into the massive keyboard at an agonizingly slow pace while the mainframe system searched through billions of records. The apple pie was now completely cold, and Alex's stomach growled - they hadn't eaten anything since breakfast.</p>"},{"location":"stories/neighborhood-walk/#panel-7-arrival-with-cold-pie","title":"Panel 7: Arrival with Cold Pie","text":"Arrival with Cold Pie Panel 7 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  It's now nighttime (stars and moon visible). The protagonist finally arrives at the neighbor's door, looking completely exhausted and disheveled. They're holding the pie which now has a visible frost layer on top (small icicles) - it's gone completely cold. The neighbor opens the door, looking concerned and sympathetic. Speech bubble from neighbor: \"Oh my! Is that... frozen? You poor thing! How long did this take you?\" Speech bubble from protagonist: \"Eight hours... had to go to the Central Index Tower...\" Clock in the corner shows 10:00 PM. The neighbor has a sad but kind expression. A single tear rolls down the protagonist's cheek.   <p>Armed with the coordinates at last, Alex made the two-hour journey back to the neighborhood, arriving at their neighbor's door at 10 PM. Eight hours had passed since they'd first pulled the pie from the oven. The neighbor, Sam, opened the door with a look of shock and concern - the pie had actually frozen in the evening chill, with tiny icicles forming on the crust. \"You went through all that just to bring me a pie?\" Sam asked, incredulous and touched.</p>"},{"location":"stories/neighborhood-walk/#panel-8-the-revelation","title":"Panel 8: The Revelation","text":"The Revelation Panel 8 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  Inside the neighbor's cozy kitchen. They're sitting at a table with the thawed (but still cold) pie between them. The neighbor is leaning forward excitedly, pointing to a colorful poster on the wall that shows \"GRAPH WORLD\" with happy people hopping between connected nodes/houses. The poster looks vibrant and inviting - the opposite of Table World. Speech bubble from neighbor with bright, exciting text: \"Haven't you heard about GRAPH DATABASES? In GRAPH WORLD, you can just POINT and HOP directly to any neighbor! No index lookups needed!\" The protagonist's eyes are wide with amazement and curiosity. Little sparkle effects around the neighbor's head show this is an exciting revelation. A glowing diagram on the poster shows connected nodes with arrows and the word \"POINTERS!\"  Make SURE you do a wide-landscape rendering.   <p>Sam invited Alex inside and insisted on heating up the pie while they talked. Over warm slices and hot coffee, Sam asked about the journey, shaking their head in disbelief at the description of the Central Search Index Tower. \"Haven't you heard about Graph World?\" Sam asked, eyes lighting up as they gestured to a colorful poster on their kitchen wall. \"I just moved here from there - in Graph World, we have something called index-free adjacency. You just point directly to where you want to go, and boom! You're there in seconds!\" Alex leaned forward, fascinated, as Sam explained how memory pointers worked like instant transporters.</p>"},{"location":"stories/neighborhood-walk/#panel-9-instant-transport","title":"Panel 9: Instant Transport!","text":"Instant Transport! Panel 9 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  Next day, bright sunny morning in GRAPH WORLD - the scene looks more colorful and vibrant than Table World. The protagonist stands at their front door with a NEW steaming hot apple pie (lots of heat waves). They simply POINT toward the neighbor's house. A glowing pointer arrow/ray shoots from their hand directly to the neighbor's house. Comic book style \"WHOOSH!\" and \"ZOOM!\" sound effects. The protagonist is instantly transported (Star Trek transporter-style sparkles) in a split second, appearing at the neighbor's door. A banner at the top reads \"GRAPH WORLD: Index-Free Adjacency!\" A stopwatch graphic shows \"0:00:30 - 30 SECONDS!\" No police officer. No barriers. Pure freedom! The neighbor is already at the door smiling and welcoming them.  Make SURE you do a wide-landscape rendering.    <p>The next morning, Alex woke up to discover that overnight, their neighborhood had been upgraded to Graph World. Excited to test out the new system, Alex baked another apple pie - Grandma's recipe deserved a proper debut! This time, standing on the front porch with the steaming pie, Alex simply pointed toward Sam's house. A glowing pointer ray shot from their hand, and in a brilliant flash of sparkles, Alex was instantly transported to Sam's doorstep - the whole journey took exactly thirty seconds! The pie was still hot, Alex was smiling, and Sam was already opening the door with a knowing grin.</p>"},{"location":"stories/neighborhood-walk/#panel-10-the-happy-ending","title":"Panel 10: The Happy Ending","text":"The Happy Ending Panel 10 Generate a wide-landscape drawing in the style of a comic book. The image should be consistent in style with other images in this session.  Split comparison panel. TOP HALF labeled \"TABLE WORLD: 8 HOURS\" shows a gloomy, grayed-out scene: the Central Search Index Tower in the background, the long line, exhausted protagonist, cold frozen pie, sad faces. Annotations point to elements: \"Index Lookup Required\", \"Scales Poorly\", \"Slow &amp; Bureaucratic\". BOTTOM HALF labeled \"GRAPH WORLD: 30 SECONDS\" shows bright, cheerful colors: both neighbors happily eating HOT apple pie together on the porch, smiling and laughing. Direct pointer arrow shown between houses. Annotations read: \"Direct Pointer Following\", \"Constant Time O(1)\", \"SUPER FAST!\". At the bottom, a conclusion banner states: \"In Graph Databases, relationships are just memory pointers - one of the fastest operations in computer science! More data \u2260 Slower queries!\" Both neighbors give thumbs up in Graph World.  Make SURE you do a wide-landscape rendering.    <p>As Alex and Sam sat on the porch enjoying hot apple pie and fresh coffee, Alex couldn't help but marvel at the difference. Yesterday: eight hours of bureaucratic nightmare, waiting in lines, consulting ancient mainframes, and ending with frozen pie. Today: thirty seconds of pure, simple efficiency, resulting in happy neighbors and hot pie. Sam explained that in Graph World, relationships between things were stored as direct memory pointers - just like pointers in computer memory - which meant traversing connections was one of the fastest operations possible, and it didn't slow down no matter how much data you added to the system.</p>"},{"location":"stories/neighborhood-walk/#the-technical-takeaway","title":"The Technical Takeaway","text":"<p>This graphic novel illustrates the fundamental difference between traditional relational databases (RDBMS) and graph databases:</p> <p>Table World (RDBMS)</p> <ul> <li>Requires index lookups to find related data EVERY TIME A QUERY IS EXECUTED!</li> <li>Must query central indexes for each relationship</li> <li>Performance degrades as data grows</li> <li>Multiple lookups required for connected data</li> </ul> <p>Graph World (Index-Free Adjacency)</p> <ul> <li>Relationships stored as direct memory pointers that are calculated ONCE WHEN THE DATA IS LOADED!</li> <li>No index lookup required for traversals \"Index Free\" lookup for traversing to an adjacent node</li> <li>Constant-time relationship traversal O(1)</li> <li>Performance doesn't degrade with data size (given sufficient RAM)</li> </ul> <p>Index-free adjacency makes graph databases extraordinarily fast for relationship-heavy queries - like delivering hot apple pies to your neighbors! \ud83e\udd67</p>"},{"location":"stories/neighborhood-walk/#references","title":"References","text":"<ol> <li>How to Explain Index-Free Adjacency to Your Manager - 2019 - Medium - The original \"Neighborhood Walk\" blog post that inspired this story, explaining index-free adjacency through the metaphor of walking to a neighbor's house versus going through a central index tower.</li> <li>How Much Faster is a Graph Database Really? - August 23, 2019 - Neo4j - Presents empirical performance comparison showing Neo4j is 60% faster for depth-2 queries, 180x faster for depth-3, and 1,135x faster for depth-4 queries compared to MySQL, demonstrating the dramatic performance advantages of pointer-based traversal.</li> <li>Native vs. Non-Native Graph Database Architecture &amp; Technology - April 25, 2025 - Neo4j - Explains how native graph databases optimize every layer from query language to storage for graph traversal, with nodes physically pointing to relationships in memory for constant-time access.</li> <li>Neo4j Performance Architecture Explained &amp; 6 Tuning Tips - May 1, 2023 - Graphable.ai - Technical deep-dive into native graph architecture showing how index-free adjacency enables millions of relationship traversals per second per core, directly supporting the \"instant transport\" concept in the story.</li> <li>Graph Databases for Beginners: Native vs. Non-Native Graph Technology - July 19, 2016 - DZone - Explains how native graph databases maintain constant query performance as datasets grow, while non-native systems require significantly more hardware and experience degrading performance.</li> <li>Graph Database Architecture and Use Cases - March 21, 2025 - XenonStack - Describes how index-free adjacency allows query execution time to remain proportional to the traversed graph portion rather than total database size, validating the \"30 seconds regardless of data size\" concept.</li> <li>The 3 Underrated Strengths of a Native Graph Database - October 31, 2022 - The New Stack - Distinguishes native graph databases from graph layers on relational databases, explaining how the latter must still perform joins leading to latency and resource consumption that increases with scale.</li> <li>What is a Graph Database - Getting Started - 2025 - Neo4j Developer Guide - Foundational resource explaining that graph databases don't use JOINs but instead store relationships natively alongside nodes, allowing millions of connections to be accessed per second.</li> <li>Graph Database Performance Comparison: Neo4j vs NebulaGraph vs JanusGraph - 2024 - NebulaGraph - Independent performance testing showing native graph databases significantly outperform traditional approaches for multi-hop queries and traversal operations, supporting the \"hot pie vs cold pie\" performance metaphor.</li> <li>Demystifying Native vs. Multi-Model Graph Database Myths - March 7, 2024 - Aerospike - Provides balanced perspective on index-free adjacency advantages and tradeoffs, explaining how pointer-based traversal avoids index lookups but noting this architectural choice has implications for write operations and data locality.</li> </ol> <p>These references provide authoritative technical backing for the core concepts illustrated in the graphic novel: the dramatic performance difference between index-based lookups (relational databases) and pointer-based traversal (native graph databases), and why index-free adjacency enables constant-time relationship traversal regardless of dataset size.</p>"}]}